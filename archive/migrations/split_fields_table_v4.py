"""
å­—æ®µåˆ†è¡¨è¿ç§»è„šæœ¬ V4
ä¸‰å±‚å»é‡ç­–ç•¥ï¼š
1. æœ‰ upstream_column_id â†’ æŒ‰ upstream_column_id å»é‡
2. æœ‰ table_id â†’ æŒ‰ (table_id, name) å»é‡  
3. æœ‰ datasource_id â†’ æŒ‰ (datasource_id, name) å»é‡
"""
import os
import sys
import uuid
import hashlib

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from backend.config import Config
from backend.models import (
    Base, UniqueRegularField, RegularField, 
    UniqueCalculatedField, CalculatedField,
    CalcFieldDependency, RegularFieldFullLineage, CalcFieldFullLineage,
    regular_field_to_view, calc_field_to_view
)

def get_session():
    engine = create_engine(f'sqlite:///{Config.DATABASE_PATH}', echo=False)
    Session = sessionmaker(bind=engine)
    return Session(), engine

def cleanup_tables(session):
    print("ğŸ§¹ æ¸…ç†æ—§è¡¨...")
    for t in ['regular_fields', 'unique_regular_fields', 'unique_calculated_fields', 
              'calculated_fields', 'regular_field_to_view', 'calc_field_to_view', 
              'calc_field_dependencies', 'regular_field_full_lineage', 'calc_field_full_lineage']:
        try:
            session.execute(text(f'DROP TABLE IF EXISTS {t}'))
        except: pass
    session.commit()

def create_tables(engine):
    print("ğŸ“¦ åˆ›å»ºå››è¡¨æ¶æ„...")
    Base.metadata.create_all(engine, tables=[
        UniqueRegularField.__table__,
        RegularField.__table__,
        UniqueCalculatedField.__table__,
        CalculatedField.__table__,
        CalcFieldDependency.__table__,
        RegularFieldFullLineage.__table__,
        CalcFieldFullLineage.__table__,
        regular_field_to_view,
        calc_field_to_view
    ])

def generate_uuid():
    return str(uuid.uuid4())

def get_formula_hash(formula):
    if not formula:
        return 'empty_' + generate_uuid()
    return hashlib.md5(formula.encode('utf-8')).hexdigest()

def migrate_regular_fields(session):
    """ä¸‰å±‚å»é‡ç­–ç•¥"""
    print("\n[1/4] è¿ç§»åŸå§‹å­—æ®µï¼ˆä¸‰å±‚å»é‡ï¼‰...")
    
    rows = session.execute(text("""
        SELECT * FROM fields 
        WHERE is_calculated = 0 OR is_calculated IS NULL
    """)).mappings().all()
    
    print(f"  - å¾…å¤„ç†å®ä¾‹: {len(rows)}")
    
    unique_map = {} 
    new_unique_records = []
    new_instance_records = []
    
    stats = {'col': 0, 'table': 0, 'ds': 0}
    
    for row in rows:
        # ä¸‰å±‚å»é‡ç­–ç•¥
        if row['upstream_column_id']:
            # ç­–ç•¥1: æŒ‰ upstream_column_id å»é‡
            key = f"col::{row['upstream_column_id']}"
            stats['col'] += 1
        elif row['table_id']:
            # ç­–ç•¥2: æŒ‰ (table_id, name) å»é‡
            key = f"table::{row['table_id']}::{row['name']}"
            stats['table'] += 1
        elif row['datasource_id']:
            # ç­–ç•¥3: æŒ‰ (datasource_id, name) å»é‡
            key = f"ds::{row['datasource_id']}::{row['name']}"
            stats['ds'] += 1
        else:
            # æ— æ³•å»é‡ï¼Œå½“ä½œç‹¬ç«‹
            key = f"orphan::{row['id']}"
        
        if key not in unique_map:
            unique_id = generate_uuid()
            unique_map[key] = unique_id
            
            new_unique_records.append({
                'id': unique_id,
                'name': row['name'],
                'upstream_column_id': row['upstream_column_id'],
                'upstream_column_name': row['upstream_column_name'],
                'table_id': row['table_id'],
                'remote_type': row['remote_type'],
                'description': row['description'],
                'created_at': row['created_at']
            })
        else:
            unique_id = unique_map[key]
            
        new_instance_records.append({
            'id': row['id'],
            'unique_id': unique_id,
            'name': row['name'],
            'data_type': row['data_type'],
            'remote_type': row['remote_type'],
            'description': row['description'],
            'table_id': row['table_id'],
            'upstream_column_id': row['upstream_column_id'],
            'upstream_column_name': row['upstream_column_name'],
            'datasource_id': row['datasource_id'],
            'workbook_id': row['workbook_id'],
            'role': row['role'],
            'aggregation': row['aggregation'],
            'is_hidden': row['is_hidden'],
            'folder_name': row['folder_name'],
            'fully_qualified_name': row['fully_qualified_name'],
            'caption': row['caption'],
            'semantic_role': row['semantic_role'],
            'default_format': row['default_format'],
            'remote_field_id': row['remote_field_id'],
            'remote_field_name': row['remote_field_name'],
            'created_at': row['created_at'],
            'updated_at': row['updated_at'],
            'usage_count': row['usage_count']
        })
    
    if new_unique_records:
        session.bulk_insert_mappings(UniqueRegularField, new_unique_records)
    if new_instance_records:
        session.bulk_insert_mappings(RegularField, new_instance_records)
    
    print(f"  - å»é‡ç­–ç•¥åˆ†å¸ƒ: ç‰©ç†åˆ—={stats['col']}, è¡¨={stats['table']}, æ•°æ®æº={stats['ds']}")
    print(f"  âœ… åŸå§‹å­—æ®µ: {len(new_instance_records)} å®ä¾‹ -> {len(new_unique_records)} æ ‡å‡†å­—æ®µ")
    return len(new_instance_records)

def migrate_calculated_fields(session):
    """è®¡ç®—å­—æ®µå»é‡ï¼š(formula_hash, datasource_id ç©¿é€å)"""
    print("\n[2/4] è¿ç§»è®¡ç®—å­—æ®µ...")
    
    # è·å–åµŒå…¥å¼æ•°æ®æºâ†’å‘å¸ƒå¼æ•°æ®æºæ˜ å°„
    ds_penetration = {}
    ds_rows = session.execute(text("""
        SELECT id, source_published_datasource_id FROM datasources WHERE source_published_datasource_id IS NOT NULL
    """)).fetchall()
    for ds_id, source_id in ds_rows:
        ds_penetration[ds_id] = source_id
    
    rows = session.execute(text("SELECT * FROM fields WHERE is_calculated = 1")).mappings().all()
    print(f"  - å¾…å¤„ç†å®ä¾‹: {len(rows)}")
    
    unique_map = {}
    new_unique_records = []
    new_instance_records = []
    
    for row in rows:
        formula = row['formula'] or ''
        formula_hash = get_formula_hash(formula)
        
        # ç©¿é€åˆ°å‘å¸ƒå¼æ•°æ®æº
        ds_id = row['datasource_id']
        if ds_id and ds_id in ds_penetration:
            ds_id = ds_penetration[ds_id]
        
        # å»é‡é”®: (formula_hash, ç©¿é€åçš„datasource_id)
        key = f"{formula_hash}::{ds_id or 'none'}"
        
        if key not in unique_map:
            unique_id = generate_uuid()
            unique_map[key] = unique_id
            
            new_unique_records.append({
                'id': unique_id,
                'name': row['name'],
                'formula': formula,
                'formula_hash': formula_hash,
                'description': row['description'],
                'complexity_score': 0,
                'created_at': row['created_at']
            })
        else:
            unique_id = unique_map[key]
            
        new_instance_records.append({
            'id': row['id'],
            'unique_id': unique_id,
            'name': row['name'],
            'data_type': row['data_type'],
            'description': row['description'],
            'formula': formula,
            'formula_hash': formula_hash,
            'complexity_score': 0,
            'datasource_id': row['datasource_id'],
            'workbook_id': row['workbook_id'],
            'table_id': row['table_id'],
            'role': row['role'],
            'is_hidden': row['is_hidden'],
            'folder_name': row['folder_name'],
            'fully_qualified_name': row['fully_qualified_name'],
            'caption': row['caption'],
            'usage_count': row['usage_count'],
            'created_at': row['created_at'],
            'updated_at': row['updated_at']
        })
        
    if new_unique_records:
        session.bulk_insert_mappings(UniqueCalculatedField, new_unique_records)
    if new_instance_records:
        session.bulk_insert_mappings(CalculatedField, new_instance_records)

    print(f"  âœ… è®¡ç®—å­—æ®µ: {len(new_instance_records)} å®ä¾‹ -> {len(new_unique_records)} æ ‡å‡†æŒ‡æ ‡")
    return len(new_instance_records)

def migrate_relations(session):
    print("\n[3/4] è¿ç§»å…³è”å…³ç³»...")
    
    session.execute(text("""
        INSERT INTO regular_field_to_view (field_id, view_id)
        SELECT fv.field_id, fv.view_id 
        FROM field_to_view fv
        JOIN fields f ON fv.field_id = f.id
        WHERE f.is_calculated = 0 OR f.is_calculated IS NULL
    """))
    
    session.execute(text("""
        INSERT INTO calc_field_to_view (field_id, view_id)
        SELECT fv.field_id, fv.view_id 
        FROM field_to_view fv
        JOIN fields f ON fv.field_id = f.id
        WHERE f.is_calculated = 1
    """))
    
    print("  âœ… è§†å›¾å…³è”è¿ç§»å®Œæˆ")
    
    session.execute(text("""
        INSERT INTO calc_field_dependencies (
            source_field_id, 
            dependency_regular_field_id,
            dependency_calc_field_id,
            dependency_name, 
            dependency_type
        )
        SELECT 
            fd.source_field_id,
            CASE WHEN dep.is_calculated = 0 OR dep.is_calculated IS NULL 
                 THEN fd.dependency_field_id ELSE NULL END,
            CASE WHEN dep.is_calculated = 1 
                 THEN fd.dependency_field_id ELSE NULL END,
            fd.dependency_name,
            fd.dependency_type
        FROM field_dependencies fd
        LEFT JOIN fields dep ON fd.dependency_field_id = dep.id
        WHERE fd.source_field_id IN (SELECT id FROM fields WHERE is_calculated = 1)
    """))
    
    print("  âœ… ä¾èµ–å…³ç³»è¿ç§»å®Œæˆ")

def migrate_lineage(session):
    print("\n[4/4] è¿ç§»è¡€ç¼˜æ•°æ®...")
    
    session.execute(text("""
        INSERT INTO regular_field_full_lineage (
            field_id, table_id, datasource_id, workbook_id, lineage_type, lineage_path
        )
        SELECT fl.field_id, fl.table_id, fl.datasource_id, fl.workbook_id, fl.lineage_type, fl.lineage_path
        FROM field_full_lineage fl
        JOIN fields f ON fl.field_id = f.id
        WHERE f.is_calculated = 0 OR f.is_calculated IS NULL
    """))
    
    session.execute(text("""
        INSERT INTO calc_field_full_lineage (
            field_id, table_id, datasource_id, workbook_id, lineage_type, lineage_path
        )
        SELECT fl.field_id, fl.table_id, fl.datasource_id, fl.workbook_id, fl.lineage_type, fl.lineage_path
        FROM field_full_lineage fl
        JOIN fields f ON fl.field_id = f.id
        WHERE f.is_calculated = 1
    """))
    
    print("  âœ… è¡€ç¼˜æ•°æ®è¿ç§»å®Œæˆ")

def main():
    print("ğŸš€ å¼€å§‹å››è¡¨æ¶æ„è¿ç§» V4...")
    print("ä¸‰å±‚å»é‡ç­–ç•¥ï¼š")
    print("  1. æœ‰ upstream_column_id â†’ æŒ‰ç‰©ç†åˆ—å»é‡ï¼ˆè·¨æ•°æ®æºï¼‰")
    print("  2. æœ‰ table_id â†’ æŒ‰ (table_id, name) å»é‡")
    print("  3. æœ‰ datasource_id â†’ æŒ‰ (datasource_id, name) å»é‡")
    
    session, engine = get_session()
    
    try:
        cleanup_tables(session)
        create_tables(engine)
        
        migrate_regular_fields(session)
        migrate_calculated_fields(session)
        migrate_relations(session)
        migrate_lineage(session)
        
        session.commit()
        print("\nâœ¨ è¿ç§»å®Œæˆï¼")
        
    except Exception as e:
        session.rollback()
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        session.close()

if __name__ == "__main__":
    main()
