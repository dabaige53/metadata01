"""
å…ƒæ•°æ®åŒæ­¥ç®¡ç†å™¨
å°† Tableau å…ƒæ•°æ®å­˜å…¥æœ¬åœ°æ•°æ®åº“
"""
import os
import sys
import hashlib
from collections import defaultdict
from datetime import datetime
from typing import Optional, List, Dict, Any
from sqlalchemy import select, text
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.config import Config
from backend.migrations import split_fields_table_v5
from backend.models import (
    Base, get_engine, init_db, get_session,
    Database, DBTable, DBColumn, Field, Datasource, Workbook, View,
    TableauUser, Project,
    table_to_datasource, datasource_to_workbook, field_to_view, CalculatedField, SyncLog,
    FieldDependency, Metric, dashboard_to_sheet
)
from .tableau_client import TableauMetadataClient

class MetadataSync:
    """å…ƒæ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, client: TableauMetadataClient, db_path: str = None):
        self.client = client
        self.db_path = db_path or Config.DATABASE_PATH
        self.engine = get_engine(self.db_path)
        self.session = get_session(self.engine)
        self.sync_log: Optional[SyncLog] = None
        self.deduplication_map = {} # skipped_id -> survivor_id (è·¨é˜¶æ®µå»é‡æ˜ å°„)
    
    def _start_sync_log(self, sync_type: str):
        """å¼€å§‹åŒæ­¥æ—¥å¿—"""
        self.sync_log = SyncLog(
            sync_type=sync_type,
            status="running",
            started_at=datetime.now(),
            records_synced=0
        )
        self.session.add(self.sync_log)
        self.session.commit()
    
    def _complete_sync_log(self, records: int, error: str = None):
        """å®ŒæˆåŒæ­¥æ—¥å¿—"""
        if self.sync_log:
            self.sync_log.status = "failed" if error else "completed"
            self.sync_log.completed_at = datetime.utcnow()
            self.sync_log.records_synced = records
            self.sync_log.error_message = error
            self.session.commit()

    def _cleanup_orphaned_records(self, model_class, current_ids: List[str], filter_condition=None):
        """æ¸…ç†æ•°æ®åº“ä¸­å­˜åœ¨ä½†æœ¬æ¬¡åŒæ­¥æœªå‘ç°çš„è®°å½•ï¼ˆç‰©ç†åˆ é™¤ï¼‰"""
        if not current_ids and filter_condition is None:
            return 0
        
        query = self.session.query(model_class)
        if filter_condition is not None:
            query = query.filter(filter_condition)
        
        # è¿‡æ»¤æ‰æœ¬æ¬¡åŒæ­¥å‘ç°çš„ IDs
        orphaned = query.filter(~model_class.id.in_(current_ids)).all()
        
        count = 0
        for record in orphaned:
            # å¯¹äº Fieldï¼Œè¿˜éœ€è¦æ¸…ç†ç›¸å…³çš„ä¾èµ–å’Œå…³è”
            if model_class == Field:
                # æ–°æ¨¡å‹: CalculatedField ä½¿ç”¨ id ä½œä¸ºä¸»é”®ï¼Œä¸ Field.id ç›¸åŒ
                self.session.query(CalculatedField).filter_by(id=record.id).delete()
                self.session.query(FieldDependency).filter(
                    (FieldDependency.source_field_id == record.id) | 
                    (FieldDependency.dependency_field_id == record.id)
                ).delete()
                from backend.models import Metric, field_to_view
                self.session.query(Metric).filter_by(id=record.id).delete()
                self.session.execute(field_to_view.delete().where(field_to_view.c.field_id == record.id))
                self.session.execute(text("DELETE FROM field_full_lineage WHERE field_id = :fid"), {"fid": record.id})
            
            self.session.delete(record)
            count += 1
            
        if count > 0:
            print(f"  ğŸ§¹ æ¸…ç†äº† {count} ä¸ªå·²ä¸å­˜åœ¨çš„ {model_class.__name__} è®°å½•")
        return count
    
    def sync_databases(self) -> int:
        """åŒæ­¥æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“¦ åŒæ­¥æ•°æ®åº“...")
        self._start_sync_log("databases")
        
        try:
            databases = self.client.fetch_databases()
            count = 0
            current_ids = []
            
            for db_data in databases:
                if not db_data or not db_data.get("id"):
                    continue
                
                current_ids.append(db_data["id"])
                db = self.session.query(Database).filter_by(id=db_data["id"]).first()
                if not db:
                    db = Database(id=db_data["id"])
                    self.session.add(db)
                
                db.name = db_data.get("name", "")
                db.luid = db_data.get("luid")
                db.connection_type = db_data.get("connectionType")
                db.host_name = db_data.get("hostName")
                db.port = db_data.get("port")
                db.service = db_data.get("service")
                db.description = db_data.get("description")
                db.is_certified = db_data.get("isCertified", False)
                db.certification_note = db_data.get("certificationNote")
                db.platform = db_data.get("platform")
                db.updated_at = datetime.now()
                
                count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®åº“è®°å½•
            self._cleanup_orphaned_records(Database, current_ids)
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®åº“")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_tables(self) -> int:
        """åŒæ­¥æ•°æ®è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“‹ åŒæ­¥æ•°æ®è¡¨...")
        self._start_sync_log("tables")
        
        try:
            tables = self.client.fetch_tables()
            table_count = 0
            column_count = 0
            current_ids = []
            
            for table_data in tables:
                if not table_data or not table_data.get("id"):
                    continue
                
                # ä¸å†è·³è¿‡åµŒå…¥å¼è¡¨ï¼Œå› ä¸ºå­—æ®µçš„ upstream_column_id å¯èƒ½å¼•ç”¨åµŒå…¥å¼è¡¨çš„åˆ—
                # æ ‡è®° is_embedded ä»¥ä¾¿åœ¨ UI ä¸­åŒºåˆ†
                is_embedded = table_data.get("isEmbedded", False)
                
                current_ids.append(table_data["id"])
                table = self.session.query(DBTable).filter_by(id=table_data["id"]).first()
                if not table:
                    table = DBTable(id=table_data["id"])
                    self.session.add(table)
                
                table.name = table_data.get("name", "")
                table.luid = table_data.get("luid")
                table.full_name = table_data.get("fullName")
                table.schema = table_data.get("schema")
                
                # å…³è”æ•°æ®åº“
                db_info = table_data.get("database", {})
                if db_info:
                    table.database_id = db_info.get("id")
                    table.connection_type = db_info.get("connectionType", "")
                
                table.table_type = table_data.get("tableType")
                table.description = table_data.get("description")
                table.is_embedded = is_embedded  # æ­£ç¡®æ ‡è®°åµŒå…¥å¼è¡¨
                table.is_certified = table_data.get("isCertified", False)
                table.certification_note = table_data.get("certificationNote")
                table.project_name = table_data.get("projectName")
                
                # è§£ææ—¶é—´
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = table_data.get(time_field)
                    if time_val:
                        try:
                            # å…¼å®¹ä¸åŒæ ¼å¼
                            dt = datetime.fromisoformat(time_val.replace("Z", "+00:00"))
                            setattr(table, attr_name, dt)
                        except:
                            pass
                
                # åŒæ­¥åˆ— (Columns)
                columns = table_data.get("columns", [])
                for col_data in columns:
                    if not col_data or not col_data.get("id"):
                        continue
                    
                    col = self.session.query(DBColumn).filter_by(id=col_data["id"]).first()
                    if not col:
                        col = DBColumn(id=col_data["id"])
                        self.session.add(col)
                    
                    col.name = col_data.get("name", "")
                    col.remote_type = col_data.get("remoteType")
                    col.description = col_data.get("description")
                    col.is_nullable = col_data.get("isNullable")
                    col.table_id = table.id
                    column_count += 1
                
                table_count += 1
                if table_count % 100 == 0:
                    self.session.commit()
                    print(f"  - æ•°æ®è¡¨: å·²å¤„ç† {table_count}/{len(tables)}")
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ­£å¼è¡¨è®°å½•ï¼ˆæ’é™¤åµŒå…¥å¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†åŒæ­¥å®ƒä»¬ï¼‰
            self._cleanup_orphaned_records(DBTable, current_ids, filter_condition=(DBTable.is_embedded == False))
            
            self._complete_sync_log(table_count)
            print(f"  âœ… åŒæ­¥ {table_count} ä¸ªæ•°æ®è¡¨, {column_count} ä¸ªåˆ—")
            return table_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_datasources(self) -> int:
        """åŒæ­¥æ•°æ®æºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ”— åŒæ­¥æ•°æ®æº...")
        self._start_sync_log("datasources")
        
        try:
            datasources = self.client.fetch_datasources()
            count = 0
            current_ids = []
            
            for ds_data in datasources:
                if not ds_data or not ds_data.get("id"):
                    continue
                
                # ä»…åŒæ­¥å‘å¸ƒå¼æ•°æ®æº
                if ds_data.get("isEmbedded"):
                    continue
                
                current_ids.append(ds_data["id"])
                ds = self.session.query(Datasource).filter_by(id=ds_data["id"]).first()
                if not ds:
                    ds = Datasource(id=ds_data["id"])
                    self.session.add(ds)
                
                ds.name = ds_data.get("name", "")
                ds.luid = ds_data.get("luid")
                ds.description = ds_data.get("description")
                ds.uri = ds_data.get("uri")
                ds.project_name = ds_data.get("projectName", "")
                ds.has_extract = ds_data.get("hasExtracts", False)
                ds.is_certified = ds_data.get("isCertified", False)
                ds.certification_note = ds_data.get("certificationNote")
                ds.certifier_display_name = ds_data.get("certifierDisplayName")
                ds.contains_unsupported_custom_sql = ds_data.get("containsUnsupportedCustomSql", False)
                ds.has_active_warning = ds_data.get("hasActiveWarning", False)
                ds.vizportal_url_id = ds_data.get("vizportalUrlId")
                ds.is_embedded = False # æ˜ç¡®è®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬è¿‡æ»¤æ‰äº†åµŒå…¥å¼æ•°æ®æº
                
                owner = ds_data.get("owner", {})
                if owner:
                    ds.owner = owner.get("username", "")
                    ds.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [
                    ("extractLastRefreshTime", "extract_last_refresh_time"),
                    ("extractLastIncrementalUpdateTime", "extract_last_incremental_update_time"),
                    ("extractLastUpdateTime", "extract_last_update_time"),
                    ("createdAt", "created_at"),
                    ("updatedAt", "updated_at")
                ]:
                    time_val = ds_data.get(time_field)
                    if time_val:
                        try:
                            setattr(ds, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                count += 1
                
                # åŒæ­¥è¡¨åˆ°æ•°æ®æºçš„å…³ç³»
                upstream_tables = ds_data.get("upstreamTables", [])
                if upstream_tables:
                    print(f"  ğŸ“Š æ•°æ®æº {ds_data.get('name')} çš„ä¸Šæ¸¸è¡¨: {len(upstream_tables)} ä¸ª")
                    # æŠ½æ ·æ‰“å° ID æ ¼å¼
                    if len(upstream_tables) > 0:
                        print(f"     ç¤ºä¾‹è¡¨ ID: {upstream_tables[0].get('id')}")

                for tbl in upstream_tables:
                    if not tbl or not tbl.get("id"):
                        continue
                    rel = self.session.execute(
                        select(table_to_datasource).where(
                            table_to_datasource.c.table_id == tbl["id"],
                            table_to_datasource.c.datasource_id == ds_data["id"]
                        )
                    ).first()
                    
                    if not rel:
                        try:
                            self.session.execute(
                                table_to_datasource.insert().values(
                                    table_id=tbl["id"],
                                    datasource_id=ds_data["id"],
                                    relationship_type="upstream"
                                )
                            )
                        except:
                            pass
                
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®æºï¼ˆæ’é™¤åµŒå…¥å¼ï¼‰
            self._cleanup_orphaned_records(Datasource, current_ids, filter_condition=(Datasource.is_embedded == False))
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®æº")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
            
    def _save_embedded_datasource(self, ds_data: Dict, workbook_id: str, source_published_ds_id: str = None):
        """ä¿å­˜åµŒå…¥å¼æ•°æ®æº (åŒ…æ‹¬ç›´è¿åœºæ™¯å’Œå¼•ç”¨å·²å‘å¸ƒæ•°æ®æºåœºæ™¯)"""
        try:
            ds_id = ds_data["id"]
            ds = self.session.query(Datasource).filter_by(id=ds_id).first()
            if not ds:
                ds = Datasource(id=ds_id)
                self.session.add(ds)
            
            ds.name = ds_data.get("name") or "Embedded Datasource"
            ds.is_embedded = True
            ds.project_name = "(Embedded)" # åµŒå…¥å¼æºé€šå¸¸æ²¡æœ‰ç‹¬ç«‹çš„é¡¹ç›®å½’å±ï¼Œå› ä¸ºå®ƒå±äºå·¥ä½œç°¿
            
            # ğŸ†• è®¾ç½®æºå·²å‘å¸ƒæ•°æ®æºIDï¼ˆè¡€ç¼˜å…³ç³»ï¼‰
            if source_published_ds_id:
                ds.source_published_datasource_id = source_published_ds_id
            
            # å»ºç«‹ä¸Šæ¸¸è¡¨å…³è” (ç›´è¿æºçš„å…³é”®è¡€ç¼˜)
            upstream_tables = ds_data.get("upstreamTables", [])
            for tbl in upstream_tables:
                if not tbl or not tbl.get("id"):
                     continue
                
                # ä½¿ç”¨ select æŸ¥è¯¢é¿å…é‡å¤æ’å…¥é”™è¯¯
                rel = self.session.execute(
                    select(table_to_datasource).where(
                        table_to_datasource.c.table_id == tbl["id"],
                        table_to_datasource.c.datasource_id == ds_id
                    )
                ).first()
                
                if not rel:
                    try:
                        self.session.execute(
                            table_to_datasource.insert().values(
                                table_id=tbl["id"],
                                datasource_id=ds_id,
                                relationship_type="upstream"
                            )
                        )
                    except:
                        pass # å¿½ç•¥ä¸»é”®å†²çª

        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜åµŒå…¥å¼æ•°æ®æºå¤±è´¥: {e}")
    
    def sync_workbooks(self) -> int:
        """åŒæ­¥å·¥ä½œç°¿å’Œè§†å›¾ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“Š åŒæ­¥å·¥ä½œç°¿...")
        self._start_sync_log("workbooks")
        
        try:
            workbooks = self.client.fetch_workbooks()
            wb_count = 0
            view_count = 0
            current_wb_ids = []
            current_view_ids = []
            
            for wb_data in workbooks:
                current_wb_ids.append(wb_data["id"])
                wb = self.session.query(Workbook).filter_by(id=wb_data["id"]).first()
                if not wb:
                    wb = Workbook(id=wb_data["id"])
                    self.session.add(wb)
                
                wb.name = wb_data.get("name", "")
                wb.luid = wb_data.get("luid")
                wb.description = wb_data.get("description")
                wb.uri = wb_data.get("uri")
                wb.project_name = wb_data.get("projectName", "")
                wb.contains_unsupported_custom_sql = wb_data.get("containsUnsupportedCustomSql", False)
                wb.has_active_warning = wb_data.get("hasActiveWarning", False)
                wb.vizportal_url_id = wb_data.get("vizportalUrlId")
                
                owner = wb_data.get("owner", {})
                if owner:
                    wb.owner = owner.get("username", "")
                    wb.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = wb_data.get(time_field)
                    if time_val:
                        try:
                            setattr(wb, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                wb_count += 1
                
                # åŒæ­¥æ•°æ®æºåˆ°å·¥ä½œç°¿çš„å…³ç³» (Published)
                upstream_ds = wb_data.get("upstreamDatasources", [])
                for ds in upstream_ds:
                    if not ds or not ds.get("id"):
                        continue
                    self._link_datasource_to_workbook(ds["id"], wb_data["id"])

                # åŒæ­¥åµŒå…¥å¼æ•°æ®æº (Embedded)
                embedded_ds = wb_data.get("embeddedDatasources", [])
                for eds in embedded_ds:
                    if not eds or not eds.get("id"):
                        continue
                    
                    upstream_published = eds.get("upstreamDatasources", [])
                    upstream_ds_id = None
                    
                    if upstream_published:
                        # åœºæ™¯1ï¼šåµŒå…¥å¼æºå¼•ç”¨äº†å·²å‘å¸ƒæ•°æ®æº (ç©¿é€æ¨¡å¼)
                        # å°†ä¸Šæ¸¸å‘å¸ƒå¼æ•°æ®æºå…³è”åˆ°å·¥ä½œç°¿
                        for up_ds in upstream_published:
                            if up_ds and up_ds.get("id"):
                                self._link_datasource_to_workbook(up_ds["id"], wb_data["id"])
                        upstream_ds_id = upstream_published[0]["id"]
                        
                        # ğŸ†• åœºæ™¯1ä¹Ÿä¿å­˜åµŒå…¥å¼æ•°æ®æºè®°å½•ï¼Œå¹¶è®¾ç½® source_published_datasource_id
                        self._save_embedded_datasource(eds, wb_data["id"], source_published_ds_id=upstream_ds_id)
                        # ğŸ”§ ä¿®å¤ï¼šåœºæ™¯1ä¹Ÿéœ€è¦å»ºç«‹åµŒå…¥å¼æ•°æ®æºåˆ°å·¥ä½œç°¿çš„å…³è”
                        self._link_datasource_to_workbook(eds["id"], wb_data["id"])
                    else:
                        # åœºæ™¯2ï¼šå®Œå…¨ç‹¬ç«‹çš„åµŒå…¥å¼ç›´è¿æº (ä¿ç•™æ¨¡å¼)
                        # ä¿å­˜è¯¥åµŒå…¥å¼æ•°æ®æºï¼Œæ ‡è®° is_embedded=True
                        # è¿™æ ·å·¥ä½œç°¿å°±æœ‰äº†ä¸€ä¸ªå…³è”çš„ Datasourceï¼Œå­—æ®µä¹Ÿæœ‰äº†å½’å±
                        self._save_embedded_datasource(eds, wb_data["id"])
                        # åŒæ—¶ä¹Ÿå»ºç«‹ Datasource -> Workbook å…³è” (è™½ç„¶ä¸Šé¢å·²ç»åœ¨ DB å±‚é¢å»ºç«‹äº†ï¼Œä½†è¿™é‡Œæ˜¾å¼é“¾æ¥)
                        self._link_datasource_to_workbook(eds["id"], wb_data["id"])
                        upstream_ds_id = eds["id"]
                    
                    # åŒæ­¥åµŒå…¥å¼å­—æ®µ
                    eds_fields = eds.get("fields", [])
                    for f_data in eds_fields:
                        self._sync_field(f_data, datasource_id=upstream_ds_id, workbook_id=wb_data["id"])



                
                # åŒæ­¥è§†å›¾ (sheets + dashboards)
                for idx, sheet in enumerate(wb_data.get("sheets", [])):
                    if not sheet or not sheet.get("id"):
                        continue
                    current_view_ids.append(sheet["id"])
                    view = self.session.query(View).filter_by(id=sheet["id"]).first()
                    if not view:
                        view = View(id=sheet["id"])
                        self.session.add(view)
                    
                    view.name = sheet.get("name", "")
                    view.luid = sheet.get("luid")
                    view.path = sheet.get("path")
                    view.index = sheet.get("index", idx)
                    view.view_type = "sheet"
                    view.workbook_id = wb_data["id"]
                    
                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = sheet.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    view_count += 1
                
                # åŒæ­¥ä»ªè¡¨æ¿ (dashboards)
                for idx, dashboard in enumerate(wb_data.get("dashboards", [])):
                    if not dashboard or not dashboard.get("id"):
                        continue
                    current_view_ids.append(dashboard["id"])
                    view = self.session.query(View).filter_by(id=dashboard["id"]).first()
                    if not view:
                        view = View(id=dashboard["id"])
                        self.session.add(view)
                    
                    view.name = dashboard.get("name", "")
                    view.luid = dashboard.get("luid")
                    view.path = dashboard.get("path")
                    view.index = dashboard.get("index", idx)
                    view.view_type = "dashboard"
                    view.workbook_id = wb_data["id"]
                    

                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = dashboard.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    # åŒæ­¥ä»ªè¡¨æ¿ä¸ sheet çš„å…³è”
                    contained_sheets = dashboard.get("sheets", [])
                    for contained_sheet in contained_sheets:
                         if contained_sheet and contained_sheet.get("id"):
                             sheet_id = contained_sheet.get("id")
                             # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                             rel = self.session.execute(
                                 select(dashboard_to_sheet).where(
                                     dashboard_to_sheet.c.dashboard_id == dashboard["id"],
                                     dashboard_to_sheet.c.sheet_id == sheet_id
                                 )
                             ).first()
                             if not rel:
                                 try:
                                     self.session.execute(
                                         dashboard_to_sheet.insert().values(
                                             dashboard_id=dashboard["id"],
                                             sheet_id=sheet_id
                                         )
                                     )
                                 except Exception as e:
                                     print(f"  âš ï¸ å…³è” sheet å¤±è´¥: {e}")
                                     pass
                    
                    view_count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„å·¥ä½œç°¿
            self._cleanup_orphaned_records(Workbook, current_wb_ids)
            # æ¸…ç†å·²ä¸å­˜åœ¨çš„è§†å›¾
            self._cleanup_orphaned_records(View, current_view_ids)
            
            self._complete_sync_log(wb_count)
            print(f"  âœ… åŒæ­¥ {wb_count} ä¸ªå·¥ä½œç°¿, {view_count} ä¸ªè§†å›¾")
            return wb_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
            return 0
    
    def sync_fields(self) -> int:
        """åŒæ­¥å­—æ®µï¼ˆå«å»é‡é€»è¾‘ï¼‰"""
        print("\nğŸ”¤ åŒæ­¥å­—æ®µ...")
        self._start_sync_log("fields")
        
        try:
            from backend.models import table_to_datasource, Datasource
            # å»ºç«‹ç‰©ç†è¡¨åˆ°å‘å¸ƒå¼æ•°æ®æºçš„æ˜ å°„ï¼Œç”¨äºç©¿é€è¡¥é½
            # ä»…åŒ…å«éåµŒå…¥å¼æ•°æ®æº
            table_real_ds_map = {}
            ds_to_table_rels = self.session.execute(
                select(table_to_datasource.c.table_id, table_to_datasource.c.datasource_id)
                .join(Datasource, Datasource.id == table_to_datasource.c.datasource_id)
                .where(Datasource.is_embedded == 0)
            ).fetchall()
            for tid, dsid in ds_to_table_rels:
                if tid not in table_real_ds_map:
                    table_real_ds_map[tid] = dsid

            # --- å»é‡å‡†å¤‡å¼€å§‹ ---
            # ç¼“å­˜å·²å‘å¸ƒçš„å­—æ®µï¼š(datasource_id, name) -> field_id
            published_field_cache = {}
            physical_column_cache = {} # (table_name, column_name) -> field_id
            calc_field_cache = {}      # (field_name, formula_hash) -> field_id
            self.deduplication_map = {} # skipped_id -> survivor_id

            all_fields = self.client.fetch_fields()
            ds_fields_map = {} # {datasource_id: [field_data, ...]}
            published_datasources = [] # å­˜å‚¨å·²å‘å¸ƒçš„datasourceå¯¹è±¡
            embedded_fields = [] # å­˜å‚¨åµŒå…¥å¼å­—æ®µ

            print(f"  - æ‹‰å–åˆ° {len(all_fields)} ä¸ªå­—æ®µï¼Œå¼€å§‹åˆ†ç±»...")

            for f in all_fields:
                if not f or not f.get("id"): continue
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºåµŒå…¥å¼ï¼šä½¿ç”¨ is_from_embedded_ds æ ‡è®°ï¼ˆåœ¨ fetch_fields ä¸­è®¾ç½®ï¼‰
                is_from_embedded = f.get("is_from_embedded_ds", False)
                
                is_from_embedded = f.get("is_from_embedded_ds", False)
                
                if is_from_embedded:
                    # å¯¹äºåµŒå…¥å¼å­—æ®µï¼Œå°è¯•ä»æ•°æ®æºåå‘æŸ¥æ‰¾ workbook_id
                    # æ³¨æ„ï¼šfetch_fields è¿”å›çš„æ•°æ®ä¸­å¯èƒ½æ²¡æœ‰ workbook ä¿¡æ¯ï¼Œ
                    # è¿™é‡Œçš„ f æ˜¯ä» self.client.fetch_fields() è·å–çš„ã€‚
                    # å¦‚æœ fetch_fields æ²¡æœ‰è¿”å› workbookï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œè¡¥å…¨ã€‚
                    # ä½† fetch_fields å®é™…ä¸Šæ˜¯å…¨é‡æ‹‰å–ï¼Œå¯¹äº embedded datasourceï¼Œé€šå¸¸èƒ½å…³è”åˆ° parent workbookã€‚
                    # æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ f é‡Œé¢æ˜¯å¦æœ‰ workbook å¯¹è±¡ã€‚
                    if "workbook" in f and f["workbook"]:
                         f["workbook_id"] = f["workbook"]["id"]
                    
                    embedded_fields.append(f)
                else:
                    # è¿™æ˜¯ä¸€ä¸ªå·²å‘å¸ƒå­—æ®µï¼Œå°†å…¶å½’ç±»åˆ°å…¶æ•°æ®æºä¸‹
                    ds_id = f.get("datasource_id")
                    if ds_id:
                        if ds_id not in ds_fields_map:
                            ds_fields_map[ds_id] = []
                            # é¦–æ¬¡é‡åˆ°è¿™ä¸ªæ•°æ®æºï¼Œå°è¯•è·å–å…¶ä¿¡æ¯
                            ds_info = self.client.fetch_datasource_by_id(ds_id)
                            if ds_info:
                                published_datasources.append(ds_info)
                        ds_fields_map[ds_id].append(f)
            
            # --- å»é‡å‡†å¤‡å¼€å§‹ ---
            # ç¼“å­˜å·²å‘å¸ƒçš„å­—æ®µï¼š(datasource_id, name) -> field_id
            published_field_cache = {}
            physical_column_cache = {} # (table_name, column_name) -> field_id
            calc_field_cache = {}      # (field_name, formula_hash) -> field_id
            self.deduplication_map = {} # skipped_id -> survivor_id (è®°å½•è¢«å»é‡çš„å­—æ®µæ˜ å°„å…³ç³»)

            print(f"  - å­—æ®µé¢„å¤„ç†: å·²å‘å¸ƒæ•°æ®æº {len(published_datasources)} ä¸ª, åµŒå…¥å¼å­—æ®µ {len(embedded_fields)} ä¸ª")
            
            count = 0
            calc_count = 0
            skipped_count = 0
            current_ids = []
            
            # 1. ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†å‘å¸ƒå¼æ•°æ®æº (PublishedDatasource)
            # è¿™äº›æ˜¯"çœŸèº«"ï¼Œä¼˜å…ˆä¿å­˜å¹¶å»ºç«‹ç¼“å­˜
            # ----------------------------------------------------
            for ds in published_datasources:
                ds_id = ds["id"]
                # é‡æ–°æ„å»º table_real_ds_mapï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å‘å¸ƒå¼æ•°æ®æºçš„æ˜ å°„
                # è¿™ä¸€æ­¥å¯èƒ½éœ€è¦æ›´ç²¾ç»†çš„é€»è¾‘ï¼Œè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥æ›´æ–°
                # å®é™…ä¸Šï¼Œtable_real_ds_map åº”è¯¥åœ¨æ‰€æœ‰å‘å¸ƒå¼æ•°æ®æºå¤„ç†å‰æ„å»ºå®Œæˆ
                # ä½†ä¸ºäº†ä¸ç”¨æˆ·æä¾›çš„diffä¿æŒä¸€è‡´ï¼Œè¿™é‡Œæš‚æ—¶ä¿ç•™
                # self._get_table_to_datasource_map(ds) åº”è¯¥è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œç„¶åç”¨ update
                # è€ƒè™‘åˆ°åŸå§‹ä»£ç ä¸­ table_real_ds_map å·²ç»é€šè¿‡ DB æŸ¥è¯¢æ„å»ºï¼Œè¿™é‡Œå¯èƒ½ä¸éœ€è¦å†æ¬¡æ›´æ–°
                # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œå¦‚æœéœ€è¦ï¼Œå†æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                # table_real_ds_map.update(self._get_table_to_datasource_map(ds))
                
                fields = ds_fields_map.get(ds_id, [])
                for f_data in fields:
                    if not f_data: continue
                    
                    # ä¸ºå‘å¸ƒå¼å­—æ®µè®¾ç½®æ­£ç¡®çš„ datasource_id
                    f_data["datasource_id"] = ds_id
                    
                    self._process_single_field(f_data, table_real_ds_map)
                    current_ids.append(f_data["id"])
                    
                    # å¡«å……ä¸€çº§ç¼“å­˜ (å‘å¸ƒå¼å­—æ®µç¼“å­˜)
                    name = f_data.get("name")
                    if name:
                        published_field_cache[(ds_id, name)] = f_data["id"]

                    # å¡«å……äºŒçº§ç¼“å­˜ (ç‰©ç†åˆ—ç¼“å­˜)
                    upstream_cols = f_data.get("upstreamColumns") or []
                    if upstream_cols:
                        first_col = upstream_cols[0]
                        col_name = first_col.get("name")
                        table_info = first_col.get("table")
                        if table_info:
                            table_name = table_info.get("name")
                            if table_name and col_name:
                                physical_column_cache[(table_name, col_name)] = f_data["id"]
                                
                    # å¡«å……ä¸‰çº§ç¼“å­˜ (è®¡ç®—å­—æ®µç¼“å­˜) - å‘å¸ƒå¼ä¹Ÿå¯èƒ½è¢«åç»­åµŒå…¥å¼å¼•ç”¨
                    if f_data.get("isCalculated") or f_data.get("__typename") == "CalculatedField":
                        formula = f_data.get("formula") or ""
                        norm_formula = "".join(formula.split()).lower()
                        if norm_formula and name:
                            import hashlib
                            f_hash = hashlib.md5(norm_formula.encode('utf-8')).hexdigest()
                            
                            # å‘å¸ƒæ•°æ®æºçš„å­—æ®µï¼Œroot_entity_id å°±æ˜¯ ds_id
                            calc_field_cache[(ds_id, name, f_hash)] = f_data["id"]
                            
                            if f_data.get("__typename") == "CalculatedField":
                                 calc_count += 1
                    
                    count += 1
                    if count % 1000 == 0:
                        self.session.commit()
            
            # --- ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†åµŒå…¥å¼å­—æ®µ (ä¸å»é‡ï¼Œå…¨éƒ¨ä¿å­˜) ---
            # å»é‡é€»è¾‘ç§»è‡³å››è¡¨è¿ç§»é˜¶æ®µ (split_fields_table_v5.py)
            for f_data in embedded_fields:
                # ç›´æ¥ä¿å­˜åµŒå…¥å¼å­—æ®µï¼Œä¸åšä»»ä½•å»é‡è·³è¿‡
                wb_id = f_data.get("workbook_id")  # åµŒå…¥å¼å­—æ®µæ•°æ®ä¸­åº”æºå¸¦ workbook_id
                self._process_single_field(f_data, table_real_ds_map, workbook_id=wb_id)
                current_ids.append(f_data["id"])
                
                # ç»Ÿè®¡è®¡ç®—å­—æ®µ
                if f_data.get("isCalculated") or f_data.get("__typename") == "CalculatedField":
                    name = f_data.get("name")
                    if name:
                        calc_count += 1
                
                count += 1
                if count % 1000 == 0:
                    self.session.commit()
                
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„è®°å½•
            self._cleanup_orphaned_records(Field, current_ids)
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µ (å…¶ä¸­ {calc_count} ä¸ªè®¡ç®—å­—æ®µ)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def _process_single_field(self, f_data, table_real_ds_map, workbook_id=None):
        """è¾…åŠ©ï¼šå¤„ç†å•ä¸ªå­—æ®µçš„ä¿å­˜é€»è¾‘"""
        from backend.models import DBTable, DBColumn

        # è·å–/åˆ›å»º Field è®°å½•
        field = self.session.query(Field).filter_by(id=f_data["id"]).first()
        if not field:
            field = Field(id=f_data["id"])
            self.session.add(field)
        
        field.name = f_data.get("name") or ""
        field.description = f_data.get("description") or ""
        field.workbook_id = workbook_id  # ä¿å­˜ Workbook ID
        
        # è·å–åˆå§‹ datasource_id
        ds_id = f_data.get("datasource_id")
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µè¯¦æƒ… (æå‰è§£æä»¥ä¾¿è·å– table_id å’Œ schema ç©¿é€)
        typename = f_data.get("__typename")
        target_table_id = None
        
        if typename == "ColumnField":
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    table_info = first_col.get("table")
                    if table_info:
                        target_table_id = self._get_physical_table_id(table_info)
                        field.table_id = target_table_id

        # è¡€ç¼˜è¡¥é½ï¼šå¦‚æœå½“å‰ datasource_id æŒ‡å‘çš„ä¸æ˜¯å‘å¸ƒå¼ï¼ˆæˆ–ä¸å­˜åœ¨ï¼‰ï¼Œå°è¯•é€šè¿‡ table_id æ‰¾å‘å¸ƒå¼
        if ds_id:
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå…è®¸å¼•ç”¨åµŒå…¥å¼æ•°æ®æº (is_embedded=1)ï¼Œåªè¦å®ƒå­˜åœ¨äºæ•°æ®åº“ä¸­
            exists = self.session.query(Datasource).filter_by(id=ds_id).first()
            if not exists:
                # å°è¯•é€šè¿‡ table_id æ‰¾å‘å¸ƒå¼æ•°æ®æº
                if target_table_id in table_real_ds_map:
                    ds_id = table_real_ds_map[target_table_id]
                else:
                    # æ— æ³•ç©¿é€åˆ°æœ‰æ•ˆæ•°æ®æºï¼Œåˆ™è®¾ä¸º NULL
                    # é™¤éè¿™é‡Œæ˜¯åµŒå…¥å¼å­—æ®µï¼Œä¸” ds_id å°±æŒ‡å‘é‚£ä¸ªåµŒå…¥å¼æ•°æ®æº (è™½ç„¶å¯èƒ½åœ¨ DB ä¸­æ‰¾ä¸åˆ° published record)
                    # æ­¤æ—¶ä¿ç•™åŸå§‹ ds_id å¯èƒ½æ›´å¥½ï¼Œä»¥ä¾¿è‡³å°‘çŸ¥é“å®ƒå±äºå“ªä¸ª "Logical Datasource"
                    # ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼ˆæŒ‡å‘å‘å¸ƒçš„ï¼‰ï¼Œå¦‚æœè¿™çœŸçš„æ˜¯ä¸€ä¸ª embedded datasource id ä¸”æ²¡æœ‰ map åˆ° published, 
                    # æˆ‘ä»¬å¯èƒ½åº”è¯¥è®©å®ƒä¿æŒä¸º None æˆ–è€…æŒ‡å‘åµŒå…¥å¼æ•°æ®æºè®°å½•(å¦‚æœæˆ‘ä»¬å­˜äº†çš„è¯)
                    # ç›®å‰ç³»ç»Ÿé‡Œä¼¼ä¹å­˜äº†åµŒå…¥å¼æ•°æ®æºï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥è®©å®ƒæŒ‡å‘é‚£é‡Œ
                    pass
        else:
             # å¦‚æœ ds_id ä¸ºç©ºï¼Œå°è¯•é€šè¿‡ workbook + table æ¨å¯¼ (é’ˆå¯¹åµŒå…¥å¼ä¸”æ²¡å…³è”å¥½ DS çš„æƒ…å†µ)
             # ä½†è¿™æ¯”è¾ƒå¤æ‚ï¼Œæš‚ä¸”ä¾é  table_real_ds_map
             if target_table_id in table_real_ds_map:
                 ds_id = table_real_ds_map[target_table_id]

        field.datasource_id = ds_id

        
        # é»˜è®¤å€¼
        field.data_type = ""
        field.role = ""
        field.is_calculated = False
        field.formula = ""
        field.is_hidden = False
        field.folder_name = f_data.get("folderName")
        field.fully_qualified_name = ""
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µ
        typename = f_data.get("__typename")
        if typename == "CalculatedField":
            field.is_calculated = True
            field.formula = f_data.get("formula") or ""
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            field.folder_name = f_data.get("folderName")
            
            # æŒ‡æ ‡è¡€ç¼˜ç©¿é€ï¼šé€šè¿‡ upstreamFields æ‰¾ç‰©ç†æ•°æ®æºå’Œç‰©ç†è¡¨
            upstream_fields = f_data.get("upstreamFields") or []
            for uf in upstream_fields:
                if uf:
                    # 1. å°è¯•è·å–ç‰©ç†è¡¨ (ä»ä¸Šæ¸¸å­—æ®µçš„ upstreamColumns)
                    upstream_cols = uf.get("upstreamColumns") or []
                    if upstream_cols and not field.table_id:
                        for col in upstream_cols:
                            if col and col.get("table"):
                                field.table_id = self._get_physical_table_id(col["table"])
                                break
                    
                    # 2. å°è¯•è·å–å‘å¸ƒå¼æ•°æ®æº
                    if uf.get("datasource"):
                        ref_ds_id = uf["datasource"].get("id")
                        if ref_ds_id:
                            exists = self.session.query(Datasource).filter_by(id=ref_ds_id, is_embedded=0).first()
                            if exists:
                                ds_id = ref_ds_id
                                # ç»§ç»­éå†ä»¥æ‰¾æ›´å¤šçš„table_idï¼Œä½†æ•°æ®æºå·²ç¡®å®š
        elif typename == "ColumnField":
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            field.folder_name = f_data.get("folderName")
            
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    
                    # B1 Fix: å°è¯•è¡¥å…¨ç¼ºå¤±çš„ç‰©ç†åˆ—
                    if field.upstream_column_id:
                        db_col = self.session.query(DBColumn).filter_by(id=field.upstream_column_id).first()
                        
                        # å¦‚æœæœ¬åœ°æ²¡æœ‰è¯¥åˆ—ï¼Œä½†æˆ‘ä»¬çŸ¥é“å®ƒå±äºæŸå¼ è¡¨ï¼Œåˆ™åˆ›å»ºè¯¥è¡¨å’Œåˆ—
                        if not db_col and first_col.get("table") and first_col["table"].get("id"):
                            try:
                                real_table_id = first_col["table"]["id"]
                                # ç¡®ä¿è¡¨å­˜åœ¨
                                real_table_id = first_col["table"]["id"]
                                table_typename = first_col["table"].get("__typename")
                                
                                # ç¡®ä¿è¡¨å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
                                real_table = self.session.query(DBTable).filter_by(id=real_table_id).first()
                                new_name = first_col["table"].get("name")
                                
                                if not real_table:
                                    real_table = DBTable(id=real_table_id)
                                    real_table.name = new_name or "Unknown Table"
                                    # å¦‚æœæ˜¯ DatabaseTableï¼Œåˆ™è®¤ä¸ºæ˜¯ç‰©ç†è¡¨ï¼›å¦åˆ™ (EmbeddedTableç­‰) ä¸ºåµŒå…¥è¡¨
                                    real_table.is_embedded = (table_typename != "DatabaseTable")
                                    self.session.add(real_table)
                                    print(f"    ğŸ”¨ è¡¥å…¨ç¼ºå¤±è¡¨: {real_table.name} (Type: {table_typename})")
                                elif real_table.name == "Unknown Table" and new_name:
                                    real_table.name = new_name
                                    print(f"    ğŸ”¨ æ›´æ–°ç¼ºå¤±è¡¨å: {real_table.name}")

                                # åˆ›å»ºè¡¥å…¨åˆ—
                                new_col = DBColumn(id=field.upstream_column_id)
                                new_col.name = field.upstream_column_name
                                new_col.remote_type = first_col.get("remoteType")
                                new_col.table_id = real_table_id
                                self.session.add(new_col)
                                self.session.flush() # ç«‹å³æäº¤
                                print(f"    ğŸ”¨ ä¿®å¤ç¼ºå¤±ç‰©ç†åˆ—: {new_col.name} -> {real_table.name}")
                                db_col = new_col
                            except Exception as e:
                                print(f"    âš ï¸ ä¿®å¤ç‰©ç†åˆ—/è¡¨å¤±è´¥: {e}")

                        if db_col and db_col.remote_type:
                            field.remote_type = db_col.remote_type
                    
                    table_info = first_col.get("table")
                    if table_info:
                        field.table_id = self._get_physical_table_id(table_info)
        elif typename == "DatasourceField":
            # å¤„ç† DatasourceFieldï¼ˆé€šå¸¸æ˜¯åµŒå…¥å¼æ•°æ®æºä¸­å¼•ç”¨å·²å‘å¸ƒæ•°æ®æºçš„å­—æ®µï¼‰
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            
            # è§£æ remoteFieldï¼ˆæŒ‡å‘å·²å‘å¸ƒæ•°æ®æºä¸­çš„åŸå§‹å­—æ®µï¼‰
            remote_field = f_data.get("remoteField")
            if remote_field:
                field.remote_field_id = remote_field.get("id")
                field.remote_field_name = remote_field.get("name")
                
                # å¦‚æœæœ‰ remoteFieldï¼Œå°è¯•è·å–å…¶æ•°æ®æºä¿¡æ¯ç”¨äºè¿½æº¯
                remote_ds = remote_field.get("datasource")
                if remote_ds:
                    remote_ds_id = remote_ds.get("id")
                    # æ£€æŸ¥ remoteField çš„æ•°æ®æºæ˜¯å¦ä¸ºå·²å‘å¸ƒæ•°æ®æº
                    remote_ds_type = remote_ds.get("__typename")
                    if remote_ds_type == "PublishedDatasource":
                        # æ›´æ–°å½“å‰å­—æ®µæ‰€å±çš„åµŒå…¥å¼æ•°æ®æºçš„ source_published_datasource_id
                        # ä½¿ç”¨ parent_datasource_id è€Œä¸æ˜¯ ds_idï¼Œå› ä¸º ds_id å¯èƒ½å·²è¢«è¡€ç¼˜ç©¿é€
                        parent_ds_id = f_data.get("parent_datasource_id")
                        if parent_ds_id:
                            current_ds = self.session.query(Datasource).filter_by(id=parent_ds_id).first()
                            if current_ds and current_ds.is_embedded and not current_ds.source_published_datasource_id:
                                current_ds.source_published_datasource_id = remote_ds_id
            
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    table_info = first_col.get("table")
                    if table_info:
                        field.table_id = self._get_physical_table_id(table_info)
        
        # å¤„ç†è®¡ç®—å­—æ®µè¯¦æƒ…
        if f_data.get("isCalculated"):
            calc_field = self.session.query(CalculatedField).filter_by(
                field_id=f_data["id"]
            ).first()
            if not calc_field:
                calc_field = CalculatedField(field_id=f_data["id"])
                self.session.add(calc_field)
            
            calc_field.name = f_data.get("name") or ""
            calc_field.formula = f_data.get("formula") or ""

        # D Fix: å¦‚æœæ­¤æ—¶è¿˜æ²¡æœ‰ table_idï¼Œå°è¯•é€šè¿‡åç§°åŒ¹é…ç‰©ç†è¡¨
        if not field.table_id and field.name:
            # ä»…å½“å­—æ®µåä¸ç°æœ‰ç‰©ç†è¡¨åå®Œå…¨ä¸€è‡´æ—¶å…³è”
            # æ’é™¤å¸¸è§çš„é€šç”¨åç§°
            ignored_names = [":Measure Names", "Measure Values", "Number of Records", "è®°å½•æ•°"]
            if field.name not in ignored_names:
                matched_table = self.session.query(DBTable).filter_by(name=field.name).first()
                if matched_table:
                    field.table_id = matched_table.id
                    print(f"    ğŸ”¨ ä¿®å¤æ— å…³è”è¡¨å­—æ®µ: {field.name} -> å…³è”åˆ°è¡¨ {matched_table.name}")

    def _get_physical_table_id(self, table_info):
        """å°è¯•ä» Table å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯ EmbeddedTable æˆ– CustomSQLTableï¼‰ä¸­æå–ç‰©ç† Table ID"""
        if not table_info:
            return None
            
        typename = table_info.get("__typename")
        table_id = table_info.get("id")
        
        # å¦‚æœæ˜¯ DatabaseTableï¼Œç›´æ¥è¿”å›å…¶ ID
        if typename == "DatabaseTable":
            return table_id
            
        # å¦‚æœæ˜¯ EmbeddedTable æˆ– CustomSQLTableï¼Œå°è¯•ç©¿é€åˆ° upstreamTables
        upstream_tables = table_info.get("upstreamTables") or []
        if upstream_tables and len(upstream_tables) > 0:
            # è¿”å›ç¬¬ä¸€ä¸ªä¸Šæ¸¸ç‰©ç†è¡¨çš„ ID (é€šå¸¸æ˜¯ DatabaseTable)
            # æ³¨æ„ï¼šupstreamTables å¯èƒ½è¿”å›å¤šä¸ªï¼Œé€šå¸¸å–ç¬¬ä¸€ä¸ª
            return upstream_tables[0].get("id")
            
        return table_id

    
    def sync_calculated_fields(self) -> int:
        """åŒæ­¥è®¡ç®—å­—æ®µ"""
        print("\nğŸ“ åŒæ­¥è®¡ç®—å­—æ®µ...")
        self._start_sync_log("calculated_fields")
        
        try:
            calc_fields = self.client.fetch_calculated_fields()
            count = 0
            
            for cf_data in calc_fields:
                if not cf_data or not cf_data.get("id"):
                    continue

                # ğŸ’¡ å»é‡æ£€æŸ¥ï¼šå¦‚æœè¯¥å­—æ®µå·²åœ¨ fields åŒæ­¥é˜¶æ®µè¢«åˆ¤å®šä¸ºé‡å¤å¹¶è·³è¿‡ï¼Œåˆ™åœ¨æ­¤ä¸å†å¤„ç†
                if hasattr(self, 'deduplication_map') and cf_data["id"] in self.deduplication_map:
                    continue
                
                # å…ˆç¡®ä¿ Field è®°å½•å­˜åœ¨
                field = self.session.query(Field).filter_by(id=cf_data["id"]).first()
                if not field:
                    field = Field(id=cf_data["id"])
                    self.session.add(field)
                
                field.name = cf_data.get("name") or ""
                field.description = cf_data.get("description") or ""
                field.data_type = cf_data.get("dataType") or ""
                field.is_calculated = True
                field.formula = cf_data.get("formula") or ""
                field.role = (cf_data.get("role") or "").lower()
                if not field.datasource_id:
                    field.datasource_id = cf_data.get("datasource_id")
                
                # æ›´æ–°/åˆ›å»º CalculatedField è®°å½•
                calc_field = self.session.query(CalculatedField).filter_by(
                    id=cf_data["id"]
                ).first()
                if not calc_field:
                    calc_field = CalculatedField(id=cf_data["id"])
                    self.session.add(calc_field)
                
                calc_field.name = cf_data.get("name") or ""
                calc_field.formula = cf_data.get("formula") or ""
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªè®¡ç®—å­—æ®µ")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_field_to_view(self) -> int:
        """åŒæ­¥å­—æ®µåˆ°è§†å›¾çš„å…³è”å…³ç³»ï¼ˆå«æ™ºèƒ½é‡è¿ï¼‰"""
        print("\nğŸ”— åŒæ­¥å­—æ®µâ†’è§†å›¾å…³è”...")
        self._start_sync_log("field_to_view")
        
        try:
            # 1. æ¸…ç†æ—§æ•°æ® (å…¨é‡åŒæ­¥ç­–ç•¥)
            # ç”±äºæˆ‘ä»¬åšäº†å»é‡ï¼Œå¿…é¡»æ¸…é™¤æ—§çš„å¯èƒ½æŒ‡å‘æ— æ•ˆIDçš„é“¾æ¥
            self.session.execute(text("DELETE FROM field_to_view"))
            self.session.commit()
            print("  ğŸ§¹ å·²æ¸…ç©ºæ—§çš„å­—æ®µå…³è”å…³ç³»")

            view_fields = self.client.fetch_views_with_fields()
            
            # 2. å‡†å¤‡æŸ¥æ‰¾ç¼“å­˜ (Name + Datasource -> FieldID)
            # ç”¨äºå½“åŸå§‹ field_id æ˜¯åµŒå…¥å¼å‰¯æœ¬ï¼ˆå·²è¢«å»é‡ï¼‰æ—¶ï¼Œæ‰¾å›å·²å‘å¸ƒçš„çœŸèº«
            from backend.models import Datasource
            print("  - æ„å»ºå­—æ®µæŸ¥æ‰¾ç¼“å­˜...")
            
            # è·å–æ‰€æœ‰å­—æ®µä¿¡æ¯: (datasource_id, name) -> field_id
            # ä¿®æ­£ï¼šåŠ è½½æ‰€æœ‰å­—æ®µï¼ˆåŒ…æ‹¬åµŒå…¥å¼ï¼‰ï¼Œé¿å…å› è¿‡æ»¤å¯¼è‡´æœ‰æ•ˆå…³è”è¢«ä¸¢å¼ƒ
            published_fields_map = {} 
            result = self.session.execute(
                select(Field.id, Field.name, Field.datasource_id)
            ).fetchall()
            
            for fid, fname, fdsid in result:
                if fdsid and fname:
                    published_fields_map[(fdsid, fname)] = fid
            
            # ... (ä¸­é—´æ³¨é‡Šçœç•¥)
            
            count = 0
            relinked_count = 0
            skipped = 0
            
            # ç¼“å­˜æœ‰æ•ˆå­—æ®µIDé›†åˆ
            valid_field_ids = set([r[0] for r in result])
            
            # ä¸ºäº†å¤„ç†åµŒå…¥å¼æ•°æ®æºID -> å‘å¸ƒå¼IDï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¾…åŠ©æ˜ å°„
            # å› ä¸º view_fields è¿”å›çš„æ•°æ®ä¸­ï¼Œfield å¾€å¾€å¸¦ç€åµŒå…¥å¼ DS ID
            # æˆ‘ä»¬éœ€è¦æ„å»º: embedded_ds_id -> published_ds_id
            # è¿™å¯ä»¥é€šè¿‡ "fetch_fields" çš„é€»è¾‘å¤ç°ï¼Œæˆ–è€…æ›´ç®€å•åœ°ï¼š
            # åœ¨ sync_fields é˜¶æ®µæ²¡æœ‰æŒä¹…åŒ–è¿™ä¸ªæ˜ å°„æœ‰ç‚¹å¯æƒœã€‚
            # è¡¥æ•‘ç­–ç•¥ï¼š
            # å¦‚æœç›´æ¥æ‰¾ä¸åˆ° IDï¼Œå°è¯•ç”¨ (ä»»ä½•å‘å¸ƒå¼DS, name) åŒ¹é…ï¼Ÿä¸ï¼Œå¤ªå®½æ³›ã€‚
            # æˆ‘ä»¬å¯ä»¥å°è¯•åŒ¹é… (view.workbook -> upstreamDatasource, name)
            
            # æ„å»º Workbook -> Published Datasources æ˜ å°„
            wb_ds_map = {}
            wb_ds_rels = self.session.execute(
                select(datasource_to_workbook.c.workbook_id, datasource_to_workbook.c.datasource_id)
            ).fetchall()
            for wbid, dsid in wb_ds_rels:
                if wbid not in wb_ds_map:
                    wb_ds_map[wbid] = []
                wb_ds_map[wbid].append(dsid)

            for vf in view_fields:
                field_id = vf.get("field_id")
                field_name = vf.get("field_name")
                view_id = vf.get("view_id")
                workbook_id = vf.get("workbook_id") # éœ€è¦ fetch_views_with_fields è¿”å› workbook_id
                
                if not field_id or not view_id:
                    skipped += 1
                    continue
                
                final_field_id = field_id
                
                # æ£€æŸ¥IDæ˜¯å¦æœ‰æ•ˆ
                if field_id not in valid_field_ids:
                    # ID æ— æ•ˆï¼ˆå¯èƒ½æ˜¯è¢«å»é‡çš„åµŒå…¥å¼å­—æ®µï¼‰
                    found_new_id = None  # åˆå§‹åŒ–å˜é‡
                    
                    # ç­–ç•¥1: æ£€æŸ¥å»é‡æ˜ å°„è¡¨ (Deduplication Map) - æœ€å‡†ç¡®
                    # è¿™æ˜¯æˆ‘ä»¬åœ¨ sync_fields é˜¶æ®µè®°å½•çš„ "Skipped ID -> Survivor ID"
                    if field_id in self.deduplication_map:
                        final_field_id = self.deduplication_map[field_id]
                        
                        # å†æ¬¡æ£€æŸ¥ map å‡ºæ¥çš„ id æ˜¯å¦æœ‰æ•ˆ (é˜²æ­¢é“¾å¼å»é‡æˆ– survivor ä¹Ÿè¢«åˆ é™¤)
                        if final_field_id in valid_field_ids:
                             relinked_count += 1
                             # ç»§ç»­æ‰§è¡Œæ’å…¥ï¼Œè·³è¿‡åç»­åŒ¹é…é€»è¾‘
                        else:
                             # æ˜ å°„çš„ç›®æ ‡ä¹Ÿæ— æ•ˆï¼Ÿå°è¯•ç­–ç•¥2
                             pass
                    
                    # ç­–ç•¥2: å°è¯•æ™ºèƒ½é‡è¿ (Name åŒ¹é…) - ä»…å½“ç­–ç•¥1æœªæˆåŠŸæ—¶
                    if final_field_id not in valid_field_ids:
                        if workbook_id and field_name and workbook_id in wb_ds_map:
                            potential_ds_ids = wb_ds_map[workbook_id]
                            for p_ds_id in potential_ds_ids:
                                key = (p_ds_id, field_name)
                                if key in published_fields_map:
                                    found_new_id = published_fields_map[key]
                                    break
                        
                        if found_new_id:
                            final_field_id = found_new_id
                            relinked_count += 1
                        else:
                            # ç¡®å®æ‰¾ä¸åˆ°ï¼Œæ”¾å¼ƒ
                            skipped += 1
                            continue
                
                # æ’å…¥å…³è” (æ‰¹é‡æ’å…¥ä¼˜åŒ–å¯ç•™å¾…åç»­ï¼Œç›®å‰å•æ¡æ’å…¥å¹¶å¿½ç•¥é”™è¯¯)
                try:
                    self.session.execute(
                        field_to_view.insert().values(
                            field_id=final_field_id,
                            view_id=view_id,
                            used_in_formula=False
                        )
                    )
                    count += 1
                except Exception as e:
                    # å¯èƒ½æ˜¯ä¸»é”®å†²çªï¼ˆå¦‚æœé€»è¾‘æœ‰è¯¯å¯¼è‡´é‡å¤æ’å…¥ï¼‰
                    skipped += 1
                    continue
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µâ†’è§†å›¾å…³è” (é‡è¿ {relinked_count} ä¸ª, è·³è¿‡ {skipped} ä¸ª)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_users(self) -> int:
        """åŒæ­¥ Tableau ç”¨æˆ·"""
        print("\nğŸ‘¥ åŒæ­¥ç”¨æˆ·...")
        self._start_sync_log("users")
        
        try:
            users = self.client.fetch_users()
            count = 0
            
            for u_data in users:
                if not u_data or not u_data.get("id"):
                    continue
                    
                user = self.session.query(TableauUser).filter_by(id=u_data["id"]).first()
                if not user:
                    user = TableauUser(id=u_data["id"])
                    self.session.add(user)
                
                user.luid = u_data.get("luid")
                user.name = u_data.get("username") or u_data.get("name") or ""
                user.display_name = u_data.get("name")
                user.email = u_data.get("email")
                user.domain = u_data.get("domain")
                user.site_role = u_data.get("siteRole")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªç”¨æˆ·")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_projects(self) -> int:
        """åŒæ­¥ Tableau é¡¹ç›®"""
        print("\nğŸ“ åŒæ­¥é¡¹ç›®...")
        self._start_sync_log("projects")
        
        try:
            projects = self.client.fetch_projects()
            count = 0
            
            for p_data in projects:
                if not p_data or not p_data.get("name"):
                    continue
                
                project_id = p_data.get("id")
                
                project = self.session.query(Project).filter_by(id=project_id).first()
                if not project:
                    project = Project(id=project_id)
                    self.session.add(project)
                
                project.name = p_data.get("name") or ""
                project.vizportal_url_id = p_data.get("vizportalUrlId")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªé¡¹ç›®")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def sync_lineage(self) -> int:
        """åŒæ­¥æŒ‡æ ‡ä¸è¡€ç¼˜å…³ç³» (DBæŒä¹…åŒ–)"""
        print("\nğŸ•¸ï¸ åŒæ­¥è¡€ç¼˜ä¸æŒ‡æ ‡å…³ç³»...")
        count = 0
        
        try:
            # 1. æ¸…ç†ç°æœ‰ä¾èµ–å…³ç³» (å…¨é‡åŒæ­¥ç­–ç•¥)
            self.session.query(FieldDependency).delete()
            self.session.query(Metric).delete() # é‡æ–°æ„å»ºæŒ‡æ ‡è¡¨
            self.session.commit()
            
            # 2. è·å–æ‰€æœ‰è®¡ç®—å­—æ®µ
            # æ³¨æ„ï¼šCalculatedField æ˜¯ç‹¬ç«‹è¡¨ï¼Œä¸å†éœ€è¦ join Field
            calc_fields = self.session.query(CalculatedField).all()
            
            # æ„å»ºå­—æ®µç´¢å¼• (Name -> ID lookup cache)
            all_fields = self.session.query(Field).all()
            field_map = {} # (datasource_id, name) -> field_id
            global_field_map = {} # name -> field_id (fallback)
            
            for f in all_fields:
                key = (f.datasource_id, f.name)
                field_map[key] = f.id
                global_field_map[f.name] = f.id
            
            for calc in calc_fields:
                formula = calc.formula
                if not formula:
                    continue
                    
                # A. è¯†åˆ« Metric
                # è§„åˆ™: è®¡ç®—å­—æ®µ ä¸” Role=Measure
                if calc.role == 'measure':
                    metric = Metric(
                        id=calc.id,
                        name=calc.name,
                        description=calc.description,
                        formula=formula,
                        metric_type='Calculated',
                        owner=None # æš‚ä¸è·å– Owner
                    )
                    self.session.merge(metric)
                
                # B. è§£æä¾èµ– (åç«¯æŒä¹…åŒ–)
                refs = re.findall(r'\[(.*?)\]', formula)
                unique_refs = set(refs)
                
                for ref_name in unique_refs:
                    dep_id = None
                    
                    # 1. å°è¯•åŒæ•°æ®æºåŒ¹é…
                    if calc.datasource_id:
                        dep_id = field_map.get((calc.datasource_id, ref_name))
                    
                    # 2. å°è¯•å…¨å±€åŒ¹é…
                    if not dep_id:
                        dep_id = global_field_map.get(ref_name)
                    
                    # 3. åˆ›å»ºä¾èµ–è®°å½•
                    dependency = FieldDependency(
                        source_field_id=calc.id,
                        dependency_field_id=dep_id, 
                        dependency_name=ref_name,
                        dependency_type='formula'
                    )
                    self.session.add(dependency)
                    count += 1
            
            self.session.commit()
            print(f"  âœ… åŒæ­¥ {count} æ¡ä¾èµ–å…³ç³»")
            return count
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ è¡€ç¼˜åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _link_datasource_to_workbook(self, datasource_id: str, workbook_id: str):
        """å»ºç«‹æ•°æ®æºä¸å·¥ä½œç°¿çš„å…³è”"""
        rel = self.session.execute(
            select(datasource_to_workbook).where(
                datasource_to_workbook.c.datasource_id == datasource_id,
                datasource_to_workbook.c.workbook_id == workbook_id
            )
        ).first()
        if not rel:
            try:
                self.session.execute(
                    datasource_to_workbook.insert().values(
                        datasource_id=datasource_id,
                        workbook_id=workbook_id
                    )
                )
            except:
                pass

    def _sync_field(self, f_data: Dict, datasource_id: str = None, workbook_id: str = None):
        """åŒæ­¥å•ä¸ªå­—æ®µï¼ˆåµŒå…¥å¼æˆ–å‘å¸ƒï¼‰"""
        if not f_data or not f_data.get("id"):
            return

        field = self.session.query(Field).filter_by(id=f_data["id"]).first()
        if not field:
            field = Field(id=f_data["id"])
            self.session.add(field)
        
        field.name = f_data.get("name") or ""
        field.description = f_data.get("description") or ""
        field.datasource_id = datasource_id
        field.workbook_id = workbook_id
        
        # é»˜è®¤å€¼
        if not field.data_type: field.data_type = ""
        if not field.role: field.role = ""
        field.is_calculated = False
        if not field.formula: field.formula = ""
        field.is_hidden = False
        field.folder_name = f_data.get("folderName")
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µ
        typename = f_data.get("__typename")
        # æŸäº› embedded field å¯èƒ½æ²¡æœ‰ __typenameï¼Œå°è¯•æ¨æ–­æˆ–è¯»å–ç›´æ¥å±æ€§
        if typename == "CalculatedField" or f_data.get("formula"):
            field.is_calculated = True
            field.formula = f_data.get("formula") or ""
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = (f_data.get("role") or field.role or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            
            # ç¡®ä¿ CalculatedField è®°å½•
            calc_sub = self.session.query(CalculatedField).filter_by(id=field.id).first()
            if not calc_sub:
                calc_sub = CalculatedField(id=field.id)
                self.session.add(calc_sub)
            calc_sub.name = field.name
            calc_sub.formula = field.formula

        elif typename == "ColumnField" or f_data.get("remoteType"):
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = (f_data.get("role") or field.role or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            # åµŒå…¥å¼åˆ—é€šå¸¸æ²¡æœ‰ upstreamColumns å› ä¸ºå®ƒæ˜¯ç›´æ¥è¿æ¥

    
    def sync_all(self):
        """å…¨é‡åŒæ­¥æ‰€æœ‰å®ä½“"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ Tableau Metadata")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # æŒ‰ä¾èµ–é¡ºåºåŒæ­¥
        user_count = self.sync_users()  # å…ˆåŒæ­¥ç”¨æˆ·
        project_count = self.sync_projects()  # åŒæ­¥é¡¹ç›®
        db_count = self.sync_databases()
        table_count = self.sync_tables()
        ds_count = self.sync_datasources()
        wb_count = self.sync_workbooks()
        field_count = self.sync_fields()
        calc_count = self.sync_calculated_fields()
        calc_count = self.sync_calculated_fields()
        ftv_count = self.sync_field_to_view()
        lineage_count = self.sync_lineage()

        # è‡ªåŠ¨æ‰§è¡Œå››è¡¨æ¶æ„è¿ç§»ä¸ç»Ÿè®¡æ›´æ–°
        print("-" * 30)
        print("ğŸ›  è‡ªåŠ¨è§¦å‘ V5 æ•°æ®è¿ç§»ä¸ç»Ÿè®¡...")
        try:
            # ç¡®ä¿å½“å‰ä¼šè¯å·²æäº¤ï¼Œé¿å…é”ç«äº‰
            self.session.commit()
            split_fields_table_v5.main()
        except Exception as e:
            print(f"âŒ V5 è¿ç§»å¤±è´¥: {e}")
        
        duration = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print("ğŸ“ˆ åŒæ­¥å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"  ç”¨æˆ·:   {user_count}")
        print(f"  é¡¹ç›®:   {project_count}")
        print(f"  æ•°æ®åº“: {db_count}")
        print(f"  æ•°æ®è¡¨: {table_count}")
        print(f"  æ•°æ®æº: {ds_count}")
        print(f"  å·¥ä½œç°¿: {wb_count}")
        print(f"  è¡€ç¼˜:   {lineage_count}")
        print(f"  å­—æ®µ:   {field_count}")
        print(f"  è®¡ç®—å­—æ®µ: {calc_count}")
        print(f"  å­—æ®µâ†’è§†å›¾: {ftv_count}")
        print(f"  è€—æ—¶: {duration:.2f} ç§’")
        print("=" * 60)
        
        # åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰
        self.sync_views_usage()
        
        # æœ€åï¼šè®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ
        self.calculate_stats()
    
    def sync_views_usage(self) -> int:
        """åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰å¹¶è®°å½•å†å²å¿«ç…§"""
        print("\nğŸ“Š åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ (REST API)...")
        
        try:
            usage_map = self.client.fetch_views_usage()
            
            if not usage_map:
                print("  âš ï¸ æœªè·å–åˆ°è§†å›¾ä½¿ç”¨ç»Ÿè®¡")
                return 0
            
            updated = 0
            history_count = 0
            views = self.session.query(View).all()
            
            for view in views:
                # REST API è¿”å›çš„æ˜¯ luidï¼Œéœ€è¦åŒ¹é…
                if view.luid and view.luid in usage_map:
                    new_count = usage_map[view.luid]
                    
                    # åªæœ‰å½“è®¿é—®æ¬¡æ•°å‘ç”Ÿå˜åŒ–æ—¶æ‰è®°å½•å†å²
                    if view.total_view_count != new_count:
                        # è®°å½•å†å²å¿«ç…§
                        from backend.models import ViewUsageHistory
                        history = ViewUsageHistory(
                            view_id=view.id,
                            view_luid=view.luid,
                            total_view_count=new_count
                        )
                        self.session.add(history)
                        history_count += 1
                    
                    view.total_view_count = new_count
                    updated += 1
            
            self.session.commit()
            print(f"  âœ… æ›´æ–° {updated} ä¸ªè§†å›¾çš„ä½¿ç”¨ç»Ÿè®¡, è®°å½• {history_count} æ¡å†å²")
            
            # å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
            self._propagate_dashboard_views()
            
            return updated
            
        except Exception as e:
            print(f"  âŒ åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _propagate_dashboard_views(self):
        """å°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
        
        é€»è¾‘ï¼šå¦‚æœä¸€ä¸ªsheetè¢«åŒ…å«åœ¨ä»ªè¡¨ç›˜ä¸­ï¼Œç”¨æˆ·è®¿é—®ä»ªè¡¨ç›˜æ—¶ä¹Ÿç›¸å½“äºè®¿é—®äº†è¿™äº›sheetã€‚
        å› æ­¤ï¼Œå°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡å®Œæ•´ç´¯åŠ åˆ°æ¯ä¸ªåŒ…å«çš„sheetä¸Šã€‚
        """
        print("  ğŸ“ˆ å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°åŒ…å«çš„sheet...")
        
        try:
            from sqlalchemy import text
            
            # ä½¿ç”¨ SQL ç›´æ¥æ›´æ–°ï¼Œæ›´é«˜æ•ˆ
            # å¯¹äºæ¯ä¸ªè¢«ä»ªè¡¨ç›˜åŒ…å«çš„sheetï¼Œç´¯åŠ æ‰€æœ‰åŒ…å«å®ƒçš„ä»ªè¡¨ç›˜çš„è®¿é—®é‡
            update_sql = text("""
                UPDATE views
                SET total_view_count = COALESCE(total_view_count, 0) + COALESCE((
                    SELECT SUM(COALESCE(d.total_view_count, 0))
                    FROM dashboard_to_sheet ds
                    JOIN views d ON ds.dashboard_id = d.id
                    WHERE ds.sheet_id = views.id
                ), 0)
                WHERE id IN (SELECT DISTINCT sheet_id FROM dashboard_to_sheet)
            """)
            
            result = self.session.execute(update_sql)
            self.session.commit()
            
            # ç»Ÿè®¡å—å½±å“çš„sheetæ•°é‡
            affected = self.session.execute(text(
                "SELECT COUNT(DISTINCT sheet_id) FROM dashboard_to_sheet"
            )).scalar()
            
            print(f"  âœ… å·²å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ° {affected} ä¸ªsheet")
            
        except Exception as e:
            print(f"  âš ï¸ åˆ†æ‘Šä»ªè¡¨ç›˜è®¿é—®é‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def calculate_stats(self):
        """è®¡ç®—å¹¶æ›´æ–°é¢„å­˜ç»Ÿè®¡å­—æ®µï¼ˆåŒæ­¥ç»“æŸåè°ƒç”¨ï¼‰"""
        print("\nğŸ“Š è®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ...")
        
        try:
            # ========== Workbook ç»Ÿè®¡ ==========
            workbooks = self.session.query(Workbook).all()
            for wb in workbooks:
                wb.view_count = len(wb.views) if wb.views else 0
                wb.datasource_count = len(wb.datasources) if wb.datasources else 0
                
                # ç»Ÿè®¡å­—æ®µå’ŒæŒ‡æ ‡ï¼ˆæ’é™¤åµŒå…¥å¼æ•°æ®æºä¸­çš„é‡å¤ï¼‰
                field_ids = set()
                metric_ids = set()
                
                # æ–¹æ¡ˆ1ï¼šé€šè¿‡å…³è”çš„æ•°æ®æºç»Ÿè®¡ï¼ˆæ›´å‡†ç¡®ä¸”åŒ…å«æœªå¼•ç”¨çš„èµ„äº§ï¼‰
                for ds in (wb.datasources or []):
                    # ä»…ç»Ÿè®¡éåµŒå…¥å¼æ•°æ®æºï¼Œé™¤éå·¥ä½œç°¿æœ¬èº«æ²¡æœ‰å‘å¸ƒå¼æ•°æ®æº
                    if ds.is_embedded and len([d for d in wb.datasources if not d.is_embedded]) > 0:
                        continue
                        
                    for f in (ds.fields or []):
                        if f.is_calculated:
                            if f.role == 'measure' or f.role is None:
                                metric_ids.add(f.id)
                        else:
                            field_ids.add(f.id)
                
                # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°è§†å›¾å¼•ç”¨ï¼ˆå¦‚æœä¸Šè¿°ä¸ºç©ºï¼‰
                if len(field_ids) == 0 and len(metric_ids) == 0:
                    for v in (wb.views or []):
                        for f in (v.fields or []):
                            if f.is_calculated:
                                if f.role == 'measure' or f.role is None:
                                    metric_ids.add(f.id)
                            else:
                                field_ids.add(f.id)
                
                wb.field_count = len(field_ids)
                wb.metric_count = len(metric_ids)

            
            # ========== Datasource ç»Ÿè®¡ ==========
            datasources = self.session.query(Datasource).all()
            for ds in datasources:
                ds.table_count = len(ds.tables) if ds.tables else 0
                ds.workbook_count = len(ds.workbooks) if ds.workbooks else 0
                
                field_count = 0
                metric_count = 0
                for f in (ds.fields or []):
                    if f.is_calculated:
                        if f.role == 'measure' or f.role is None:
                            metric_count += 1
                    else:
                        field_count += 1
                ds.field_count = field_count
                ds.metric_count = metric_count
            
            # ========== Field & CalculatedField æ·±åº¦ç»Ÿè®¡ (æŒ‡æ ‡é¢„è®¡ç®—ä¼˜åŒ–) ==========
            print("  - è®¡ç®—å­—æ®µå’ŒæŒ‡æ ‡æ·±åº¦ç»Ÿè®¡...")
            
            # 1. è®¡ç®—å­—æ®µå…¬å¼å“ˆå¸ŒåŠæŸ¥é‡
            calc_fields = self.session.query(CalculatedField).all()
            formula_map = defaultdict(list)
            for cf in calc_fields:
                if cf.formula:
                    # æ ‡å‡†åŒ–å…¬å¼å¹¶è®¡ç®—å“ˆå¸Œ
                    formula_clean = cf.formula.strip()
                    h = hashlib.md5(formula_clean.encode('utf-8')).hexdigest()
                    cf.formula_hash = h
                    formula_map[h].append(cf)
            
            # æ›´æ–°æŸ¥é‡ä¿¡æ¯
            for h, cfs in formula_map.items():
                is_duplicate = len(cfs) > 1
                for cf in cfs:
                    cf.has_duplicates = is_duplicate
                    cf.duplicate_count = len(cfs) - 1
            
            # 2. ç»Ÿè®¡å­—æ®µè¢«è§†å›¾å¼•ç”¨çš„æ¬¡æ•° (usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°è§†å›¾å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET usage_count = (
                    SELECT COUNT(*) FROM field_to_view 
                    WHERE field_to_view.field_id = fields.id
                )
            """))
            
            # 3. ç»Ÿè®¡å­—æ®µè¢«æŒ‡æ ‡å¼•ç”¨çš„æ¬¡æ•° (metric_usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET metric_usage_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_name = fields.name
                )
            """))
            
            # 4. ç»Ÿè®¡æŒ‡æ ‡ä¾èµ–æ•° (dependency_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡ä¾èµ–æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET dependency_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.source_field_id = calculated_fields.id
                )
            """))

            # 5. ç»Ÿè®¡æŒ‡æ ‡å¼•ç”¨æ•° (reference_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET reference_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_field_id = calculated_fields.id
                )
            """))

            self.session.commit()
            print(f"  âœ… å·²æ›´æ–° {len(workbooks)} ä¸ªå·¥ä½œç°¿, {len(datasources)} ä¸ªæ•°æ®æº, {len(calc_fields)} ä¸ªè®¡ç®—å­—æ®µçš„ç»Ÿè®¡å­—æ®µ")
            
            # ========== é¢„è®¡ç®—å®Œæ•´è¡€ç¼˜é“¾ (field_full_lineage) ==========
            print("  - é¢„è®¡ç®—å®Œæ•´è¡€ç¼˜é“¾...")
            self._compute_full_lineage()
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _compute_full_lineage(self):
        """é¢„è®¡ç®—æ‰€æœ‰å­—æ®µçš„å®Œæ•´è¡€ç¼˜é“¾å¹¶å­˜å…¥ field_full_lineage è¡¨
        
        ä¿®å¤ç‰ˆï¼šé€šè¿‡ datasource_to_workbook æ¨å¯¼å­—æ®µçš„å·¥ä½œç°¿å…³è”ï¼Œ
        è§£å†³å‘å¸ƒæ•°æ®æºå­—æ®µ workbook_id ä¸º NULL å¯¼è‡´è¡€ç¼˜ä¸¢å¤±çš„é—®é¢˜ã€‚
        """
        from backend.models import FieldFullLineage, Field, Datasource
        
        try:
            # æ¸…ç©ºæ—§æ•°æ®
            self.session.execute(text("DELETE FROM field_full_lineage"))
            
            # æ„å»ºæ•°æ®æº -> ç‰©ç†è¡¨çš„æ˜ å°„
            ds_table_map = {}  # datasource_id -> [table_ids]
            result = self.session.execute(text(
                "SELECT datasource_id, table_id FROM table_to_datasource"
            )).fetchall()
            for ds_id, tbl_id in result:
                if ds_id not in ds_table_map:
                    ds_table_map[ds_id] = []
                ds_table_map[ds_id].append(tbl_id)
            
            # æ„å»ºæ•°æ®æº -> å·¥ä½œç°¿çš„æ˜ å°„ (æ ¸å¿ƒä¿®å¤)
            ds_workbook_map = {}  # datasource_id -> [workbook_ids]
            result = self.session.execute(text(
                "SELECT datasource_id, workbook_id FROM datasource_to_workbook"
            )).fetchall()
            for ds_id, wb_id in result:
                if ds_id not in ds_workbook_map:
                    ds_workbook_map[ds_id] = []
                ds_workbook_map[ds_id].append(wb_id)
            
            # éå†æ‰€æœ‰å­—æ®µ
            fields = self.session.query(Field).all()
            lineage_records = []
            
            for f in fields:
                # ç¡®å®šæ‰€æœ‰å…³è”çš„å·¥ä½œç°¿ (æ ¸å¿ƒä¿®å¤é€»è¾‘)
                workbook_ids = set()
                if f.workbook_id:
                    workbook_ids.add(f.workbook_id)
                # é€šè¿‡æ•°æ®æºæ¨å¯¼å·¥ä½œç°¿
                if f.datasource_id and f.datasource_id in ds_workbook_map:
                    for wb_id in ds_workbook_map[f.datasource_id]:
                        workbook_ids.add(wb_id)
                # å¦‚æœæ²¡æœ‰ä»»ä½•å·¥ä½œç°¿ï¼Œä»è®°å½•ä¸€æ¡ (workbook_id=None)
                if not workbook_ids:
                    workbook_ids.add(None)
                
                if not f.is_calculated:
                    # åŸå§‹å­—æ®µ: ç›´æ¥è¡€ç¼˜
                    table_ids = []
                    if f.table_id:
                         # ä¿®æ­£ï¼šå³ä½¿æ˜¯åŸå§‹å­—æ®µï¼Œä¹Ÿè¦éªŒè¯ table_id æ˜¯å¦æœ‰æ•ˆï¼ˆå­˜åœ¨äº DBTableï¼‰
                         # é¿å…é‡æŒ‡é’ˆ
                         # ä½†è€ƒè™‘åˆ°æ€§èƒ½ï¼Œè¿™é‡Œå‡è®¾ DB çº¦æŸæˆ– sync é€»è¾‘ä¿è¯äº† table_id æœ‰æ•ˆï¼Œæˆ–è€…å·¦è¿æ¥æŸ¥è¯¢æ—¶è‡ªç„¶è¿‡æ»¤
                         table_ids = [f.table_id]
                    elif f.datasource_id and f.datasource_id in ds_table_map:
                        table_ids = ds_table_map[f.datasource_id]
                    
                    if table_ids:
                        for tbl_id in table_ids:
                            for wb_id in workbook_ids:
                                lineage_records.append({
                                    'field_id': f.id,
                                    'table_id': tbl_id,
                                    'datasource_id': f.datasource_id,
                                    'workbook_id': wb_id,
                                    'lineage_type': 'direct',
                                    'lineage_path': 'Field -> DS -> Table'
                                })
                    else:
                        # æ— ç‰©ç†è¡¨å…³è”
                        for wb_id in workbook_ids:
                            lineage_records.append({
                                'field_id': f.id,
                                'table_id': None,
                                'datasource_id': f.datasource_id,
                                'workbook_id': wb_id,
                                'lineage_type': 'direct',
                                'lineage_path': 'Field -> DS (no table)'
                            })
                else:
                    # è®¡ç®—å­—æ®µ: é—´æ¥è¡€ç¼˜
                    table_ids = []
                    # 1. ä¼˜å…ˆä½¿ç”¨å­—æ®µè‡ªèº«çš„ table_id (å¦‚æœåœ¨ sync_fields ä¸­å·²ä¿®å¤)
                    if f.table_id:
                        table_ids = [f.table_id]
                    # 2. å…¶æ¬¡ä½¿ç”¨æ•°æ®æºå…³è”çš„è¡¨
                    elif f.datasource_id and f.datasource_id in ds_table_map:
                        table_ids = ds_table_map[f.datasource_id]
                    
                    if table_ids:
                        for tbl_id in table_ids:
                            for wb_id in workbook_ids:
                                lineage_records.append({
                                    'field_id': f.id,
                                    'table_id': tbl_id,
                                    'datasource_id': f.datasource_id,
                                    'workbook_id': wb_id,
                                    'lineage_type': 'indirect',
                                    'lineage_path': 'CalcField -> DS -> Table'
                                })
                    else:
                        for wb_id in workbook_ids:
                            lineage_records.append({
                                'field_id': f.id,
                                'table_id': None,
                                'datasource_id': f.datasource_id,
                                'workbook_id': wb_id,
                                'lineage_type': 'indirect',
                                'lineage_path': 'CalcField -> DS (no table)'
                            })
            
            # æ‰¹é‡æ’å…¥
            if lineage_records:
                self.session.execute(
                    text("""
                        INSERT INTO field_full_lineage 
                        (field_id, table_id, datasource_id, workbook_id, lineage_type, lineage_path)
                        VALUES (:field_id, :table_id, :datasource_id, :workbook_id, :lineage_type, :lineage_path)
                    """),
                    lineage_records
                )
                self.session.commit()
            
            print(f"  âœ… é¢„è®¡ç®— {len(lineage_records)} æ¡å®Œæ•´è¡€ç¼˜è®°å½•")
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ é¢„è®¡ç®—è¡€ç¼˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()
