"""
åŒæ­¥æŠ¥å‘Šç”Ÿæˆæ¨¡å—
åœ¨æ¯æ¬¡ Tableau å…ƒæ•°æ®åŒæ­¥å®Œæˆåç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
"""
import os
import json
from datetime import datetime
from sqlalchemy import text


class SyncReportGenerator:
    """åŒæ­¥æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, session, sync_stats: dict = None):
        """
        Args:
            session: æ•°æ®åº“ä¼šè¯
            sync_stats: åŒæ­¥è¿‡ç¨‹ä¸­æ”¶é›†çš„ç»Ÿè®¡ä¿¡æ¯
        """
        self.session = session
        self.sync_stats = sync_stats or {}
        self.report_data = {}
        
    def generate_report(self, output_dir: str = None) -> dict:
        """ç”Ÿæˆå®Œæ•´çš„åŒæ­¥æŠ¥å‘Š"""
        self.report_data = {
            "report_time": datetime.now().isoformat(),
            "sync_summary": self._get_sync_summary(),
            "module_stats": self._get_module_stats(),
            "module_lineage_details": self._get_module_lineage_details(),  # æ–°å¢ï¼šå„æ¨¡å—è¡€ç¼˜è¯¦æƒ…
            "lineage_stats": self._get_lineage_stats(),
            "label_distribution": self._get_label_distribution(),
            "type_distribution": self._get_type_distribution(),  # æ–°å¢ï¼šç±»å‹åˆ†å¸ƒ
            "coverage_stats": self._get_coverage_stats(),  # æ–°å¢ï¼šè¦†ç›–ç‡ç»Ÿè®¡
            "hot_assets": self._get_hot_assets(),  # æ–°å¢ï¼šçƒ­é—¨èµ„äº§
            "project_distribution": self._get_project_distribution(),  # æ–°å¢ï¼šé¡¹ç›®åˆ†å¸ƒ
            "deduplication_stats": self._get_deduplication_stats(),
            "dedup_top_duplicates": self._get_top_duplicates(),  # æ–°å¢ï¼šé‡å¤æœ€å¤šçš„å­—æ®µ
            "unestablished_lineage": self._get_unestablished_lineage(),
        }
        
        # è¾“å‡ºæŠ¥å‘Š
        if output_dir:
            self._save_report(output_dir)
            self._print_report()
            
        return self.report_data
    
    def _get_sync_summary(self) -> dict:
        """è·å–åŒæ­¥æ¦‚è¦"""
        return {
            "start_time": self.sync_stats.get("start_time"),
            "end_time": self.sync_stats.get("end_time"),
            "duration_seconds": self.sync_stats.get("duration"),
            "status": self.sync_stats.get("status", "completed")
        }
    
    def _get_module_stats(self) -> dict:
        """è·å–å„æ¨¡å—åŒæ­¥ç»Ÿè®¡"""
        result = {}
        
        # ä»æ•°æ®åº“ç›´æ¥ç»Ÿè®¡
        queries = {
            "users": "SELECT COUNT(*) FROM tableau_users",
            "projects": "SELECT COUNT(*) FROM projects",
            "databases": "SELECT COUNT(*) FROM databases",
            "tables": "SELECT COUNT(*) FROM tables",
            "datasources": "SELECT COUNT(*) FROM datasources",
            "workbooks": "SELECT COUNT(*) FROM workbooks",
            "views": "SELECT COUNT(*) FROM views",
            "fields": "SELECT COUNT(*) FROM fields",
            "calculated_fields": "SELECT COUNT(*) FROM calculated_fields",
            "regular_fields": "SELECT COUNT(*) FROM regular_fields",
            "unique_regular_fields": "SELECT COUNT(*) FROM unique_regular_fields",
            "unique_calculated_fields": "SELECT COUNT(*) FROM unique_calculated_fields",
        }
        
        for module_name, sql in queries.items():
            try:
                count = self.session.execute(text(sql)).scalar() or 0
                result[module_name] = {"count": count}
            except Exception as e:
                result[module_name] = {"count": 0, "error": str(e)}
        
        # ä» sync_stats è¡¥å……åŒæ­¥å¢é‡ä¿¡æ¯
        for key in ["user_count", "project_count", "db_count", "table_count", 
                    "ds_count", "wb_count", "field_count", "calc_count", "ftv_count"]:
            if key in self.sync_stats:
                module_name = key.replace("_count", "")
                if module_name in result:
                    result[module_name]["synced"] = self.sync_stats[key]
                    
        return result
    
    def _get_lineage_stats(self) -> dict:
        """è·å–è¡€ç¼˜å»ºç«‹ç»Ÿè®¡"""
        result = {}
        
        # å…³è”è¡¨ç»Ÿè®¡
        association_queries = {
            "table_to_datasource": {
                "count": "SELECT COUNT(*) FROM table_to_datasource",
                "by_source": "SELECT lineage_source, COUNT(*) as cnt FROM table_to_datasource GROUP BY lineage_source"
            },
            "datasource_to_workbook": {
                "count": "SELECT COUNT(*) FROM datasource_to_workbook",
                "by_source": "SELECT lineage_source, COUNT(*) as cnt FROM datasource_to_workbook GROUP BY lineage_source"
            },
            "field_to_view": {
                "count": "SELECT COUNT(*) FROM field_to_view",
                "by_source": "SELECT lineage_source, COUNT(*) as cnt FROM field_to_view GROUP BY lineage_source"
            },
            "dashboard_to_sheet": {
                "count": "SELECT COUNT(*) FROM dashboard_to_sheet",
                "by_source": "SELECT lineage_source, COUNT(*) as cnt FROM dashboard_to_sheet GROUP BY lineage_source"
            },
            "field_dependencies": {
                "count": "SELECT COUNT(*) FROM field_dependencies",
            },
            "calc_field_dependencies": {
                "count": "SELECT COUNT(*) FROM calc_field_dependencies",
            },
            "field_full_lineage": {
                "count": "SELECT COUNT(*) FROM field_full_lineage",
            },
            "regular_field_full_lineage": {
                "count": "SELECT COUNT(*) FROM regular_field_full_lineage",
            },
            "calc_field_full_lineage": {
                "count": "SELECT COUNT(*) FROM calc_field_full_lineage",
            },
        }
        
        for table_name, queries in association_queries.items():
            try:
                count = self.session.execute(text(queries["count"])).scalar() or 0
                item = {"total": count}
                
                if "by_source" in queries:
                    rows = self.session.execute(text(queries["by_source"])).fetchall()
                    item["by_lineage_source"] = {str(r[0] or "null"): r[1] for r in rows}
                    
                result[table_name] = item
            except Exception as e:
                result[table_name] = {"total": 0, "error": str(e)}
                
        return result
    
    def _get_label_distribution(self) -> dict:
        """è·å–è¡€ç¼˜æ ‡ç­¾åˆ†å¸ƒ"""
        result = {}
        
        # fields è¡¨çš„æ ‡ç­¾åˆ†å¸ƒ
        try:
            fields_source = self.session.execute(text(
                "SELECT lineage_source, COUNT(*) FROM fields GROUP BY lineage_source"
            )).fetchall()
            result["fields_by_source"] = {str(r[0] or "null"): r[1] for r in fields_source}
            
            fields_penetration = self.session.execute(text(
                "SELECT penetration_status, COUNT(*) FROM fields GROUP BY penetration_status"
            )).fetchall()
            result["fields_by_penetration"] = {str(r[0] or "null"): r[1] for r in fields_penetration}
        except Exception as e:
            result["fields_error"] = str(e)
            
        # regular_fields è¡¨çš„æ ‡ç­¾åˆ†å¸ƒ
        try:
            rf_source = self.session.execute(text(
                "SELECT lineage_source, COUNT(*) FROM regular_fields GROUP BY lineage_source"
            )).fetchall()
            result["regular_fields_by_source"] = {str(r[0] or "null"): r[1] for r in rf_source}
            
            rf_penetration = self.session.execute(text(
                "SELECT penetration_status, COUNT(*) FROM regular_fields GROUP BY penetration_status"
            )).fetchall()
            result["regular_fields_by_penetration"] = {str(r[0] or "null"): r[1] for r in rf_penetration}
        except Exception as e:
            result["regular_fields_error"] = str(e)
            
        return result
    
    def _get_deduplication_stats(self) -> dict:
        """è·å–å»é‡ç»Ÿè®¡"""
        result = {}
        
        try:
            # åŸå§‹å­—æ®µå»é‡ç»Ÿè®¡
            rf_instances = self.session.execute(text(
                "SELECT COUNT(*) FROM regular_fields"
            )).scalar() or 0
            rf_unique = self.session.execute(text(
                "SELECT COUNT(*) FROM unique_regular_fields"
            )).scalar() or 0
            
            result["regular_fields"] = {
                "total_instances": rf_instances,
                "unique_fields": rf_unique,
                "dedup_ratio": round(1 - rf_unique / rf_instances, 4) if rf_instances > 0 else 0,
                "avg_instances_per_field": round(rf_instances / rf_unique, 2) if rf_unique > 0 else 0
            }
            
            # å»é‡ç­–ç•¥åˆ†å¸ƒ (å¦‚æœæœ‰è®°å½•çš„è¯)
            # è¿™éœ€è¦åœ¨è¿ç§»æ—¶è®°å½•ç­–ç•¥ï¼Œç›®å‰å…ˆç»Ÿè®¡è¡¨å…³è”æƒ…å†µ
            rf_by_table = self.session.execute(text("""
                SELECT 
                    CASE WHEN urf.table_id IS NOT NULL THEN 'has_table' ELSE 'no_table' END as has_tbl,
                    COUNT(*)
                FROM unique_regular_fields urf
                GROUP BY has_tbl
            """)).fetchall()
            result["regular_fields"]["by_table_association"] = {r[0]: r[1] for r in rf_by_table}
            
        except Exception as e:
            result["regular_fields_error"] = str(e)
            
        try:
            # è®¡ç®—å­—æ®µå»é‡ç»Ÿè®¡
            cf_instances = self.session.execute(text(
                "SELECT COUNT(*) FROM calculated_fields"
            )).scalar() or 0
            cf_unique = self.session.execute(text(
                "SELECT COUNT(*) FROM unique_calculated_fields"
            )).scalar() or 0
            
            result["calculated_fields"] = {
                "total_instances": cf_instances,
                "unique_fields": cf_unique,
                "dedup_ratio": round(1 - cf_unique / cf_instances, 4) if cf_instances > 0 else 0,
                "avg_instances_per_field": round(cf_instances / cf_unique, 2) if cf_unique > 0 else 0
            }
            
            # å¤æ‚åº¦åˆ†å¸ƒ
            cf_complexity = self.session.execute(text("""
                SELECT 
                    CASE 
                        WHEN complexity_score < 5 THEN 'simple'
                        WHEN complexity_score < 20 THEN 'medium'
                        ELSE 'complex'
                    END as complexity_level,
                    COUNT(*)
                FROM unique_calculated_fields
                GROUP BY complexity_level
            """)).fetchall()
            result["calculated_fields"]["by_complexity"] = {r[0]: r[1] for r in cf_complexity}
            
        except Exception as e:
            result["calculated_fields_error"] = str(e)
            
        return result
    
    def _get_unestablished_lineage(self) -> dict:
        """è·å–æœªå»ºç«‹è¡€ç¼˜çš„æƒ…å†µ"""
        result = {}
        
        try:
            # æ²¡æœ‰å…³è”æ•°æ®æºçš„å­—æ®µ
            no_ds = self.session.execute(text("""
                SELECT COUNT(*) FROM fields WHERE datasource_id IS NULL
            """)).scalar() or 0
            result["fields_without_datasource"] = no_ds
            
            # æ²¡æœ‰å…³è”è¡¨çš„å­—æ®µ
            no_table = self.session.execute(text("""
                SELECT COUNT(*) FROM fields WHERE table_id IS NULL
            """)).scalar() or 0
            result["fields_without_table"] = no_table
            
            # æ²¡æœ‰å…³è”å·¥ä½œç°¿çš„å­—æ®µ
            no_wb = self.session.execute(text("""
                SELECT COUNT(*) FROM fields WHERE workbook_id IS NULL
            """)).scalar() or 0
            result["fields_without_workbook"] = no_wb
            
            # æ²¡æœ‰è§†å›¾å¼•ç”¨çš„å­—æ®µ
            no_views = self.session.execute(text("""
                SELECT COUNT(*) FROM fields f 
                WHERE NOT EXISTS (SELECT 1 FROM field_to_view ftv WHERE ftv.field_id = f.id)
            """)).scalar() or 0
            result["fields_without_views"] = no_views
            
            # ç©¿é€å¤±è´¥çš„å­—æ®µ
            penetration_failed = self.session.execute(text("""
                SELECT COUNT(*) FROM fields WHERE penetration_status = 'failed'
            """)).scalar() or 0
            result["penetration_failed"] = penetration_failed
            
            # æ²¡æœ‰ä¸Šæ¸¸åˆ—çš„å­—æ®µï¼ˆColumnField åº”è¯¥æœ‰ï¼‰
            no_upstream_col = self.session.execute(text("""
                SELECT COUNT(*) FROM fields 
                WHERE is_calculated = 0 AND upstream_column_id IS NULL
            """)).scalar() or 0
            result["column_fields_without_upstream"] = no_upstream_col
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_module_lineage_details(self) -> dict:
        """è·å–å„æ¨¡å—çš„è¡€ç¼˜å»ºç«‹è¯¦æƒ…"""
        result = {}
        
        try:
            # æ•°æ®åº“è¡€ç¼˜ç»Ÿè®¡
            db_stats = self.session.execute(text("""
                SELECT 
                    COUNT(DISTINCT db.id) as db_count,
                    COUNT(DISTINCT t.id) as tables_with_db,
                    (SELECT COUNT(*) FROM tables WHERE database_id IS NULL) as orphan_tables
                FROM databases db
                LEFT JOIN tables t ON t.database_id = db.id
            """)).first()
            result["databases"] = {
                "total": db_stats[0] if db_stats else 0,
                "linked_tables": db_stats[1] if db_stats else 0,
                "orphan_tables": db_stats[2] if db_stats else 0
            }
            
            # æ•°æ®è¡¨è¡€ç¼˜ç»Ÿè®¡
            table_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN is_embedded = 1 THEN 1 ELSE 0 END) as embedded,
                    SUM(CASE WHEN is_embedded = 0 OR is_embedded IS NULL THEN 1 ELSE 0 END) as physical,
                    (SELECT COUNT(DISTINCT table_id) FROM table_to_datasource) as linked_to_ds
                FROM tables
            """)).first()
            result["tables"] = {
                "total": table_stats[0] if table_stats else 0,
                "embedded": table_stats[1] if table_stats else 0,
                "physical": table_stats[2] if table_stats else 0,
                "linked_to_datasource": table_stats[3] if table_stats else 0
            }
            
            # æ•°æ®æºè¡€ç¼˜ç»Ÿè®¡
            ds_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN is_embedded = 1 THEN 1 ELSE 0 END) as embedded,
                    SUM(CASE WHEN is_certified = 1 THEN 1 ELSE 0 END) as certified,
                    (SELECT COUNT(DISTINCT datasource_id) FROM table_to_datasource) as with_tables,
                    (SELECT COUNT(DISTINCT datasource_id) FROM datasource_to_workbook) as with_workbooks,
                    (SELECT COUNT(DISTINCT datasource_id) FROM fields WHERE datasource_id IS NOT NULL) as with_fields
                FROM datasources
            """)).first()
            result["datasources"] = {
                "total": ds_stats[0] if ds_stats else 0,
                "embedded": ds_stats[1] if ds_stats else 0,
                "certified": ds_stats[2] if ds_stats else 0,
                "with_upstream_tables": ds_stats[3] if ds_stats else 0,
                "with_downstream_workbooks": ds_stats[4] if ds_stats else 0,
                "with_fields": ds_stats[5] if ds_stats else 0
            }
            
            # å·¥ä½œç°¿è¡€ç¼˜ç»Ÿè®¡
            wb_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    (SELECT COUNT(DISTINCT workbook_id) FROM views) as with_views,
                    (SELECT COUNT(DISTINCT workbook_id) FROM datasource_to_workbook) as with_datasources,
                    (SELECT COUNT(DISTINCT workbook_id) FROM fields WHERE workbook_id IS NOT NULL) as with_fields
                FROM workbooks
            """)).first()
            result["workbooks"] = {
                "total": wb_stats[0] if wb_stats else 0,
                "with_views": wb_stats[1] if wb_stats else 0,
                "with_datasources": wb_stats[2] if wb_stats else 0,
                "with_fields": wb_stats[3] if wb_stats else 0
            }
            
            # è§†å›¾è¡€ç¼˜ç»Ÿè®¡
            view_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN view_type = 'dashboard' THEN 1 ELSE 0 END) as dashboards,
                    SUM(CASE WHEN view_type != 'dashboard' OR view_type IS NULL THEN 1 ELSE 0 END) as sheets,
                    (SELECT COUNT(DISTINCT view_id) FROM field_to_view) as with_fields,
                    (SELECT COUNT(DISTINCT sheet_id) FROM dashboard_to_sheet) as sheets_in_dashboards
                FROM views
            """)).first()
            result["views"] = {
                "total": view_stats[0] if view_stats else 0,
                "dashboards": view_stats[1] if view_stats else 0,
                "sheets": view_stats[2] if view_stats else 0,
                "with_field_references": view_stats[3] if view_stats else 0,
                "sheets_included_in_dashboards": view_stats[4] if view_stats else 0
            }
            
            # å­—æ®µè¡€ç¼˜ç»Ÿè®¡
            field_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN is_calculated = 1 THEN 1 ELSE 0 END) as calculated,
                    SUM(CASE WHEN is_calculated = 0 OR is_calculated IS NULL THEN 1 ELSE 0 END) as regular,
                    SUM(CASE WHEN table_id IS NOT NULL THEN 1 ELSE 0 END) as with_table,
                    SUM(CASE WHEN datasource_id IS NOT NULL THEN 1 ELSE 0 END) as with_datasource,
                    SUM(CASE WHEN workbook_id IS NOT NULL THEN 1 ELSE 0 END) as with_workbook,
                    (SELECT COUNT(DISTINCT field_id) FROM field_to_view) as used_in_views
                FROM fields
            """)).first()
            result["fields"] = {
                "total": field_stats[0] if field_stats else 0,
                "calculated": field_stats[1] if field_stats else 0,
                "regular": field_stats[2] if field_stats else 0,
                "with_table": field_stats[3] if field_stats else 0,
                "with_datasource": field_stats[4] if field_stats else 0,
                "with_workbook": field_stats[5] if field_stats else 0,
                "used_in_views": field_stats[6] if field_stats else 0
            }
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_type_distribution(self) -> dict:
        """è·å–å„ç±»å‹åˆ†å¸ƒ"""
        result = {}
        
        try:
            # å­—æ®µè§’è‰²åˆ†å¸ƒ (Dimension vs Measure)
            role_dist = self.session.execute(text("""
                SELECT role, COUNT(*) FROM fields GROUP BY role
            """)).fetchall()
            result["field_roles"] = {str(r[0] or "unknown"): r[1] for r in role_dist}
            
            # å­—æ®µæ•°æ®ç±»å‹åˆ†å¸ƒ
            dtype_dist = self.session.execute(text("""
                SELECT data_type, COUNT(*) FROM fields WHERE data_type IS NOT NULL AND data_type != '' GROUP BY data_type ORDER BY COUNT(*) DESC LIMIT 15
            """)).fetchall()
            result["field_data_types"] = {str(r[0]): r[1] for r in dtype_dist}
            
            # è¡¨ç±»å‹åˆ†å¸ƒ
            table_type_dist = self.session.execute(text("""
                SELECT 
                    CASE WHEN is_embedded = 1 THEN 'embedded' ELSE 'physical' END,
                    COUNT(*)
                FROM tables 
                GROUP BY CASE WHEN is_embedded = 1 THEN 'embedded' ELSE 'physical' END
            """)).fetchall()
            result["table_types"] = {r[0]: r[1] for r in table_type_dist}
            
            # è§†å›¾ç±»å‹åˆ†å¸ƒ
            view_type_dist = self.session.execute(text("""
                SELECT view_type, COUNT(*) FROM views GROUP BY view_type
            """)).fetchall()
            result["view_types"] = {str(r[0] or "sheet"): r[1] for r in view_type_dist}
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_coverage_stats(self) -> dict:
        """è·å–è¦†ç›–ç‡ç»Ÿè®¡"""
        result = {}
        
        try:
            # å­—æ®µæè¿°è¦†ç›–ç‡
            desc_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN description IS NOT NULL AND description != '' THEN 1 ELSE 0 END) as has_desc
                FROM fields
            """)).first()
            total = desc_stats[0] if desc_stats else 0
            has_desc = desc_stats[1] if desc_stats else 0
            result["field_description"] = {
                "total": total,
                "with_description": has_desc,
                "without_description": total - has_desc,
                "coverage_rate": round(has_desc / total, 4) if total > 0 else 0
            }
            
            # æ•°æ®æºæè¿°è¦†ç›–ç‡
            ds_desc = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN description IS NOT NULL AND description != '' THEN 1 ELSE 0 END) as has_desc
                FROM datasources
            """)).first()
            ds_total = ds_desc[0] if ds_desc else 0
            ds_has = ds_desc[1] if ds_desc else 0
            result["datasource_description"] = {
                "total": ds_total,
                "with_description": ds_has,
                "coverage_rate": round(ds_has / ds_total, 4) if ds_total > 0 else 0
            }
            
            # æ•°æ®æºè®¤è¯è¦†ç›–ç‡
            cert_stats = self.session.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN is_certified = 1 THEN 1 ELSE 0 END) as certified
                FROM datasources WHERE is_embedded = 0 OR is_embedded IS NULL
            """)).first()
            cert_total = cert_stats[0] if cert_stats else 0
            certified = cert_stats[1] if cert_stats else 0
            result["datasource_certification"] = {
                "published_total": cert_total,
                "certified": certified,
                "certification_rate": round(certified / cert_total, 4) if cert_total > 0 else 0
            }
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_hot_assets(self) -> dict:
        """è·å–çƒ­é—¨èµ„äº§"""
        result = {}
        
        try:
            # çƒ­é—¨è§†å›¾ Top 10
            hot_views = self.session.execute(text("""
                SELECT name, total_view_count, workbook_id
                FROM views
                WHERE total_view_count > 0
                ORDER BY total_view_count DESC
                LIMIT 10
            """)).fetchall()
            result["hot_views"] = [{"name": r[0], "views": r[1], "workbook_id": r[2]} for r in hot_views]
            
            # é«˜é¢‘ä½¿ç”¨å­—æ®µ Top 10
            hot_fields = self.session.execute(text("""
                SELECT name, (usage_count + COALESCE(metric_usage_count, 0)) as total_usage, datasource_id
                FROM fields
                WHERE usage_count > 0
                ORDER BY total_usage DESC
                LIMIT 10
            """)).fetchall()
            result["hot_fields"] = [{"name": r[0], "usage": r[1], "datasource_id": r[2]} for r in hot_fields]
            
            # è¢«å¼•ç”¨æœ€å¤šçš„æ•°æ®æº Top 10
            hot_ds = self.session.execute(text("""
                SELECT d.name, COUNT(DISTINCT dw.workbook_id) as wb_count, d.id
                FROM datasources d
                LEFT JOIN datasource_to_workbook dw ON d.id = dw.datasource_id
                GROUP BY d.id
                ORDER BY wb_count DESC
                LIMIT 10
            """)).fetchall()
            result["hot_datasources"] = [{"name": r[0], "workbook_count": r[1], "id": r[2]} for r in hot_ds]
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_project_distribution(self) -> dict:
        """è·å–é¡¹ç›®åˆ†å¸ƒ"""
        result = {}
        
        try:
            # å„é¡¹ç›®çš„èµ„äº§æ•°é‡
            project_stats = self.session.execute(text("""
                SELECT 
                    p.name,
                    p.id,
                    (SELECT COUNT(*) FROM datasources WHERE project_name = p.name) as ds_count,
                    (SELECT COUNT(*) FROM workbooks WHERE project_name = p.name) as wb_count
                FROM projects p
                ORDER BY ds_count + wb_count DESC
            """)).fetchall()
            result["by_project"] = [{
                "name": r[0],
                "id": r[1],
                "datasource_count": r[2],
                "workbook_count": r[3],
                "total_assets": r[2] + r[3]
            } for r in project_stats]
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _get_top_duplicates(self) -> dict:
        """è·å–é‡å¤æœ€å¤šçš„å­—æ®µ"""
        result = {}
        
        try:
            # å®ä¾‹æ•°æœ€å¤šçš„æ ‡å‡†å­—æ®µ Top 10
            top_rf = self.session.execute(text("""
                SELECT urf.name, COUNT(rf.id) as instance_count, urf.id
                FROM unique_regular_fields urf
                JOIN regular_fields rf ON rf.unique_id = urf.id
                GROUP BY urf.id
                ORDER BY instance_count DESC
                LIMIT 10
            """)).fetchall()
            result["top_regular_fields"] = [{"name": r[0], "instances": r[1], "id": r[2]} for r in top_rf]
            
            # å®ä¾‹æ•°æœ€å¤šçš„æ ‡å‡†æŒ‡æ ‡ Top 10
            top_cf = self.session.execute(text("""
                SELECT ucf.name, COUNT(cf.id) as instance_count, ucf.id
                FROM unique_calculated_fields ucf
                JOIN calculated_fields cf ON cf.unique_id = ucf.id
                GROUP BY ucf.id
                ORDER BY instance_count DESC
                LIMIT 10
            """)).fetchall()
            result["top_calculated_fields"] = [{"name": r[0], "instances": r[1], "id": r[2]} for r in top_cf]
            
        except Exception as e:
            result["error"] = str(e)
            
        return result
    
    def _save_report(self, output_dir: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ JSON æŠ¥å‘Š
        json_filename = f"sync_report_{timestamp}.json"
        json_filepath = os.path.join(output_dir, json_filename)
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æœ€æ–° JSON æŠ¥å‘Šå‰¯æœ¬
        latest_json_path = os.path.join(output_dir, "sync_report_latest.json")
        with open(latest_json_path, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆå¹¶ä¿å­˜ Markdown æ–‡å­—æŠ¥å‘Š
        md_content = self._generate_markdown_report()
        md_filename = f"sync_report_{timestamp}.md"
        md_filepath = os.path.join(output_dir, md_filename)
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # ä¿å­˜æœ€æ–° Markdown æŠ¥å‘Šå‰¯æœ¬
        latest_md_path = os.path.join(output_dir, "sync_report_latest.md")
        with open(latest_md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   JSON: {json_filepath}")
        print(f"   æ–‡å­—: {md_filepath}")
    
    def _generate_markdown_report(self) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼çš„æ–‡å­—æŠ¥å‘Š"""
        lines = []
        
        # æ ‡é¢˜
        summary = self.report_data.get("sync_summary", {})
        lines.append("# Tableau å…ƒæ•°æ®åŒæ­¥æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {self.report_data.get('report_time', '-')}")
        lines.append(f"**åŒæ­¥çŠ¶æ€**: {summary.get('status', 'unknown')}")
        duration = summary.get('duration_seconds')
        lines.append(f"**åŒæ­¥è€—æ—¶**: {duration:.2f} ç§’" if duration else "**åŒæ­¥è€—æ—¶**: -")
        lines.append("")
        
        # æ¨¡å—åŒæ­¥ç»Ÿè®¡
        lines.append("---")
        lines.append("## ğŸ“¦ æ¨¡å—åŒæ­¥ç»Ÿè®¡")
        lines.append("")
        lines.append("| æ¨¡å— | æ•°é‡ | è¯´æ˜ |")
        lines.append("|------|------|------|")
        
        ms = self.report_data.get("module_stats", {})
        module_names = {
            "users": "ç”¨æˆ·",
            "projects": "é¡¹ç›®",
            "databases": "æ•°æ®åº“",
            "tables": "æ•°æ®è¡¨",
            "datasources": "æ•°æ®æº",
            "workbooks": "å·¥ä½œç°¿",
            "views": "è§†å›¾",
            "fields": "å­—æ®µ (åŸå§‹)",
            "calculated_fields": "è®¡ç®—å­—æ®µ",
            "regular_fields": "æ™®é€šå­—æ®µå®ä¾‹",
            "unique_regular_fields": "æ ‡å‡†å­—æ®µ (å»é‡å)",
            "unique_calculated_fields": "æ ‡å‡†æŒ‡æ ‡ (å»é‡å)",
        }
        for module, stats in ms.items():
            name = module_names.get(module, module)
            count = stats.get("count", 0)
            synced = stats.get("synced", "")
            note = f"æœ¬æ¬¡åŒæ­¥: {synced}" if synced else ""
            if stats.get("error"):
                note = f"âš ï¸ é”™è¯¯"
            lines.append(f"| {name} | {count:,} | {note} |")
        lines.append("")
        
        # è¡€ç¼˜å…³è”ç»Ÿè®¡
        lines.append("---")
        lines.append("## ğŸ”— è¡€ç¼˜å…³è”ç»Ÿè®¡")
        lines.append("")
        lines.append("### å…³è”è¡¨è®°å½•æ•°")
        lines.append("")
        lines.append("| å…³è”è¡¨ | æ€»è®°å½•æ•° | æ ‡ç­¾åˆ†å¸ƒ |")
        lines.append("|--------|----------|----------|")
        
        ls = self.report_data.get("lineage_stats", {})
        table_names = {
            "table_to_datasource": "è¡¨â†’æ•°æ®æº",
            "datasource_to_workbook": "æ•°æ®æºâ†’å·¥ä½œç°¿",
            "field_to_view": "å­—æ®µâ†’è§†å›¾",
            "dashboard_to_sheet": "ä»ªè¡¨ç›˜â†’Sheet",
            "field_dependencies": "å­—æ®µä¾èµ–",
            "calc_field_dependencies": "è®¡ç®—å­—æ®µä¾èµ–",
            "field_full_lineage": "å®Œæ•´è¡€ç¼˜é“¾",
            "regular_field_full_lineage": "æ™®é€šå­—æ®µè¡€ç¼˜",
            "calc_field_full_lineage": "è®¡ç®—å­—æ®µè¡€ç¼˜",
        }
        for table, stats in ls.items():
            name = table_names.get(table, table)
            total = stats.get("total", 0)
            by_source = stats.get("by_lineage_source", {})
            source_str = ", ".join([f"{k}: {v}" for k, v in by_source.items()]) if by_source else "-"
            lines.append(f"| {name} | {total:,} | {source_str} |")
        lines.append("")
        
        # è¡€ç¼˜æ ‡ç­¾åˆ†å¸ƒ
        lines.append("### è¡€ç¼˜æ ‡ç­¾åˆ†å¸ƒ")
        lines.append("")
        ld = self.report_data.get("label_distribution", {})
        
        if "fields_by_source" in ld:
            lines.append("**å­—æ®µè¡€ç¼˜æ¥æº (lineage_source)**:")
            lines.append("")
            for source, count in ld["fields_by_source"].items():
                label = {"api": "ğŸ”— API ç›´æ¥è¿”å›", "derived": "ğŸ”„ æ™ºèƒ½é‡è¿æ¨å¯¼", "computed": "ğŸ“Š é¢„è®¡ç®—", "null": "â“ æœªæ ‡è®°"}.get(source, source)
                lines.append(f"- {label}: {count:,} ä¸ª")
            lines.append("")
            
        if "fields_by_penetration" in ld:
            lines.append("**å­—æ®µç©¿é€çŠ¶æ€ (penetration_status)**:")
            lines.append("")
            for status, count in ld["fields_by_penetration"].items():
                label = {"success": "âœ… ç©¿é€æˆåŠŸ", "failed": "âŒ ç©¿é€å¤±è´¥", "not_applicable": "â– æ— éœ€ç©¿é€", "null": "â“ æœªæ ‡è®°"}.get(status, status)
                lines.append(f"- {label}: {count:,} ä¸ª")
            lines.append("")
        
        # å»é‡ç»Ÿè®¡
        lines.append("---")
        lines.append("## ğŸ“Š å­—æ®µå»é‡ç»Ÿè®¡")
        lines.append("")
        
        ds = self.report_data.get("deduplication_stats", {})
        
        if "regular_fields" in ds:
            rf = ds["regular_fields"]
            lines.append("### åŸå§‹å­—æ®µå»é‡")
            lines.append("")
            lines.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
            lines.append("|------|------|")
            lines.append(f"| åŸå§‹å®ä¾‹æ•° | {rf.get('total_instances', 0):,} |")
            lines.append(f"| å»é‡åæ ‡å‡†å­—æ®µæ•° | {rf.get('unique_fields', 0):,} |")
            lines.append(f"| å»é‡ç‡ | {rf.get('dedup_ratio', 0):.2%} |")
            lines.append(f"| å¹³å‡æ¯ä¸ªæ ‡å‡†å­—æ®µçš„å®ä¾‹æ•° | {rf.get('avg_instances_per_field', 0):.1f} |")
            lines.append("")
            
            if "by_table_association" in rf:
                lines.append("**è¡¨å…³è”æƒ…å†µ**:")
                for k, v in rf["by_table_association"].items():
                    label = {"has_table": "âœ… æœ‰å…³è”è¡¨", "no_table": "âŒ æ— å…³è”è¡¨"}.get(k, k)
                    lines.append(f"- {label}: {v:,} ä¸ª")
                lines.append("")
        
        if "calculated_fields" in ds:
            cf = ds["calculated_fields"]
            lines.append("### è®¡ç®—å­—æ®µå»é‡")
            lines.append("")
            lines.append(f"| æŒ‡æ ‡ | æ•°å€¼ |")
            lines.append("|------|------|")
            lines.append(f"| åŸå§‹å®ä¾‹æ•° | {cf.get('total_instances', 0):,} |")
            lines.append(f"| å»é‡åæ ‡å‡†æŒ‡æ ‡æ•° | {cf.get('unique_fields', 0):,} |")
            lines.append(f"| å»é‡ç‡ | {cf.get('dedup_ratio', 0):.2%} |")
            lines.append(f"| å¹³å‡æ¯ä¸ªæ ‡å‡†æŒ‡æ ‡çš„å®ä¾‹æ•° | {cf.get('avg_instances_per_field', 0):.1f} |")
            lines.append("")
            
            if "by_complexity" in cf:
                lines.append("**å¤æ‚åº¦åˆ†å¸ƒ**:")
                for k, v in cf["by_complexity"].items():
                    label = {"simple": "ğŸŸ¢ ç®€å• (<5åˆ†)", "medium": "ğŸŸ¡ ä¸­ç­‰ (5-20åˆ†)", "complex": "ğŸ”´ å¤æ‚ (>20åˆ†)"}.get(k, k)
                    lines.append(f"- {label}: {v:,} ä¸ª")
                lines.append("")
        
        # ========== æ–°å¢ï¼šå„æ¨¡å—è¡€ç¼˜è¯¦æƒ… ==========
        mld = self.report_data.get("module_lineage_details", {})
        if mld:
            lines.append("---")
            lines.append("## ğŸ” å„æ¨¡å—è¡€ç¼˜å»ºç«‹è¯¦æƒ…")
            lines.append("")
            
            # æ•°æ®åº“
            if "databases" in mld:
                db = mld["databases"]
                lines.append("### æ•°æ®åº“")
                lines.append(f"- æ€»æ•°: {db.get('total', 0)}")
                lines.append(f"- å·²å…³è”è¡¨æ•°: {db.get('linked_tables', 0)}")
                lines.append(f"- å­¤ç«‹è¡¨æ•°: {db.get('orphan_tables', 0)}")
                lines.append("")
            
            # æ•°æ®è¡¨
            if "tables" in mld:
                tb = mld["tables"]
                lines.append("### æ•°æ®è¡¨")
                lines.append(f"- æ€»æ•°: {tb.get('total', 0)}")
                lines.append(f"- ç‰©ç†è¡¨: {tb.get('physical', 0)}")
                lines.append(f"- åµŒå…¥å¼è¡¨: {tb.get('embedded', 0)}")
                lines.append(f"- å·²å…³è”æ•°æ®æº: {tb.get('linked_to_datasource', 0)}")
                lines.append("")
            
            # æ•°æ®æº
            if "datasources" in mld:
                ds = mld["datasources"]
                lines.append("### æ•°æ®æº")
                lines.append(f"- æ€»æ•°: {ds.get('total', 0)}")
                lines.append(f"- åµŒå…¥å¼: {ds.get('embedded', 0)}")
                lines.append(f"- å·²è®¤è¯: {ds.get('certified', 0)}")
                lines.append(f"- æœ‰ä¸Šæ¸¸è¡¨: {ds.get('with_upstream_tables', 0)}")
                lines.append(f"- è¢«å·¥ä½œç°¿å¼•ç”¨: {ds.get('with_downstream_workbooks', 0)}")
                lines.append(f"- åŒ…å«å­—æ®µ: {ds.get('with_fields', 0)}")
                lines.append("")
            
            # å·¥ä½œç°¿
            if "workbooks" in mld:
                wb = mld["workbooks"]
                lines.append("### å·¥ä½œç°¿")
                lines.append(f"- æ€»æ•°: {wb.get('total', 0)}")
                lines.append(f"- æœ‰è§†å›¾: {wb.get('with_views', 0)}")
                lines.append(f"- æœ‰æ•°æ®æº: {wb.get('with_datasources', 0)}")
                lines.append(f"- æœ‰å­—æ®µ: {wb.get('with_fields', 0)}")
                lines.append("")
            
            # è§†å›¾
            if "views" in mld:
                vw = mld["views"]
                lines.append("### è§†å›¾")
                lines.append(f"- æ€»æ•°: {vw.get('total', 0)}")
                lines.append(f"- ä»ªè¡¨ç›˜: {vw.get('dashboards', 0)}")
                lines.append(f"- Sheet: {vw.get('sheets', 0)}")
                lines.append(f"- æœ‰å­—æ®µå¼•ç”¨: {vw.get('with_field_references', 0)}")
                lines.append(f"- Sheetè¢«ä»ªè¡¨ç›˜åŒ…å«: {vw.get('sheets_included_in_dashboards', 0)}")
                lines.append("")
            
            # å­—æ®µ
            if "fields" in mld:
                fd = mld["fields"]
                lines.append("### å­—æ®µ")
                lines.append(f"- æ€»æ•°: {fd.get('total', 0)}")
                lines.append(f"- è®¡ç®—å­—æ®µ: {fd.get('calculated', 0)}")
                lines.append(f"- æ™®é€šå­—æ®µ: {fd.get('regular', 0)}")
                lines.append(f"- æœ‰è¡¨å…³è”: {fd.get('with_table', 0)}")
                lines.append(f"- æœ‰æ•°æ®æºå…³è”: {fd.get('with_datasource', 0)}")
                lines.append(f"- æœ‰å·¥ä½œç°¿å…³è”: {fd.get('with_workbook', 0)}")
                lines.append(f"- è¢«è§†å›¾ä½¿ç”¨: {fd.get('used_in_views', 0)}")
                lines.append("")
        
        # ========== æ–°å¢ï¼šç±»å‹åˆ†å¸ƒ ==========
        td = self.report_data.get("type_distribution", {})
        if td:
            lines.append("---")
            lines.append("## ğŸ“ˆ ç±»å‹åˆ†å¸ƒ")
            lines.append("")
            
            if "field_roles" in td:
                lines.append("### å­—æ®µè§’è‰²åˆ†å¸ƒ")
                for role, count in td["field_roles"].items():
                    lines.append(f"- {role}: {count:,}")
                lines.append("")
            
            if "field_data_types" in td:
                lines.append("### å­—æ®µæ•°æ®ç±»å‹ Top 15")
                for dtype, count in list(td["field_data_types"].items())[:15]:
                    lines.append(f"- {dtype}: {count:,}")
                lines.append("")
            
            if "table_types" in td:
                lines.append("### æ•°æ®è¡¨ç±»å‹")
                for ttype, count in td["table_types"].items():
                    label = {"physical": "ğŸ“‹ ç‰©ç†è¡¨", "embedded": "ğŸ“¦ åµŒå…¥å¼è¡¨"}.get(ttype, ttype)
                    lines.append(f"- {label}: {count:,}")
                lines.append("")
            
            if "view_types" in td:
                lines.append("### è§†å›¾ç±»å‹")
                for vtype, count in td["view_types"].items():
                    label = {"dashboard": "ğŸ“Š ä»ªè¡¨ç›˜", "sheet": "ğŸ“„ Sheet"}.get(vtype, vtype)
                    lines.append(f"- {label}: {count:,}")
                lines.append("")
        
        # ========== æ–°å¢ï¼šè¦†ç›–ç‡ç»Ÿè®¡ ==========
        cs = self.report_data.get("coverage_stats", {})
        if cs:
            lines.append("---")
            lines.append("## ğŸ“ è¦†ç›–ç‡ç»Ÿè®¡")
            lines.append("")
            
            if "field_description" in cs:
                fd = cs["field_description"]
                lines.append("### å­—æ®µæè¿°è¦†ç›–ç‡")
                lines.append(f"- æ€»å­—æ®µæ•°: {fd.get('total', 0):,}")
                lines.append(f"- æœ‰æè¿°: {fd.get('with_description', 0):,}")
                lines.append(f"- æ— æè¿°: {fd.get('without_description', 0):,}")
                lines.append(f"- **è¦†ç›–ç‡: {fd.get('coverage_rate', 0):.2%}**")
                lines.append("")
            
            if "datasource_description" in cs:
                dd = cs["datasource_description"]
                lines.append("### æ•°æ®æºæè¿°è¦†ç›–ç‡")
                lines.append(f"- æ€»æ•°æ®æº: {dd.get('total', 0):,}")
                lines.append(f"- æœ‰æè¿°: {dd.get('with_description', 0):,}")
                lines.append(f"- **è¦†ç›–ç‡: {dd.get('coverage_rate', 0):.2%}**")
                lines.append("")
            
            if "datasource_certification" in cs:
                dc = cs["datasource_certification"]
                lines.append("### æ•°æ®æºè®¤è¯ç‡")
                lines.append(f"- å·²å‘å¸ƒæ•°æ®æº: {dc.get('published_total', 0):,}")
                lines.append(f"- å·²è®¤è¯: {dc.get('certified', 0):,}")
                lines.append(f"- **è®¤è¯ç‡: {dc.get('certification_rate', 0):.2%}**")
                lines.append("")
        
        # ========== æ–°å¢ï¼šçƒ­é—¨èµ„äº§ ==========
        ha = self.report_data.get("hot_assets", {})
        if ha:
            lines.append("---")
            lines.append("## ğŸ”¥ çƒ­é—¨èµ„äº§ Top 10")
            lines.append("")
            
            if ha.get("hot_views"):
                lines.append("### çƒ­é—¨è§†å›¾")
                lines.append("| æ’å | è§†å›¾åç§° | è®¿é—®é‡ |")
                lines.append("|------|----------|--------|")
                for i, v in enumerate(ha["hot_views"][:10], 1):
                    lines.append(f"| {i} | {v['name']} | {v['views']:,} |")
                lines.append("")
            
            if ha.get("hot_fields"):
                lines.append("### é«˜é¢‘ä½¿ç”¨å­—æ®µ")
                lines.append("| æ’å | å­—æ®µåç§° | ä½¿ç”¨æ¬¡æ•° |")
                lines.append("|------|----------|----------|")
                for i, f in enumerate(ha["hot_fields"][:10], 1):
                    lines.append(f"| {i} | {f['name']} | {f['usage']:,} |")
                lines.append("")
            
            if ha.get("hot_datasources"):
                lines.append("### è¢«å¼•ç”¨æœ€å¤šçš„æ•°æ®æº")
                lines.append("| æ’å | æ•°æ®æºåç§° | å·¥ä½œç°¿å¼•ç”¨æ•° |")
                lines.append("|------|------------|--------------|")
                for i, d in enumerate(ha["hot_datasources"][:10], 1):
                    lines.append(f"| {i} | {d['name']} | {d['workbook_count']:,} |")
                lines.append("")
        
        # ========== æ–°å¢ï¼šé¡¹ç›®åˆ†å¸ƒ ==========
        pd = self.report_data.get("project_distribution", {})
        if pd and pd.get("by_project"):
            lines.append("---")
            lines.append("## ğŸ“ é¡¹ç›®èµ„äº§åˆ†å¸ƒ")
            lines.append("")
            lines.append("| é¡¹ç›® | æ•°æ®æº | å·¥ä½œç°¿ | æ€»èµ„äº§ |")
            lines.append("|------|--------|--------|--------|")
            for p in pd["by_project"]:
                lines.append(f"| {p['name']} | {p['datasource_count']} | {p['workbook_count']} | {p['total_assets']} |")
            lines.append("")
        
        # ========== æ–°å¢ï¼šé‡å¤æœ€å¤šçš„å­—æ®µ ==========
        td = self.report_data.get("dedup_top_duplicates", {})
        if td:
            lines.append("---")
            lines.append("## ğŸ”„ é‡å¤æœ€å¤šçš„å­—æ®µ Top 10")
            lines.append("")
            
            if td.get("top_regular_fields"):
                lines.append("### åŸå§‹å­—æ®µï¼ˆå®ä¾‹æ•°æœ€å¤šï¼‰")
                lines.append("| æ’å | å­—æ®µåç§° | å®ä¾‹æ•° |")
                lines.append("|------|----------|--------|")
                for i, f in enumerate(td["top_regular_fields"][:10], 1):
                    lines.append(f"| {i} | {f['name']} | {f['instances']} |")
                lines.append("")
            
            if td.get("top_calculated_fields"):
                lines.append("### è®¡ç®—å­—æ®µï¼ˆå®ä¾‹æ•°æœ€å¤šï¼‰")
                lines.append("| æ’å | æŒ‡æ ‡åç§° | å®ä¾‹æ•° |")
                lines.append("|------|----------|--------|")
                for i, f in enumerate(td["top_calculated_fields"][:10], 1):
                    lines.append(f"| {i} | {f['name']} | {f['instances']} |")
                lines.append("")
        
        # æœªå»ºç«‹è¡€ç¼˜
        lines.append("---")
        lines.append("## âš ï¸ æœªå»ºç«‹è¡€ç¼˜/å¼‚å¸¸æƒ…å†µ")
        lines.append("")
        
        ul = self.report_data.get("unestablished_lineage", {})
        lines.append("| å¼‚å¸¸ç±»å‹ | æ•°é‡ | è¯´æ˜ |")
        lines.append("|----------|------|------|")
        
        issues = [
            ("fields_without_datasource", "æ— æ•°æ®æºå…³è”", "å­—æ®µæœªå…³è”åˆ°ä»»ä½•æ•°æ®æº"),
            ("fields_without_table", "æ— æ•°æ®è¡¨å…³è”", "å­—æ®µæœªå…³è”åˆ°ç‰©ç†è¡¨"),
            ("fields_without_workbook", "æ— å·¥ä½œç°¿å…³è”", "å­—æ®µæœªå…³è”åˆ°å·¥ä½œç°¿"),
            ("fields_without_views", "æ— è§†å›¾å¼•ç”¨", "å­—æ®µæœªè¢«ä»»ä½•è§†å›¾ä½¿ç”¨"),
            ("penetration_failed", "ç©¿é€å¤±è´¥", "åµŒå…¥å¼è¡¨ç©¿é€åˆ°ç‰©ç†è¡¨å¤±è´¥"),
            ("column_fields_without_upstream", "åˆ—å­—æ®µæ— ä¸Šæ¸¸åˆ—", "ColumnField ç¼ºå°‘ upstream_column"),
        ]
        
        for key, name, desc in issues:
            count = ul.get(key, 0)
            if count > 0:
                lines.append(f"| âŒ {name} | {count:,} | {desc} |")
            else:
                lines.append(f"| âœ… {name} | {count} | {desc} |")
        lines.append("")
        
        # ç»“å°¾
        lines.append("---")
        lines.append("")
        lines.append("*æœ¬æŠ¥å‘Šç”± Tableau å…ƒæ•°æ®åŒæ­¥ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(lines)
            
    def _print_report(self):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦åˆ°æ§åˆ¶å°"""
        print("\n" + "=" * 70)
        print("ğŸ“Š åŒæ­¥æŠ¥å‘Šæ‘˜è¦")
        print("=" * 70)
        
        # æ¨¡å—ç»Ÿè®¡
        print("\nã€æ¨¡å—åŒæ­¥ç»Ÿè®¡ã€‘")
        ms = self.report_data.get("module_stats", {})
        for module, stats in ms.items():
            count = stats.get("count", 0)
            synced = stats.get("synced", "")
            synced_str = f" (æœ¬æ¬¡åŒæ­¥: {synced})" if synced else ""
            print(f"  Â· {module}: {count}{synced_str}")
            
        # è¡€ç¼˜ç»Ÿè®¡
        print("\nã€è¡€ç¼˜å…³è”ç»Ÿè®¡ã€‘")
        ls = self.report_data.get("lineage_stats", {})
        for table, stats in ls.items():
            total = stats.get("total", 0)
            by_source = stats.get("by_lineage_source", {})
            source_str = ", ".join([f"{k}={v}" for k, v in by_source.items()]) if by_source else ""
            print(f"  Â· {table}: {total}" + (f" ({source_str})" if source_str else ""))
        
        # æ ‡ç­¾åˆ†å¸ƒ
        print("\nã€è¡€ç¼˜æ ‡ç­¾åˆ†å¸ƒã€‘")
        ld = self.report_data.get("label_distribution", {})
        if "fields_by_source" in ld:
            print(f"  Â· fields.lineage_source: {ld['fields_by_source']}")
        if "fields_by_penetration" in ld:
            print(f"  Â· fields.penetration_status: {ld['fields_by_penetration']}")
            
        # å»é‡ç»Ÿè®¡
        print("\nã€å»é‡ç»Ÿè®¡ã€‘")
        ds = self.report_data.get("deduplication_stats", {})
        if "regular_fields" in ds:
            rf = ds["regular_fields"]
            print(f"  Â· åŸå§‹å­—æ®µ: {rf.get('total_instances', 0)} å®ä¾‹ â†’ {rf.get('unique_fields', 0)} æ ‡å‡†å­—æ®µ")
            print(f"    å»é‡ç‡: {rf.get('dedup_ratio', 0):.2%}, å¹³å‡å®ä¾‹æ•°: {rf.get('avg_instances_per_field', 0):.1f}")
        if "calculated_fields" in ds:
            cf = ds["calculated_fields"]
            print(f"  Â· è®¡ç®—å­—æ®µ: {cf.get('total_instances', 0)} å®ä¾‹ â†’ {cf.get('unique_fields', 0)} æ ‡å‡†æŒ‡æ ‡")
            print(f"    å»é‡ç‡: {cf.get('dedup_ratio', 0):.2%}, å¹³å‡å®ä¾‹æ•°: {cf.get('avg_instances_per_field', 0):.1f}")
            if "by_complexity" in cf:
                print(f"    å¤æ‚åº¦åˆ†å¸ƒ: {cf['by_complexity']}")
                
        # æœªå»ºç«‹è¡€ç¼˜
        print("\nã€æœªå»ºç«‹è¡€ç¼˜/å¼‚å¸¸æƒ…å†µã€‘")
        ul = self.report_data.get("unestablished_lineage", {})
        print(f"  Â· æ— æ•°æ®æºå…³è”: {ul.get('fields_without_datasource', 0)}")
        print(f"  Â· æ— æ•°æ®è¡¨å…³è”: {ul.get('fields_without_table', 0)}")
        print(f"  Â· æ— å·¥ä½œç°¿å…³è”: {ul.get('fields_without_workbook', 0)}")
        print(f"  Â· æ— è§†å›¾å¼•ç”¨: {ul.get('fields_without_views', 0)}")
        print(f"  Â· ç©¿é€å¤±è´¥: {ul.get('penetration_failed', 0)}")
        print(f"  Â· åˆ—å­—æ®µæ— ä¸Šæ¸¸åˆ—: {ul.get('column_fields_without_upstream', 0)}")
        
        print("\n" + "=" * 70)

