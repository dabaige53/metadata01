"""
Tableau Metadata API æ•°æ®åŒæ­¥æ¨¡å—
ä» Tableau Server æŠ“å–å…ƒæ•°æ®å¹¶å­˜å…¥æœ¬åœ°æ•°æ®åº“
"""
import os
import sys
import json
import requests
import hashlib
from collections import defaultdict
from datetime import datetime
from typing import Optional, List, Dict, Any
from sqlalchemy import select, text
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from backend.config import Config
from backend.models import (
    Base, get_engine, init_db, get_session,
    Database, DBTable, DBColumn, Field, Datasource, Workbook, View,
    TableauUser, Project,
    table_to_datasource, datasource_to_workbook, field_to_view, CalculatedField, SyncLog,
    FieldDependency, Metric, dashboard_to_sheet
)


class TableauMetadataClient:
    """Tableau Metadata API å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str, username: str = None, password: str = None, 
                 pat_name: str = None, pat_secret: str = None):
        self.base_url = base_url.rstrip('/')
        self.username = username
        self.password = password
        self.pat_name = pat_name
        self.pat_secret = pat_secret
        self.auth_token: Optional[str] = None
        self.site_id: Optional[str] = None
        self.api_version = "3.10"
    
    def sign_in(self) -> bool:
        """ç™»å½•è·å–è®¤è¯ token (æ”¯æŒç”¨æˆ·åå¯†ç æˆ– PAT)"""
        signin_url = f"{self.base_url}/api/{self.api_version}/auth/signin"
        
        # æ ¹æ®é…ç½®é€‰æ‹©è®¤è¯æ–¹å¼
        if self.pat_name and self.pat_secret:
            # PAT è®¤è¯
            payload = {
                "credentials": {
                    "personalAccessTokenName": self.pat_name,
                    "personalAccessTokenSecret": self.pat_secret,
                    "site": {"contentUrl": ""}
                }
            }
            print(f"  ä½¿ç”¨ PAT è®¤è¯: {self.pat_name}")
        else:
            # ç”¨æˆ·åå¯†ç è®¤è¯
            payload = {
                "credentials": {
                    "name": self.username,
                    "password": self.password,
                    "site": {"contentUrl": ""}
                }
            }
            print(f"  ä½¿ç”¨ç”¨æˆ·åå¯†ç è®¤è¯: {self.username}")
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        try:
            response = requests.post(signin_url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                credentials = data.get("credentials", {})
                self.auth_token = credentials.get("token")
                self.site_id = credentials.get("site", {}).get("id")
                print(f"âœ… ç™»å½•æˆåŠŸ (Token: {self.auth_token[:20]}...)")
                return True
            else:
                print(f"âŒ ç™»å½•å¤±è´¥: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return False
    
    def sign_out(self):
        """ç™»å‡ºé‡Šæ”¾ token"""
        if not self.auth_token:
            return
        
        signout_url = f"{self.base_url}/api/{self.api_version}/auth/signout"
        headers = {"X-Tableau-Auth": self.auth_token}
        
        try:
            response = requests.post(signout_url, headers=headers, timeout=30)
            if response.status_code == 204:
                print("âœ… å·²ç™»å‡º")
        except Exception as e:
            print(f"ç™»å‡ºå¼‚å¸¸: {e}")
        finally:
            self.auth_token = None
    
    def execute_query(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œ GraphQL æŸ¥è¯¢"""
        if not self.auth_token:
            raise RuntimeError("æœªç™»å½•ï¼Œè¯·å…ˆè°ƒç”¨ sign_in()")
        
        url = f"{self.base_url}/api/metadata/graphql"
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Tableau-Auth": self.auth_token
        }
        
        response = requests.post(url, headers=headers, json={"query": query}, timeout=60)
        
        if response.status_code == 200:
            return response.json()
        else:
            raise RuntimeError(f"GraphQL æŸ¥è¯¢å¤±è´¥: {response.status_code} - {response.text}")
    
    def fetch_views_usage(self) -> Dict[str, int]:
        """ä» REST API è·å–è§†å›¾ä½¿ç”¨ç»Ÿè®¡ (REST API)"""
        if not self.auth_token or not self.site_id:
            raise RuntimeError("æœªç™»å½•ï¼Œè¯·å…ˆè°ƒç”¨ sign_in()")
        
        # REST API endpoint for views
        url = f"{self.base_url}/api/{self.api_version}/sites/{self.site_id}/views"
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Tableau-Auth": self.auth_token
        }
        
        usage_map = {}
        page_number = 1
        page_size = 100
        
        print(f"  æ­£åœ¨è°ƒç”¨ REST API è·å–è®¿é—®ç»Ÿè®¡: {url}")
        
        while True:
            params = {
                "pageNumber": page_number,
                "pageSize": page_size,
                "includeUsageStatistics": "true"
            }
            
            try:
                response = requests.get(url, headers=headers, params=params, timeout=30)
                
                if response.status_code != 200:
                    print(f"  âŒ REST API è·å–å¤±è´¥: {response.status_code} - {response.text}")
                    break
                
                data = response.json()
                views = data.get("views", {}).get("view", [])
                
                if not views:
                    break
                
                for view in views:
                    luid = view.get("id")
                    usage = view.get("usage", {})
                    # usage å¯èƒ½æ˜¯ Noneï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ totalViewCount
                    if usage:
                        total_count = usage.get("totalViewCount", 0)
                        if luid:
                            usage_map[luid] = int(total_count)
                
                # Check pagination
                pagination = data.get("pagination", {})
                total_available = int(pagination.get("totalAvailable", 0))
                
                print(f"    - Page {page_number}: è·å– {len(views)} ä¸ªè§†å›¾, æ€»è¿›åº¦ {len(usage_map)}/{total_available}")
                
                if len(usage_map) >= total_available or len(views) < page_size:
                    break
                    
                page_number += 1
                
            except Exception as e:
                print(f"  âŒ è·å–è§†å›¾ç»Ÿè®¡å¼‚å¸¸: {e}")
                break
                
        return usage_map    
    def fetch_databases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            databaseServers {
                id
                luid
                name
                connectionType
                hostName
                port
                service
                description
                isCertified
                certificationNote
            }
        }
        """
        result = self.execute_query(query)
        # å…¼å®¹å¤„ç†ï¼šå…ˆå°è¯• databaseServersï¼Œå¤±è´¥åˆ™å›é€€åˆ° databases
        data = result.get("data", {})
        servers = data.get("databaseServers")
        if servers is not None:
            return servers
        # å›é€€åˆ°æ—§æŸ¥è¯¢
        return self._fetch_databases_fallback()
    
    def _fetch_databases_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨æ—§ç‰ˆæŸ¥è¯¢è·å–æ•°æ®åº“"""
        query = """
        {
            databases {
                id
                name
                connectionType
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("databases", [])
    
    def fetch_tables(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ•°æ®è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            databaseTables {
                id
                luid
                name
                schema
                fullName
                description
                isEmbedded
                projectName
                database {
                    id
                    name
                    connectionType
                }
                columns {
                    id
                    name
                    description
                    remoteType
                    isNullable
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ï¼ˆæŸäº›å­—æ®µå¯èƒ½ä¸è¢«æ”¯æŒï¼‰
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š: {result['errors']}")
            # å°è¯•ç®€åŒ–æŸ¥è¯¢
            return self._fetch_tables_fallback()
        
        return result.get("data", {}).get("databaseTables", [])
    
    def _fetch_tables_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨ç®€åŒ–æŸ¥è¯¢è·å–è¡¨ï¼ˆåŒ…å«åˆ—ä¿¡æ¯ï¼‰"""
        query = """
        {
            databaseTables {
                id
                name
                schema
                fullName
                isEmbedded
                database {
                    id
                    name
                    connectionType
                }
                columns {
                    id
                    name
                    remoteType
                    description
                    isNullable
                }
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("databaseTables", [])
    
    def fetch_datasources(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·²å‘å¸ƒæ•°æ®æºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            publishedDatasources {
                id
                luid
                name
                description
                uri
                projectName
                hasExtracts
                extractLastRefreshTime
                extractLastIncrementalUpdateTime
                extractLastUpdateTime
                isCertified
                certificationNote
                certifierDisplayName
                containsUnsupportedCustomSql
                hasActiveWarning
                createdAt
                updatedAt
                vizportalUrlId
                owner {
                    id
                    username
                    name
                }
                upstreamTables {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯å¹¶å›é€€
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š: {result['errors']}")
            return self._fetch_datasources_fallback()
        
        return result.get("data", {}).get("publishedDatasources", [])
    
    def _fetch_datasources_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨ç®€åŒ–æŸ¥è¯¢è·å–æ•°æ®æº"""
        query = """
        {
            publishedDatasources {
                id
                name
                projectName
                hasExtracts
                extractLastRefreshTime
                isCertified
                owner {
                    username
                }
                upstreamTables {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("publishedDatasources", [])
    
    def fetch_workbooks(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·¥ä½œç°¿ï¼ˆä¼˜åŒ–ç‰ˆï¼šRobust Aliased Chunking + Null Owner Fallbackï¼‰"""
        all_workbooks = []
        
        print(f"  æ­£åœ¨è·å–å·¥ä½œç°¿åˆ—è¡¨...")
        # 1. è·å–æ‰€æœ‰ ID (Safe Query)
        list_query = """
        {
            workbooks {
                id
                name
            }
        }
        """
        list_result = self.execute_query(list_query)
        if "errors" in list_result and not list_result.get("data"):
            print(f"  âš ï¸ è·å–å·¥ä½œç°¿åˆ—è¡¨å¤±è´¥: {list_result['errors']}")
            return []
            
        workbooks_meta = list_result.get("data", {}).get("workbooks") or []
        print(f"  éœ€åŒæ­¥ {len(workbooks_meta)} ä¸ªå·¥ä½œç°¿è¯¦æƒ…...")
        
        # 2. åˆ†æ‰¹è·å–è¯¦æƒ…
        chunk_size = 10
        total = len(workbooks_meta)
        
        for i in range(0, total, chunk_size):
            chunk = workbooks_meta[i:i+chunk_size]
            
            # å°è¯•æ‰¹é‡è·å– (å« metadata)
            try:
                self._fetch_workbooks_chunk(chunk, all_workbooks, include_owner=True)
            except Exception as e:
                print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} é‡åˆ°é”™è¯¯ï¼Œå°è¯•é™çº§é‡è¯• (ä¸å« Owner)...")
                try:
                    self._fetch_workbooks_chunk(chunk, all_workbooks, include_owner=False)
                except Exception as e2:
                    print(f"  âŒ æ‰¹æ¬¡ {i//chunk_size + 1} å½»åº•å¤±è´¥: {e2}")

            print(f"    - å·¥ä½œç°¿: å·²å¤„ç† {min(i+chunk_size, total)}/{total}")

        return all_workbooks
    
    def _fetch_workbooks_chunk(self, chunk: List[Dict], all_workbooks: List[Dict], include_owner: bool = True):
        """è¾…åŠ©ï¼šæ‰¹é‡è·å–å·¥ä½œç°¿è¯¦æƒ…"""
        query_parts = []
        owner_field = """
                    owner {
                        id
                        username
                        name
                    }
        """ if include_owner else ""
        
        for idx, wb in enumerate(chunk):
            wb_id = wb["id"]
            query_parts.append(f"""
            wb{idx}: workbooks(filter: {{id: "{wb_id}"}}) {{
                id
                luid
                name
                description
                uri
                projectName
                createdAt
                updatedAt
                containsUnsupportedCustomSql
                vizportalUrlId
                {owner_field}
                upstreamDatasources {{
                    id
                    name
                }}
                sheets {{
                    id
                    luid
                    name
                    path
                    index
                    createdAt
                    updatedAt
                }}
                dashboards {{
                    id
                    luid
                    name
                    path
                    index
                    createdAt
                    updatedAt
                    sheets {{
                        id
                    }}
                }}
                embeddedDatasources {{
                    id
                    name
                    upstreamDatasources {{
                        id
                        name
                    }}
                    fields {{
                        id
                        name
                        description
                        ... on ColumnField {{
                            dataType
                            role
                            isHidden
                            folderName
                        }}
                        ... on CalculatedField {{
                            dataType
                            role
                            isHidden
                            folderName
                            formula
                        }}
                    }}
                }}
            }}
            """)
        
        full_query = "{" + "\n".join(query_parts) + "}"
        result = self.execute_query(full_query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ strict error (å¯¼è‡´ data ä¸º null)
        if "data" not in result or not result["data"]:
             # æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘é™çº§
             raise Exception("Data is null, likely non-nullable field violation")
             
        data = result.get("data", {})
        
        for key, wb_list in data.items():
            if not wb_list: continue 
            wb_detail = wb_list[0]
            
            all_workbooks.append(wb_detail)
    
    def fetch_fields(self) -> List[Dict]:
        all_fields = []
        
        # 1. è·å–æ‰€æœ‰æ•°æ®æº ID
        print(f"  æ­£åœ¨è·å–æ•°æ®æºåˆ—è¡¨ä»¥åŒæ­¥å­—æ®µ...")
        ds_query = """
        {
            publishedDatasources {
                id
                name
            }
            embeddedDatasources {
                id
                name
                upstreamDatasources {
                    id
                    name
                }
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "errors" in ds_result and not ds_result.get("data"):
            print(f"  âš ï¸ è·å–æ•°æ®æºå¤±è´¥: {ds_result['errors']}")
            return []
            
        published = ds_result.get("data", {}).get("publishedDatasources") or []
        embedded = ds_result.get("data", {}).get("embeddedDatasources") or []
        
        # å»ºç«‹åµŒå…¥å¼åˆ°å‘å¸ƒæ˜ å°„
        embedded_to_published = {}
        for ds in embedded:
            upstreams = ds.get("upstreamDatasources") or []
            if upstreams:
                embedded_to_published[ds["id"]] = upstreams[0]["id"]

        # åˆ†åˆ«å¤„ç†ä¸¤ç§æ•°æ®æº
        self._batch_fetch_fields(published, "publishedDatasources", all_fields)
        self._batch_fetch_fields(embedded, "embeddedDatasources", all_fields, embedded_to_published)
        
        print(f"  âœ… å…±é‡‡é›†åˆ° {len(all_fields)} ä¸ªå­—æ®µ")
        return all_fields

    def _batch_fetch_fields(self, datasources: List[Dict], type_name: str, all_fields: List[Dict], 
                             embedded_to_published: Dict = None):
        """æ‰¹é‡è·å–å­—æ®µè¯¦æƒ… (è¾…åŠ©æ–¹æ³•)"""
        if not datasources:
            return
        
        embedded_to_published = embedded_to_published or {}

        print(f"  åŒæ­¥ {type_name}: {len(datasources)} ä¸ª...")
        chunk_size = 10
        total = len(datasources)
        
        for i in range(0, total, chunk_size):
            chunk = datasources[i:i+chunk_size]
            
            # åŠ¨æ€æ„å»º Alias Filter æŸ¥è¯¢
            query_parts = []
            for idx, ds in enumerate(chunk):
                ds_id = ds["id"]
                query_parts.append(f"""
                q{idx}: {type_name}(filter: {{id: "{ds_id}"}}) {{
                    id
                    name
                    fields {{
                        __typename
                        id
                        name
                        description
                        ... on ColumnField {{
                            dataType
                            role
                            isHidden
                            upstreamColumns {{
                                id
                                name
                                table {{
                                    id
                                }}
                            }}
                        }}
                        ... on CalculatedField {{
                            dataType
                            role
                            isHidden
                            formula
                            upstreamFields {{
                                id
                                name
                                datasource {{
                                    id
                                    name
                                }}
                            }}
                        }}
                        ... on GroupField {{
                            dataType
                            role
                            isHidden
                        }}
                    }}
                }}
                """)
            
            full_query = "{" + "\n".join(query_parts) + "}"
            
            try:
                result = self.execute_query(full_query)
                if "errors" in result:
                    print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} éƒ¨åˆ†å¤±è´¥: {result['errors'][0].get('message')}")
                
                data = result.get("data", {})
                if not data: continue
                
                for key, ds_list in data.items():
                    if not ds_list: continue
                    # filteræŸ¥è¯¢è¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                    ds_data = ds_list[0]
                    ds_id = ds_data.get("id")
                    ds_name = ds_data.get("name")
                    
                    # è¡€ç¼˜ç©¿é€ï¼šå¦‚æœæ˜¯åµŒå…¥å¼ä¸”æœ‰ä¸Šæ¸¸å‘å¸ƒå¼ï¼Œåˆ™ä½¿ç”¨å‘å¸ƒå¼çš„ ID
                    final_ds_id = embedded_to_published.get(ds_id, ds_id)
                    
                    fields_list = ds_data.get("fields") or []
                    for field in fields_list:
                        if field and field.get("id"):
                            field["datasource_id"] = final_ds_id
                            field["datasource_name"] = ds_name
                            field["parent_datasource_id"] = ds_id  # ä¿ç•™åŸå§‹ ID å¤‡ç”¨
                            all_fields.append(field)
                
                print(f"    - {type_name}: å·²å¤„ç† {min(i+chunk_size, total)}/{total}")
                
            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡æŸ¥è¯¢å¼‚å¸¸: {e}")

    def fetch_calculated_fields(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è®¡ç®—å­—æ®µ"""
        query = """
        {
            calculatedFields {
                id
                name
                description
                formula
                dataType
                role
                datasource {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯
        if "errors" in result:
            print(f"  âš ï¸ GraphQL é”™è¯¯: {result['errors']}")
            return []
        
        data = result.get("data")
        if data is None:
            return []
        
        calc_fields = data.get("calculatedFields") or []
        
        # å°è¯•ç©¿é€ï¼šå¯¹äºæ²¡æœ‰ datasource æˆ–è€… datasource ä¸ºåµŒå…¥å¼çš„ï¼Œ
        # åœ¨è¿”å›å‰å¯ä»¥æ ¹æ®å†…éƒ¨é€»è¾‘å¢å¼ºï¼Œä½†æœ€æ ¸å¿ƒçš„ç©¿é€å·²åœ¨ _batch_fetch_fields å®Œæˆã€‚
        # è¿™é‡Œæˆ‘ä»¬ç¡®ä¿ cf ä¹Ÿæºå¸¦å¿…è¦çš„ datasource_idã€‚
        for cf in calc_fields:
            if cf and cf.get("datasource"):
                cf["datasource_id"] = cf["datasource"].get("id")
            
        return calc_fields
    
    def fetch_views_with_fields(self) -> List[Dict]:
        """è·å–è§†å›¾åŠå…¶ä½¿ç”¨çš„å­—æ®µï¼ˆè¿­ä»£ä¼˜åŒ–ç‰ˆï¼šé€šè¿‡ Filter-ID åˆ†é¡µé‡‡é›†ï¼‰"""
        all_view_fields = []
        
        print(f"  æ­£åœ¨è·å–è§†å›¾å­—æ®µå…³è”(ä¼˜åŒ–ç‰ˆ)...")
        
        # 1. è·å–æ‰€æœ‰å·¥ä½œç°¿ ID
        wb_query = """
        {
            workbooks {
                id
                name
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "errors" in wb_result and not wb_result.get("data"):
             print(f"  âš ï¸ è·å–å·¥ä½œç°¿åˆ—è¡¨å¤±è´¥: {wb_result['errors']}")
             return []
             
        workbooks = wb_result.get("data", {}).get("workbooks") or []
        print(f"  éœ€åŒæ­¥ {len(workbooks)} ä¸ªå·¥ä½œç°¿çš„è§†å›¾å…³è”...")
        
        # 2. é€ä¸ªå·¥ä½œç°¿æŸ¥è¯¢ (æˆ–å°æ‰¹é‡)
        # è€ƒè™‘åˆ°è§†å›¾åŒ…å«çš„å­—æ®µå¼•ç”¨èŠ‚ç‚¹å¯èƒ½å¾ˆå¤šï¼Œè¿™é‡Œé‡‡ç”¨æ¯æ¬¡æŸ¥ 5 ä¸ªå·¥ä½œç°¿
        chunk_size = 5
        total = len(workbooks)
        
        for i in range(0, total, chunk_size):
            chunk = workbooks[i:i+chunk_size]
            
            # åŠ¨æ€æ„å»º Alias Filter æŸ¥è¯¢
            query_parts = []
            for idx, wb in enumerate(chunk):
                wb_id = wb["id"]
                query_parts.append(f"""
                wb{idx}: workbooks(filter: {{id: "{wb_id}"}}) {{
                    id
                    name
                    sheets {{
                        id
                        name
                        sheetFieldInstances {{
                            id
                            name
                            datasource {{
                                id
                            }}
                        }}
                    }}
                }}
                """)
            
            full_query = "{" + "\n".join(query_parts) + "}"
            
            try:
                result = self.execute_query(full_query)
                if "errors" in result:
                    print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} éƒ¨åˆ†å¤±è´¥: {result['errors'][0].get('message')}")
                
                data = result.get("data", {})
                if not data: continue
                
                # è§£æåˆ«åç»“æœ
                for key, wb_list in data.items():
                    if not wb_list: continue
                    # workbooks è¿”å›çš„æ˜¯åˆ—è¡¨
                    wb_data = wb_list[0]
                    wb_id = wb_data.get("id")
                    
                    sheets = wb_data.get("sheets") or []
                    for sheet in sheets:
                        if not sheet: continue
                        view_id = sheet.get("id")
                        view_name = sheet.get("name")
                        fields = sheet.get("sheetFieldInstances") or []
                        
                        for field in fields:
                            if field and field.get("id"):
                                all_view_fields.append({
                                    "view_id": view_id,
                                    "view_name": view_name,
                                    "workbook_id": wb_id,
                                    "field_id": field.get("id"),
                                    "field_name": field.get("name"),
                                    "datasource_id": (field.get("datasource") or {}).get("id")
                                })
                                
                print(f"    - å·²å¤„ç† {min(i+chunk_size, total)}/{total} ä¸ªå·¥ä½œç°¿, ç´¯è®¡å…³è” {len(all_view_fields)}")
                
            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡æŸ¥è¯¢å¼‚å¸¸: {e}")
        
        print(f"  âœ… æŠ“å–åˆ° {len(all_view_fields)} ä¸ªå­—æ®µå…³è”å…³ç³»")
        return all_view_fields
    
    def _fetch_views_with_fields_fallback(self) -> List[Dict]:
        """å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡å·¥ä½œç°¿çš„æ•°æ®æºå…³ç³»é—´æ¥è·å–"""
        # ç”±äº Tableau API é™åˆ¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€åŒ–ç­–ç•¥ï¼š
        # é€šè¿‡ calculatedFields çš„ datasource å…³ç³»æ¥å»ºç«‹å­—æ®µâ†’è§†å›¾çš„é—´æ¥å…³è”
        query = """
        {
            workbooks {
                id
                name
                sheets {
                    id
                    name
                }
                upstreamDatasources {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        if "errors" in result:
            print(f"  âš ï¸ å¤‡ç”¨æŸ¥è¯¢ä¹Ÿå¤±è´¥: {result['errors']}")
            return []
        
        data = result.get("data")
        if data is None:
            return []
        
        workbooks = data.get("workbooks") or []
        
        # è·å–æ•°æ®æºåˆ°å­—æ®µçš„æ˜ å°„
        ds_to_fields = {}
        fields_result = self.execute_query("""
        {
            publishedDatasources {
                id
                fields {
                    id
                    name
                }
            }
        }
        """)
        if "data" in fields_result and fields_result["data"]:
            for ds in (fields_result["data"].get("publishedDatasources") or []):
                if ds:
                    ds_to_fields[ds["id"]] = ds.get("fields") or []
        
        # æ„å»ºè§†å›¾â†’å­—æ®µå…³è”
        view_fields = []
        for wb in workbooks:
            if not wb:
                continue
            sheets = wb.get("sheets") or []
            datasources = wb.get("upstreamDatasources") or []
            
            for sheet in sheets:
                if not sheet:
                    continue
                # å°†æ•°æ®æºçš„å­—æ®µå…³è”åˆ°è§†å›¾
                for ds in datasources:
                    if not ds:
                        continue
                    for field in ds_to_fields.get(ds.get("id"), []):
                        if field and field.get("id"):
                            view_fields.append({
                                "view_id": sheet.get("id"),
                                "view_name": sheet.get("name"),
                                "workbook_id": wb.get("id"),
                                "field_id": field.get("id"),
                                "field_name": field.get("name")
                            })
        
        return view_fields


    def fetch_users(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ Tableau ç”¨æˆ·"""
        query = """
        {
            tableauUsers {
                id
                luid
                name
                username
                email
                domain
                siteRole
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š (users): {result['errors']}")
            # å°è¯•ç®€åŒ–æŸ¥è¯¢
            return self._fetch_users_fallback()
        
        return result.get("data", {}).get("tableauUsers", [])
    
    def _fetch_users_fallback(self) -> List[Dict]:
        """å›é€€ï¼šé€šè¿‡ owner å…³ç³»æ”¶é›†ç”¨æˆ·"""
        users_dict = {}
        
        # ä»æ•°æ®æºæ”¶é›†ç”¨æˆ·
        ds_query = """
        {
            publishedDatasources {
                owner {
                    id
                    username
                    name
                }
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "data" in ds_result and ds_result["data"]:
            for ds in (ds_result["data"].get("publishedDatasources") or []):
                if ds and ds.get("owner"):
                    owner = ds["owner"]
                    if owner.get("id"):
                        users_dict[owner["id"]] = {
                            "id": owner.get("id"),
                            "name": owner.get("name"),
                            "username": owner.get("username")
                        }
        
        # ä»å·¥ä½œç°¿æ”¶é›†ç”¨æˆ·
        wb_query = """
        {
            workbooks {
                owner {
                    id
                    username
                    name
                }
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "data" in wb_result and wb_result["data"]:
            for wb in (wb_result["data"].get("workbooks") or []):
                if wb and wb.get("owner"):
                    owner = wb["owner"]
                    if owner.get("id"):
                        users_dict[owner["id"]] = {
                            "id": owner.get("id"),
                            "name": owner.get("name"),
                            "username": owner.get("username")
                        }
        
        return list(users_dict.values())
    
    def fetch_projects(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ Tableau é¡¹ç›®"""
        # é€šè¿‡æ•°æ®æºå’Œå·¥ä½œç°¿çš„ projectName æ”¶é›†é¡¹ç›®ä¿¡æ¯
        # æ³¨æ„ï¼šTableau Metadata API æ²¡æœ‰ç›´æ¥çš„ projects æŸ¥è¯¢ï¼Œéœ€è¦é—´æ¥æ”¶é›†
        projects_dict = {}
        
        # ä»æ•°æ®æºæ”¶é›†é¡¹ç›®
        ds_query = """
        {
            publishedDatasources {
                projectName
                projectVizportalUrlId
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "data" in ds_result and ds_result["data"]:
            for ds in (ds_result["data"].get("publishedDatasources") or []):
                if ds and ds.get("projectName"):
                    project_name = ds["projectName"]
                    if project_name and project_name not in projects_dict:
                        projects_dict[project_name] = {
                            "name": project_name,
                            "vizportalUrlId": ds.get("projectVizportalUrlId")
                        }
        
        # ä»å·¥ä½œç°¿æ”¶é›†é¡¹ç›®
        wb_query = """
        {
            workbooks {
                projectName
                projectVizportalUrlId
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "data" in wb_result and wb_result["data"]:
            for wb in (wb_result["data"].get("workbooks") or []):
                if wb and wb.get("projectName"):
                    project_name = wb["projectName"]
                    if project_name and project_name not in projects_dict:
                        projects_dict[project_name] = {
                            "name": project_name,
                            "vizportalUrlId": wb.get("projectVizportalUrlId")
                        }
        
        # ç”Ÿæˆå”¯ä¸€ ID (ä½¿ç”¨ MD5 ä¿è¯ç¨³å®šæ€§)
        result = []
        for name, proj in projects_dict.items():
            # ä½¿ç”¨ MD5 ç”Ÿæˆç¨³å®šçš„ ID (å‰8ä½ä½œä¸º ID)
            import hashlib
            name_hash = hashlib.md5(name.encode('utf-8')).hexdigest()
            proj["id"] = f"project_{name_hash[:8]}"
            result.append(proj)
        
        return result


class MetadataSync:
    """å…ƒæ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, client: TableauMetadataClient, db_path: str = None):
        self.client = client
        self.db_path = db_path or Config.DATABASE_PATH
        self.engine = get_engine(self.db_path)
        self.session = get_session(self.engine)
        self.sync_log: Optional[SyncLog] = None
    
    def _start_sync_log(self, sync_type: str):
        """å¼€å§‹åŒæ­¥æ—¥å¿—"""
        self.sync_log = SyncLog(
            sync_type=sync_type,
            status="running",
            started_at=datetime.now(),
            records_synced=0
        )
        self.session.add(self.sync_log)
        self.session.commit()
    
    def _complete_sync_log(self, records: int, error: str = None):
        """å®ŒæˆåŒæ­¥æ—¥å¿—"""
        if self.sync_log:
            self.sync_log.status = "failed" if error else "completed"
            self.sync_log.completed_at = datetime.utcnow()
            self.sync_log.records_synced = records
            self.sync_log.error_message = error
            self.session.commit()

    def _cleanup_orphaned_records(self, model_class, current_ids: List[str], filter_condition=None):
        """æ¸…ç†æ•°æ®åº“ä¸­å­˜åœ¨ä½†æœ¬æ¬¡åŒæ­¥æœªå‘ç°çš„è®°å½•ï¼ˆç‰©ç†åˆ é™¤ï¼‰"""
        if not current_ids and filter_condition is None:
            return 0
        
        query = self.session.query(model_class)
        if filter_condition is not None:
            query = query.filter(filter_condition)
        
        # è¿‡æ»¤æ‰æœ¬æ¬¡åŒæ­¥å‘ç°çš„ IDs
        orphaned = query.filter(~model_class.id.in_(current_ids)).all()
        
        count = 0
        for record in orphaned:
            # å¯¹äº Fieldï¼Œè¿˜éœ€è¦æ¸…ç†ç›¸å…³çš„ CalculatedField å’Œä¾èµ–
            if model_class == Field:
                self.session.query(CalculatedField).filter_by(field_id=record.id).delete()
                self.session.query(FieldDependency).filter(
                    (FieldDependency.source_field_id == record.id) | 
                    (FieldDependency.dependency_field_id == record.id)
                ).delete()
            
            self.session.delete(record)
            count += 1
            
        if count > 0:
            print(f"  ğŸ§¹ æ¸…ç†äº† {count} ä¸ªå·²ä¸å­˜åœ¨çš„ {model_class.__name__} è®°å½•")
        return count
    
    def sync_databases(self) -> int:
        """åŒæ­¥æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“¦ åŒæ­¥æ•°æ®åº“...")
        self._start_sync_log("databases")
        
        try:
            databases = self.client.fetch_databases()
            count = 0
            current_ids = []
            
            for db_data in databases:
                if not db_data or not db_data.get("id"):
                    continue
                
                current_ids.append(db_data["id"])
                db = self.session.query(Database).filter_by(id=db_data["id"]).first()
                if not db:
                    db = Database(id=db_data["id"])
                    self.session.add(db)
                
                db.name = db_data.get("name", "")
                db.luid = db_data.get("luid")
                db.connection_type = db_data.get("connectionType")
                db.host_name = db_data.get("hostName")
                db.port = db_data.get("port")
                db.service = db_data.get("service")
                db.description = db_data.get("description")
                db.is_certified = db_data.get("isCertified", False)
                db.certification_note = db_data.get("certificationNote")
                db.platform = db_data.get("platform")
                db.updated_at = datetime.now()
                
                count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®åº“è®°å½•
            self._cleanup_orphaned_records(Database, current_ids)
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®åº“")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_tables(self) -> int:
        """åŒæ­¥æ•°æ®è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“‹ åŒæ­¥æ•°æ®è¡¨...")
        self._start_sync_log("tables")
        
        try:
            tables = self.client.fetch_tables()
            table_count = 0
            column_count = 0
            current_ids = []
            
            for table_data in tables:
                if not table_data or not table_data.get("id"):
                    continue
                
                # è¿‡æ»¤æ‰åµŒå…¥å¼è¡¨ (å·²ç»åœ¨å­˜é‡æ¸…ç†ä¸­å¤„ç†è¿‡ï¼Œè¿™é‡Œç¡®ä¿åŒæ­¥ä¹Ÿä¸æ‹‰å–/æ›´æ–°å®ƒä»¬)
                if table_data.get("isEmbedded"):
                    continue
                
                current_ids.append(table_data["id"])
                table = self.session.query(DBTable).filter_by(id=table_data["id"]).first()
                if not table:
                    table = DBTable(id=table_data["id"])
                    self.session.add(table)
                
                table.name = table_data.get("name", "")
                table.luid = table_data.get("luid")
                table.full_name = table_data.get("fullName")
                table.schema = table_data.get("schema")
                
                # å…³è”æ•°æ®åº“
                db_info = table_data.get("database", {})
                if db_info:
                    table.database_id = db_info.get("id")
                    table.connection_type = db_info.get("connectionType", "")
                
                table.table_type = table_data.get("tableType")
                table.description = table_data.get("description")
                table.is_embedded = False # æ˜ç¡®è®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬è¿‡æ»¤æ‰äº†åµŒå…¥å¼è¡¨
                table.is_certified = table_data.get("isCertified", False)
                table.certification_note = table_data.get("certificationNote")
                table.project_name = table_data.get("projectName")
                
                # è§£ææ—¶é—´
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = table_data.get(time_field)
                    if time_val:
                        try:
                            # å…¼å®¹ä¸åŒæ ¼å¼
                            dt = datetime.fromisoformat(time_val.replace("Z", "+00:00"))
                            setattr(table, attr_name, dt)
                        except:
                            pass
                
                # åŒæ­¥åˆ— (Columns)
                columns = table_data.get("columns", [])
                for col_data in columns:
                    if not col_data or not col_data.get("id"):
                        continue
                    
                    col = self.session.query(DBColumn).filter_by(id=col_data["id"]).first()
                    if not col:
                        col = DBColumn(id=col_data["id"])
                        self.session.add(col)
                    
                    col.name = col_data.get("name", "")
                    col.remote_type = col_data.get("remoteType")
                    col.description = col_data.get("description")
                    col.is_nullable = col_data.get("isNullable")
                    col.table_id = table.id
                    column_count += 1
                
                table_count += 1
                if table_count % 100 == 0:
                    self.session.commit()
                    print(f"  - æ•°æ®è¡¨: å·²å¤„ç† {table_count}/{len(tables)}")
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ­£å¼è¡¨è®°å½•ï¼ˆæ’é™¤åµŒå…¥å¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†åŒæ­¥å®ƒä»¬ï¼‰
            self._cleanup_orphaned_records(DBTable, current_ids, filter_condition=(DBTable.is_embedded == False))
            
            self._complete_sync_log(table_count)
            print(f"  âœ… åŒæ­¥ {table_count} ä¸ªæ•°æ®è¡¨, {column_count} ä¸ªåˆ—")
            return table_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_datasources(self) -> int:
        """åŒæ­¥æ•°æ®æºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ”— åŒæ­¥æ•°æ®æº...")
        self._start_sync_log("datasources")
        
        try:
            datasources = self.client.fetch_datasources()
            count = 0
            current_ids = []
            
            for ds_data in datasources:
                if not ds_data or not ds_data.get("id"):
                    continue
                
                # ä»…åŒæ­¥å‘å¸ƒå¼æ•°æ®æº
                if ds_data.get("isEmbedded"):
                    continue
                
                current_ids.append(ds_data["id"])
                ds = self.session.query(Datasource).filter_by(id=ds_data["id"]).first()
                if not ds:
                    ds = Datasource(id=ds_data["id"])
                    self.session.add(ds)
                
                ds.name = ds_data.get("name", "")
                ds.luid = ds_data.get("luid")
                ds.description = ds_data.get("description")
                ds.uri = ds_data.get("uri")
                ds.project_name = ds_data.get("projectName", "")
                ds.has_extract = ds_data.get("hasExtracts", False)
                ds.is_certified = ds_data.get("isCertified", False)
                ds.certification_note = ds_data.get("certificationNote")
                ds.certifier_display_name = ds_data.get("certifierDisplayName")
                ds.contains_unsupported_custom_sql = ds_data.get("containsUnsupportedCustomSql", False)
                ds.has_active_warning = ds_data.get("hasActiveWarning", False)
                ds.vizportal_url_id = ds_data.get("vizportalUrlId")
                ds.is_embedded = False # æ˜ç¡®è®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬è¿‡æ»¤æ‰äº†åµŒå…¥å¼æ•°æ®æº
                
                owner = ds_data.get("owner", {})
                if owner:
                    ds.owner = owner.get("username", "")
                    ds.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [
                    ("extractLastRefreshTime", "extract_last_refresh_time"),
                    ("extractLastIncrementalUpdateTime", "extract_last_incremental_update_time"),
                    ("extractLastUpdateTime", "extract_last_update_time"),
                    ("createdAt", "created_at"),
                    ("updatedAt", "updated_at")
                ]:
                    time_val = ds_data.get(time_field)
                    if time_val:
                        try:
                            setattr(ds, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                count += 1
                
                # åŒæ­¥è¡¨åˆ°æ•°æ®æºçš„å…³ç³»
                upstream_tables = ds_data.get("upstreamTables", [])
                if upstream_tables:
                    print(f"  ğŸ“Š æ•°æ®æº {ds_data.get('name')} çš„ä¸Šæ¸¸è¡¨: {len(upstream_tables)} ä¸ª")
                    # æŠ½æ ·æ‰“å° ID æ ¼å¼
                    if len(upstream_tables) > 0:
                        print(f"     ç¤ºä¾‹è¡¨ ID: {upstream_tables[0].get('id')}")

                for tbl in upstream_tables:
                    if not tbl or not tbl.get("id"):
                        continue
                    rel = self.session.execute(
                        select(table_to_datasource).where(
                            table_to_datasource.c.table_id == tbl["id"],
                            table_to_datasource.c.datasource_id == ds_data["id"]
                        )
                    ).first()
                    
                    if not rel:
                        try:
                            self.session.execute(
                                table_to_datasource.insert().values(
                                    table_id=tbl["id"],
                                    datasource_id=ds_data["id"],
                                    relationship_type="upstream"
                                )
                            )
                        except:
                            pass
                
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®æºï¼ˆæ’é™¤åµŒå…¥å¼ï¼‰
            self._cleanup_orphaned_records(Datasource, current_ids, filter_condition=(Datasource.is_embedded == False))
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®æº")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_workbooks(self) -> int:
        """åŒæ­¥å·¥ä½œç°¿å’Œè§†å›¾ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“Š åŒæ­¥å·¥ä½œç°¿...")
        self._start_sync_log("workbooks")
        
        try:
            workbooks = self.client.fetch_workbooks()
            wb_count = 0
            view_count = 0
            current_wb_ids = []
            current_view_ids = []
            
            for wb_data in workbooks:
                current_wb_ids.append(wb_data["id"])
                wb = self.session.query(Workbook).filter_by(id=wb_data["id"]).first()
                if not wb:
                    wb = Workbook(id=wb_data["id"])
                    self.session.add(wb)
                
                wb.name = wb_data.get("name", "")
                wb.luid = wb_data.get("luid")
                wb.description = wb_data.get("description")
                wb.uri = wb_data.get("uri")
                wb.project_name = wb_data.get("projectName", "")
                wb.contains_unsupported_custom_sql = wb_data.get("containsUnsupportedCustomSql", False)
                wb.has_active_warning = wb_data.get("hasActiveWarning", False)
                wb.vizportal_url_id = wb_data.get("vizportalUrlId")
                
                owner = wb_data.get("owner", {})
                if owner:
                    wb.owner = owner.get("username", "")
                    wb.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = wb_data.get(time_field)
                    if time_val:
                        try:
                            setattr(wb, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                wb_count += 1
                
                # åŒæ­¥æ•°æ®æºåˆ°å·¥ä½œç°¿çš„å…³ç³» (Published)
                upstream_ds = wb_data.get("upstreamDatasources", [])
                for ds in upstream_ds:
                    if not ds or not ds.get("id"):
                        continue
                    self._link_datasource_to_workbook(ds["id"], wb_data["id"])

                # åŒæ­¥åµŒå…¥å¼æ•°æ®æº (Embedded) - æ³¨æ„ï¼šä¸å†ä¸ºåµŒå…¥å¼æ•°æ®æºåˆ›å»º Datasource è®°å½•ï¼ŒåªåŒæ­¥å…¶å­—æ®µ
                embedded_ds = wb_data.get("embeddedDatasources", [])
                for eds in embedded_ds:
                    if not eds or not eds.get("id"):
                        continue
                    
                    # ä»…åŒæ­¥å­—æ®µï¼Œdatasource_id è®¾ä¸º Noneï¼Œworkbook_id è®¾ä¸ºå½“å‰å·¥ä½œç°¿
                    # è¿™ç¬¦åˆç”¨æˆ·â€œåµŒå…¥å¼æ•°æ®æºåŠå…¶å­—æ®µä½œä¸ºå†…éƒ¨å±æ€§â€çš„è¦æ±‚
                    eds_fields = eds.get("fields", [])
                    for f_data in eds_fields:
                       self._sync_field(f_data, datasource_id=None, workbook_id=wb_data["id"])
                
                # åŒæ­¥è§†å›¾ (sheets + dashboards)
                for idx, sheet in enumerate(wb_data.get("sheets", [])):
                    if not sheet or not sheet.get("id"):
                        continue
                    current_view_ids.append(sheet["id"])
                    view = self.session.query(View).filter_by(id=sheet["id"]).first()
                    if not view:
                        view = View(id=sheet["id"])
                        self.session.add(view)
                    
                    view.name = sheet.get("name", "")
                    view.luid = sheet.get("luid")
                    view.path = sheet.get("path")
                    view.index = sheet.get("index", idx)
                    view.view_type = "sheet"
                    view.workbook_id = wb_data["id"]
                    
                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = sheet.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    view_count += 1
                
                # åŒæ­¥ä»ªè¡¨æ¿ (dashboards)
                for idx, dashboard in enumerate(wb_data.get("dashboards", [])):
                    if not dashboard or not dashboard.get("id"):
                        continue
                    current_view_ids.append(dashboard["id"])
                    view = self.session.query(View).filter_by(id=dashboard["id"]).first()
                    if not view:
                        view = View(id=dashboard["id"])
                        self.session.add(view)
                    
                    view.name = dashboard.get("name", "")
                    view.luid = dashboard.get("luid")
                    view.path = dashboard.get("path")
                    view.index = dashboard.get("index", idx)
                    view.view_type = "dashboard"
                    view.workbook_id = wb_data["id"]
                    

                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = dashboard.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    # åŒæ­¥ä»ªè¡¨æ¿ä¸ sheet çš„å…³è”
                    contained_sheets = dashboard.get("sheets", [])
                    for contained_sheet in contained_sheets:
                         if contained_sheet and contained_sheet.get("id"):
                             sheet_id = contained_sheet.get("id")
                             # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                             rel = self.session.execute(
                                 select(dashboard_to_sheet).where(
                                     dashboard_to_sheet.c.dashboard_id == dashboard["id"],
                                     dashboard_to_sheet.c.sheet_id == sheet_id
                                 )
                             ).first()
                             if not rel:
                                 try:
                                     self.session.execute(
                                         dashboard_to_sheet.insert().values(
                                             dashboard_id=dashboard["id"],
                                             sheet_id=sheet_id
                                         )
                                     )
                                 except Exception as e:
                                     print(f"  âš ï¸ å…³è” sheet å¤±è´¥: {e}")
                                     pass
                    
                    view_count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„å·¥ä½œç°¿
            self._cleanup_orphaned_records(Workbook, current_wb_ids)
            # æ¸…ç†å·²ä¸å­˜åœ¨çš„è§†å›¾
            self._cleanup_orphaned_records(View, current_view_ids)
            
            self._complete_sync_log(wb_count)
            print(f"  âœ… åŒæ­¥ {wb_count} ä¸ªå·¥ä½œç°¿, {view_count} ä¸ªè§†å›¾")
            return wb_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
            return 0
    
    def sync_fields(self) -> int:
        """åŒæ­¥å­—æ®µ"""
        print("\nğŸ”¤ åŒæ­¥å­—æ®µ...")
        self._start_sync_log("fields")
        
        try:
            from backend.models import table_to_datasource, Datasource
            # å»ºç«‹ç‰©ç†è¡¨åˆ°å‘å¸ƒå¼æ•°æ®æºçš„æ˜ å°„ï¼Œç”¨äºç©¿é€è¡¥é½
            # ä»…åŒ…å«éåµŒå…¥å¼æ•°æ®æº
            table_real_ds_map = {}
            ds_to_table_rels = self.session.execute(
                select(table_to_datasource.c.table_id, table_to_datasource.c.datasource_id)
                .join(Datasource, Datasource.id == table_to_datasource.c.datasource_id)
                .where(Datasource.is_embedded == 0)
            ).fetchall()
            for tid, dsid in ds_to_table_rels:
                if tid not in table_real_ds_map:
                    table_real_ds_map[tid] = dsid

            fields = self.client.fetch_fields()
            count = 0
            calc_count = 0
            current_ids = []
            
            for f_data in fields:
                if not f_data or not f_data.get("id"):
                    continue
                    
                current_ids.append(f_data["id"])
                
                # è·å–/åˆ›å»º Field è®°å½•
                field = self.session.query(Field).filter_by(id=f_data["id"]).first()
                if not field:
                    field = Field(id=f_data["id"])
                    self.session.add(field)
                
                field.name = f_data.get("name") or ""
                field.description = f_data.get("description") or ""
                
                # è·å–åˆå§‹ datasource_id
                ds_id = f_data.get("datasource_id")
                
                # æ ¹æ®ç±»å‹è§£æå­—æ®µè¯¦æƒ… (æå‰è§£æä»¥ä¾¿è·å– table_id å’Œ schema ç©¿é€)
                typename = f_data.get("__typename")
                target_table_id = None
                
                if typename == "ColumnField":
                    # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
                    upstream_cols = f_data.get("upstreamColumns") or []
                    if upstream_cols and len(upstream_cols) > 0:
                        first_col = upstream_cols[0]
                        if first_col:
                            field.upstream_column_id = first_col.get("id")
                            field.upstream_column_name = first_col.get("name")
                            table_info = first_col.get("table")
                            if table_info:
                                target_table_id = table_info.get("id")
                                field.table_id = target_table_id

                # è¡€ç¼˜è¡¥é½ï¼šå¦‚æœå½“å‰ datasource_id æŒ‡å‘çš„ä¸æ˜¯å‘å¸ƒå¼ï¼ˆæˆ–ä¸å­˜åœ¨ï¼‰ï¼Œå°è¯•é€šè¿‡ table_id æ‰¾å‘å¸ƒå¼
                if ds_id:
                    exists = self.session.query(Datasource).filter_by(id=ds_id, is_embedded=0).first()
                    if not exists and target_table_id in table_real_ds_map:
                        ds_id = table_real_ds_map[target_table_id]
                
                field.datasource_id = ds_id
                
                # é»˜è®¤å€¼
                field.data_type = ""
                field.role = ""
                field.is_calculated = False
                field.formula = ""
                field.is_hidden = False
                field.folder_name = f_data.get("folderName")
                field.fully_qualified_name = ""
                
                # æ ¹æ®ç±»å‹è§£æå­—æ®µ
                typename = f_data.get("__typename")
                if typename == "CalculatedField":
                    field.is_calculated = True
                    field.formula = f_data.get("formula") or ""
                    field.data_type = f_data.get("dataType") or ""
                    field.role = f_data.get("role") or ""
                    field.is_hidden = f_data.get("isHidden") or False
                    field.folder_name = f_data.get("folderName")
                    
                    # æŒ‡æ ‡è¡€ç¼˜ç©¿é€ï¼šé€šè¿‡ upstreamFields æ‰¾ç‰©ç†æ•°æ®æº
                    upstream_fields = f_data.get("upstreamFields") or []
                    for uf in upstream_fields:
                        if uf and uf.get("datasource"):
                            ref_ds_id = uf["datasource"].get("id")
                            # å¦‚æœå¼•ç”¨çš„æ˜¯å‘å¸ƒå¼æ•°æ®æºï¼Œåˆ™ä»¥æ­¤ä¸ºå‡†
                            if ref_ds_id:
                                exists = self.session.query(Datasource).filter_by(id=ref_ds_id, is_embedded=0).first()
                                if exists:
                                    ds_id = ref_ds_id
                                    break
                elif typename == "ColumnField":
                    field.data_type = f_data.get("dataType") or ""
                    field.role = f_data.get("role") or ""
                    field.is_hidden = f_data.get("isHidden") or False
                    field.folder_name = f_data.get("folderName")
                    
                    # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
                    upstream_cols = f_data.get("upstreamColumns") or []
                    if upstream_cols and len(upstream_cols) > 0:
                        first_col = upstream_cols[0]
                        if first_col:
                            field.upstream_column_id = first_col.get("id")
                            field.upstream_column_name = first_col.get("name")
                            
                            # ä»æœ¬åœ° DBColumn è·å– Remote Type (é¿å… API schema é”™è¯¯)
                            if field.upstream_column_id:
                                from backend.models import DBColumn
                                db_col = self.session.query(DBColumn).filter_by(id=field.upstream_column_id).first()
                                if db_col and db_col.remote_type:
                                    field.remote_type = db_col.remote_type
                            
                            table_info = first_col.get("table")
                            if table_info:
                                field.table_id = table_info.get("id")
                
                count += 1
                if count % 1000 == 0:
                    self.session.commit()
                    print(f"  - å­—æ®µ: å·²å¤„ç† {count}/{len(fields)}")
                
                # å¤„ç†è®¡ç®—å­—æ®µ
                if f_data.get("isCalculated"):
                    calc_field = self.session.query(CalculatedField).filter_by(
                        field_id=f_data["id"]
                    ).first()
                    if not calc_field:
                        calc_field = CalculatedField(field_id=f_data["id"])
                        self.session.add(calc_field)
                    
                    calc_field.name = f_data.get("name") or ""
                    calc_field.formula = f_data.get("formula") or ""
                    calc_count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µ (å…¶ä¸­ {calc_count} ä¸ªè®¡ç®—å­—æ®µ)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_calculated_fields(self) -> int:
        """åŒæ­¥è®¡ç®—å­—æ®µ"""
        print("\nğŸ“ åŒæ­¥è®¡ç®—å­—æ®µ...")
        self._start_sync_log("calculated_fields")
        
        try:
            calc_fields = self.client.fetch_calculated_fields()
            count = 0
            
            for cf_data in calc_fields:
                if not cf_data or not cf_data.get("id"):
                    continue
                
                # å…ˆç¡®ä¿ Field è®°å½•å­˜åœ¨
                field = self.session.query(Field).filter_by(id=cf_data["id"]).first()
                if not field:
                    field = Field(id=cf_data["id"])
                    self.session.add(field)
                
                field.name = cf_data.get("name") or ""
                field.description = cf_data.get("description") or ""
                field.data_type = cf_data.get("dataType") or ""
                field.is_calculated = True
                field.formula = cf_data.get("formula") or ""
                field.role = cf_data.get("role") or ""
                field.datasource_id = cf_data.get("datasource_id")
                
                # æ›´æ–°/åˆ›å»º CalculatedField è®°å½•
                calc_field = self.session.query(CalculatedField).filter_by(
                    field_id=cf_data["id"]
                ).first()
                if not calc_field:
                    calc_field = CalculatedField(field_id=cf_data["id"])
                    self.session.add(calc_field)
                
                calc_field.name = cf_data.get("name") or ""
                calc_field.formula = cf_data.get("formula") or ""
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªè®¡ç®—å­—æ®µ")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_field_to_view(self) -> int:
        """åŒæ­¥å­—æ®µåˆ°è§†å›¾çš„å…³è”å…³ç³»"""
        print("\nğŸ”— åŒæ­¥å­—æ®µâ†’è§†å›¾å…³è”...")
        self._start_sync_log("field_to_view")
        
        try:
            view_fields = self.client.fetch_views_with_fields()
            count = 0
            skipped = 0
            
            for vf in view_fields:
                field_id = vf.get("field_id")
                view_id = vf.get("view_id")
                
                if not field_id or not view_id:
                    skipped += 1
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = self.session.execute(
                    select(field_to_view).where(
                        field_to_view.c.field_id == field_id,
                        field_to_view.c.view_id == view_id
                    )
                ).first()
                
                if not existing:
                    try:
                        self.session.execute(
                            field_to_view.insert().values(
                                field_id=field_id,
                                view_id=view_id,
                                used_in_formula=False
                            )
                        )
                        count += 1
                    except Exception as e:
                        # å¿½ç•¥å¤–é”®çº¦æŸé”™è¯¯ï¼ˆå­—æ®µæˆ–è§†å›¾ä¸å­˜åœ¨ï¼‰
                        skipped += 1
                        continue
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µâ†’è§†å›¾å…³è” (è·³è¿‡ {skipped} ä¸ª)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_users(self) -> int:
        """åŒæ­¥ Tableau ç”¨æˆ·"""
        print("\nğŸ‘¥ åŒæ­¥ç”¨æˆ·...")
        self._start_sync_log("users")
        
        try:
            users = self.client.fetch_users()
            count = 0
            
            for u_data in users:
                if not u_data or not u_data.get("id"):
                    continue
                    
                user = self.session.query(TableauUser).filter_by(id=u_data["id"]).first()
                if not user:
                    user = TableauUser(id=u_data["id"])
                    self.session.add(user)
                
                user.luid = u_data.get("luid")
                user.name = u_data.get("username") or u_data.get("name") or ""
                user.display_name = u_data.get("name")
                user.email = u_data.get("email")
                user.domain = u_data.get("domain")
                user.site_role = u_data.get("siteRole")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªç”¨æˆ·")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_projects(self) -> int:
        """åŒæ­¥ Tableau é¡¹ç›®"""
        print("\nğŸ“ åŒæ­¥é¡¹ç›®...")
        self._start_sync_log("projects")
        
        try:
            projects = self.client.fetch_projects()
            count = 0
            
            for p_data in projects:
                if not p_data or not p_data.get("name"):
                    continue
                
                project_id = p_data.get("id")
                
                project = self.session.query(Project).filter_by(id=project_id).first()
                if not project:
                    project = Project(id=project_id)
                    self.session.add(project)
                
                project.name = p_data.get("name") or ""
                project.vizportal_url_id = p_data.get("vizportalUrlId")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªé¡¹ç›®")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def sync_lineage(self) -> int:
        """åŒæ­¥æŒ‡æ ‡ä¸è¡€ç¼˜å…³ç³» (DBæŒä¹…åŒ–)"""
        print("\nğŸ•¸ï¸ åŒæ­¥è¡€ç¼˜ä¸æŒ‡æ ‡å…³ç³»...")
        count = 0
        
        try:
            # 1. æ¸…ç†ç°æœ‰ä¾èµ–å…³ç³» (å…¨é‡åŒæ­¥ç­–ç•¥)
            self.session.query(FieldDependency).delete()
            self.session.query(Metric).delete() # é‡æ–°æ„å»ºæŒ‡æ ‡è¡¨
            self.session.commit()
            
            # 2. è·å–æ‰€æœ‰è®¡ç®—å­—æ®µ
            calc_fields = self.session.query(CalculatedField, Field).join(
                Field, CalculatedField.field_id == Field.id
            ).all()
            
            # æ„å»ºå­—æ®µç´¢å¼• (Name -> ID lookup cache)
            all_fields = self.session.query(Field).all()
            field_map = {} # (datasource_id, name) -> field_id
            global_field_map = {} # name -> field_id (fallback)
            
            for f in all_fields:
                key = (f.datasource_id, f.name)
                field_map[key] = f.id
                global_field_map[f.name] = f.id
            
            for calc, field in calc_fields:
                formula = calc.formula
                if not formula:
                    continue
                    
                # A. è¯†åˆ« Metric
                # è§„åˆ™: è®¡ç®—å­—æ®µ ä¸” Role=Measure
                if field.role == 'measure':
                    metric = Metric(
                        id=field.id, # å¤ç”¨ Field ID
                        name=field.name,
                        description=field.description,
                        formula=formula,
                        metric_type='Calculated',
                        owner=field.datasource.owner if field.datasource else None
                    )
                    self.session.merge(metric)
                
                # B. è§£æä¾èµ– (åç«¯æŒä¹…åŒ–)
                refs = re.findall(r'\[(.*?)\]', formula)
                unique_refs = set(refs)
                
                for ref_name in unique_refs:
                    dep_id = None
                    
                    # 1. å°è¯•åŒæ•°æ®æºåŒ¹é…
                    if field.datasource_id:
                        dep_id = field_map.get((field.datasource_id, ref_name))
                    
                    # 2. å°è¯•å…¨å±€åŒ¹é…
                    if not dep_id:
                        dep_id = global_field_map.get(ref_name)
                    
                    # 3. åˆ›å»ºä¾èµ–è®°å½•
                    dependency = FieldDependency(
                        source_field_id=field.id,
                        dependency_field_id=dep_id, 
                        dependency_name=ref_name,
                        dependency_type='formula'
                    )
                    self.session.add(dependency)
                    count += 1
            
            self.session.commit()
            print(f"  âœ… åŒæ­¥ {count} æ¡ä¾èµ–å…³ç³»")
            return count
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ è¡€ç¼˜åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _link_datasource_to_workbook(self, datasource_id: str, workbook_id: str):
        """å»ºç«‹æ•°æ®æºä¸å·¥ä½œç°¿çš„å…³è”"""
        rel = self.session.execute(
            select(datasource_to_workbook).where(
                datasource_to_workbook.c.datasource_id == datasource_id,
                datasource_to_workbook.c.workbook_id == workbook_id
            )
        ).first()
        if not rel:
            try:
                self.session.execute(
                    datasource_to_workbook.insert().values(
                        datasource_id=datasource_id,
                        workbook_id=workbook_id
                    )
                )
            except:
                pass

    def _sync_field(self, f_data: Dict, datasource_id: str = None, workbook_id: str = None):
        """åŒæ­¥å•ä¸ªå­—æ®µï¼ˆåµŒå…¥å¼æˆ–å‘å¸ƒï¼‰"""
        if not f_data or not f_data.get("id"):
            return

        field = self.session.query(Field).filter_by(id=f_data["id"]).first()
        if not field:
            field = Field(id=f_data["id"])
            self.session.add(field)
        
        field.name = f_data.get("name") or ""
        field.description = f_data.get("description") or ""
        field.datasource_id = datasource_id
        field.workbook_id = workbook_id
        
        # é»˜è®¤å€¼
        if not field.data_type: field.data_type = ""
        if not field.role: field.role = ""
        field.is_calculated = False
        if not field.formula: field.formula = ""
        field.is_hidden = False
        field.folder_name = f_data.get("folderName")
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µ
        typename = f_data.get("__typename")
        # æŸäº› embedded field å¯èƒ½æ²¡æœ‰ __typenameï¼Œå°è¯•æ¨æ–­æˆ–è¯»å–ç›´æ¥å±æ€§
        if typename == "CalculatedField" or f_data.get("formula"):
            field.is_calculated = True
            field.formula = f_data.get("formula") or ""
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = f_data.get("role") or field.role
            field.is_hidden = f_data.get("isHidden") or False
            
            # ç¡®ä¿ CalculatedField è®°å½•
            calc_sub = self.session.query(CalculatedField).filter_by(field_id=field.id).first()
            if not calc_sub:
                calc_sub = CalculatedField(field_id=field.id)
                self.session.add(calc_sub)
            calc_sub.name = field.name
            calc_sub.formula = field.formula

        elif typename == "ColumnField" or f_data.get("remoteType"):
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = f_data.get("role") or field.role
            field.is_hidden = f_data.get("isHidden") or False
            # åµŒå…¥å¼åˆ—é€šå¸¸æ²¡æœ‰ upstreamColumns å› ä¸ºå®ƒæ˜¯ç›´æ¥è¿æ¥

    
    def sync_all(self):
        """å…¨é‡åŒæ­¥æ‰€æœ‰å®ä½“"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ Tableau Metadata")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # æŒ‰ä¾èµ–é¡ºåºåŒæ­¥
        user_count = self.sync_users()  # å…ˆåŒæ­¥ç”¨æˆ·
        project_count = self.sync_projects()  # åŒæ­¥é¡¹ç›®
        db_count = self.sync_databases()
        table_count = self.sync_tables()
        ds_count = self.sync_datasources()
        wb_count = self.sync_workbooks()
        field_count = self.sync_fields()
        calc_count = self.sync_calculated_fields()
        calc_count = self.sync_calculated_fields()
        ftv_count = self.sync_field_to_view()
        lineage_count = self.sync_lineage()
        
        duration = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print("ğŸ“ˆ åŒæ­¥å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"  ç”¨æˆ·:   {user_count}")
        print(f"  é¡¹ç›®:   {project_count}")
        print(f"  æ•°æ®åº“: {db_count}")
        print(f"  æ•°æ®è¡¨: {table_count}")
        print(f"  æ•°æ®æº: {ds_count}")
        print(f"  å·¥ä½œç°¿: {wb_count}")
        print(f"  è¡€ç¼˜:   {lineage_count}")
        print(f"  å­—æ®µ:   {field_count}")
        print(f"  è®¡ç®—å­—æ®µ: {calc_count}")
        print(f"  å­—æ®µâ†’è§†å›¾: {ftv_count}")
        print(f"  è€—æ—¶: {duration:.2f} ç§’")
        print("=" * 60)
        
        # åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰
        self.sync_views_usage()
        
        # æœ€åï¼šè®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ
        self.calculate_stats()
    
    def sync_views_usage(self) -> int:
        """åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰å¹¶è®°å½•å†å²å¿«ç…§"""
        print("\nğŸ“Š åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ (REST API)...")
        
        try:
            usage_map = self.client.fetch_views_usage()
            
            if not usage_map:
                print("  âš ï¸ æœªè·å–åˆ°è§†å›¾ä½¿ç”¨ç»Ÿè®¡")
                return 0
            
            updated = 0
            history_count = 0
            views = self.session.query(View).all()
            
            for view in views:
                # REST API è¿”å›çš„æ˜¯ luidï¼Œéœ€è¦åŒ¹é…
                if view.luid and view.luid in usage_map:
                    new_count = usage_map[view.luid]
                    
                    # åªæœ‰å½“è®¿é—®æ¬¡æ•°å‘ç”Ÿå˜åŒ–æ—¶æ‰è®°å½•å†å²
                    if view.total_view_count != new_count:
                        # è®°å½•å†å²å¿«ç…§
                        from backend.models import ViewUsageHistory
                        history = ViewUsageHistory(
                            view_id=view.id,
                            view_luid=view.luid,
                            total_view_count=new_count
                        )
                        self.session.add(history)
                        history_count += 1
                    
                    view.total_view_count = new_count
                    updated += 1
            
            self.session.commit()
            print(f"  âœ… æ›´æ–° {updated} ä¸ªè§†å›¾çš„ä½¿ç”¨ç»Ÿè®¡, è®°å½• {history_count} æ¡å†å²")
            
            # å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
            self._propagate_dashboard_views()
            
            return updated
            
        except Exception as e:
            print(f"  âŒ åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _propagate_dashboard_views(self):
        """å°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
        
        é€»è¾‘ï¼šå¦‚æœä¸€ä¸ªsheetè¢«åŒ…å«åœ¨ä»ªè¡¨ç›˜ä¸­ï¼Œç”¨æˆ·è®¿é—®ä»ªè¡¨ç›˜æ—¶ä¹Ÿç›¸å½“äºè®¿é—®äº†è¿™äº›sheetã€‚
        å› æ­¤ï¼Œå°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡å®Œæ•´ç´¯åŠ åˆ°æ¯ä¸ªåŒ…å«çš„sheetä¸Šã€‚
        """
        print("  ğŸ“ˆ å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°åŒ…å«çš„sheet...")
        
        try:
            from sqlalchemy import text
            
            # ä½¿ç”¨ SQL ç›´æ¥æ›´æ–°ï¼Œæ›´é«˜æ•ˆ
            # å¯¹äºæ¯ä¸ªè¢«ä»ªè¡¨ç›˜åŒ…å«çš„sheetï¼Œç´¯åŠ æ‰€æœ‰åŒ…å«å®ƒçš„ä»ªè¡¨ç›˜çš„è®¿é—®é‡
            update_sql = text("""
                UPDATE views
                SET total_view_count = COALESCE(total_view_count, 0) + COALESCE((
                    SELECT SUM(COALESCE(d.total_view_count, 0))
                    FROM dashboard_to_sheet ds
                    JOIN views d ON ds.dashboard_id = d.id
                    WHERE ds.sheet_id = views.id
                ), 0)
                WHERE id IN (SELECT DISTINCT sheet_id FROM dashboard_to_sheet)
            """)
            
            result = self.session.execute(update_sql)
            self.session.commit()
            
            # ç»Ÿè®¡å—å½±å“çš„sheetæ•°é‡
            affected = self.session.execute(text(
                "SELECT COUNT(DISTINCT sheet_id) FROM dashboard_to_sheet"
            )).scalar()
            
            print(f"  âœ… å·²å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ° {affected} ä¸ªsheet")
            
        except Exception as e:
            print(f"  âš ï¸ åˆ†æ‘Šä»ªè¡¨ç›˜è®¿é—®é‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def calculate_stats(self):
        """è®¡ç®—å¹¶æ›´æ–°é¢„å­˜ç»Ÿè®¡å­—æ®µï¼ˆåŒæ­¥ç»“æŸåè°ƒç”¨ï¼‰"""
        print("\nğŸ“Š è®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ...")
        
        try:
            # ========== Workbook ç»Ÿè®¡ ==========
            workbooks = self.session.query(Workbook).all()
            for wb in workbooks:
                wb.view_count = len(wb.views) if wb.views else 0
                wb.datasource_count = len(wb.datasources) if wb.datasources else 0
                
                # ç»Ÿè®¡å­—æ®µå’ŒæŒ‡æ ‡ï¼ˆæ’é™¤åµŒå…¥å¼æ•°æ®æºä¸­çš„é‡å¤ï¼‰
                field_ids = set()
                metric_ids = set()
                
                # æ–¹æ¡ˆ1ï¼šé€šè¿‡å…³è”çš„æ•°æ®æºç»Ÿè®¡ï¼ˆæ›´å‡†ç¡®ä¸”åŒ…å«æœªå¼•ç”¨çš„èµ„äº§ï¼‰
                for ds in (wb.datasources or []):
                    # ä»…ç»Ÿè®¡éåµŒå…¥å¼æ•°æ®æºï¼Œé™¤éå·¥ä½œç°¿æœ¬èº«æ²¡æœ‰å‘å¸ƒå¼æ•°æ®æº
                    if ds.is_embedded and len([d for d in wb.datasources if not d.is_embedded]) > 0:
                        continue
                        
                    for f in (ds.fields or []):
                        if f.is_calculated:
                            if f.role == 'measure' or f.role is None:
                                metric_ids.add(f.id)
                        else:
                            field_ids.add(f.id)
                
                # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°è§†å›¾å¼•ç”¨ï¼ˆå¦‚æœä¸Šè¿°ä¸ºç©ºï¼‰
                if len(field_ids) == 0 and len(metric_ids) == 0:
                    for v in (wb.views or []):
                        for f in (v.fields or []):
                            if f.is_calculated:
                                if f.role == 'measure' or f.role is None:
                                    metric_ids.add(f.id)
                            else:
                                field_ids.add(f.id)
                
                wb.field_count = len(field_ids)
                wb.metric_count = len(metric_ids)

            
            # ========== Datasource ç»Ÿè®¡ ==========
            datasources = self.session.query(Datasource).all()
            for ds in datasources:
                ds.table_count = len(ds.tables) if ds.tables else 0
                ds.workbook_count = len(ds.workbooks) if ds.workbooks else 0
                
                field_count = 0
                metric_count = 0
                for f in (ds.fields or []):
                    if f.is_calculated:
                        if f.role == 'measure' or f.role is None:
                            metric_count += 1
                    else:
                        field_count += 1
                ds.field_count = field_count
                ds.metric_count = metric_count
            
            # ========== Field & CalculatedField æ·±åº¦ç»Ÿè®¡ (æŒ‡æ ‡é¢„è®¡ç®—ä¼˜åŒ–) ==========
            print("  - è®¡ç®—å­—æ®µå’ŒæŒ‡æ ‡æ·±åº¦ç»Ÿè®¡...")
            
            # 1. è®¡ç®—å­—æ®µå…¬å¼å“ˆå¸ŒåŠæŸ¥é‡
            calc_fields = self.session.query(CalculatedField).all()
            formula_map = defaultdict(list)
            for cf in calc_fields:
                if cf.formula:
                    # æ ‡å‡†åŒ–å…¬å¼å¹¶è®¡ç®—å“ˆå¸Œ
                    formula_clean = cf.formula.strip()
                    h = hashlib.md5(formula_clean.encode('utf-8')).hexdigest()
                    cf.formula_hash = h
                    formula_map[h].append(cf)
            
            # æ›´æ–°æŸ¥é‡ä¿¡æ¯
            for h, cfs in formula_map.items():
                is_duplicate = len(cfs) > 1
                for cf in cfs:
                    cf.has_duplicates = is_duplicate
                    cf.duplicate_count = len(cfs) - 1
            
            # 2. ç»Ÿè®¡å­—æ®µè¢«è§†å›¾å¼•ç”¨çš„æ¬¡æ•° (usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°è§†å›¾å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET usage_count = (
                    SELECT COUNT(*) FROM field_to_view 
                    WHERE field_to_view.field_id = fields.id
                )
            """))
            
            # 3. ç»Ÿè®¡å­—æ®µè¢«æŒ‡æ ‡å¼•ç”¨çš„æ¬¡æ•° (metric_usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET metric_usage_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_name = fields.name
                )
            """))
            
            # 4. ç»Ÿè®¡æŒ‡æ ‡ä¾èµ–æ•° (dependency_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡ä¾èµ–æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET dependency_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.source_field_id = calculated_fields.field_id
                )
            """))

            # 5. ç»Ÿè®¡æŒ‡æ ‡å¼•ç”¨æ•° (reference_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET reference_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_field_id = calculated_fields.field_id
                )
            """))

            self.session.commit()
            print(f"  âœ… å·²æ›´æ–° {len(workbooks)} ä¸ªå·¥ä½œç°¿, {len(datasources)} ä¸ªæ•°æ®æº, {len(calc_fields)} ä¸ªè®¡ç®—å­—æ®µçš„ç»Ÿè®¡å­—æ®µ")
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()


def main():
    """ä¸»å‡½æ•° - æ‰§è¡ŒåŒæ­¥"""
    print("\n" + "=" * 60)
    print("Tableau Metadata åŒæ­¥å·¥å…·")
    print("=" * 60)
    
    # é…ç½®
    BASE_URL = "http://tbi.juneyaoair.com"
    USERNAME = "huangguanru"
    PASSWORD = "Admin123"
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = TableauMetadataClient(BASE_URL, USERNAME, PASSWORD)
    
    # ç™»å½•
    if not client.sign_in():
        print("æ— æ³•ç™»å½•ï¼Œé€€å‡º")
        return
    
    try:
        # åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
        sync = MetadataSync(client)
        
        # æ‰§è¡Œå…¨é‡åŒæ­¥
        sync.sync_all()
        
        sync.close()
        
    finally:
        # ç™»å‡º
        client.sign_out()


if __name__ == "__main__":
    main()
