"""
Tableau Metadata API æ•°æ®åŒæ­¥æ¨¡å—
ä» Tableau Server æŠ“å–å…ƒæ•°æ®å¹¶å­˜å…¥æœ¬åœ°æ•°æ®åº“
"""
import os
import sys
import json
import requests
import hashlib
from collections import defaultdict
from datetime import datetime
from typing import Optional, List, Dict, Any
from sqlalchemy import select, text
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from backend.config import Config
from backend.models import (
    Base, get_engine, init_db, get_session,
    Database, DBTable, DBColumn, Field, Datasource, Workbook, View,
    TableauUser, Project,
    table_to_datasource, datasource_to_workbook, field_to_view, CalculatedField, SyncLog,
    FieldDependency, Metric, dashboard_to_sheet
)


class TableauMetadataClient:
    """Tableau Metadata API å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str, username: str = None, password: str = None, 
                 pat_name: str = None, pat_secret: str = None):
        self.base_url = base_url.rstrip('/')
        self.username = username
        self.password = password
        self.pat_name = pat_name
        self.pat_secret = pat_secret
        self.auth_token: Optional[str] = None
        self.site_id: Optional[str] = None
        self.api_version = "3.10"
    
    def sign_in(self) -> bool:
        """ç™»å½•è·å–è®¤è¯ token (æ”¯æŒç”¨æˆ·åå¯†ç æˆ– PAT)"""
        signin_url = f"{self.base_url}/api/{self.api_version}/auth/signin"
        
        # æ ¹æ®é…ç½®é€‰æ‹©è®¤è¯æ–¹å¼
        if self.pat_name and self.pat_secret:
            # PAT è®¤è¯
            payload = {
                "credentials": {
                    "personalAccessTokenName": self.pat_name,
                    "personalAccessTokenSecret": self.pat_secret,
                    "site": {"contentUrl": ""}
                }
            }
            print(f"  ä½¿ç”¨ PAT è®¤è¯: {self.pat_name}")
        else:
            # ç”¨æˆ·åå¯†ç è®¤è¯
            payload = {
                "credentials": {
                    "name": self.username,
                    "password": self.password,
                    "site": {"contentUrl": ""}
                }
            }
            print(f"  ä½¿ç”¨ç”¨æˆ·åå¯†ç è®¤è¯: {self.username}")
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        try:
            response = requests.post(signin_url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                credentials = data.get("credentials", {})
                self.auth_token = credentials.get("token")
                self.site_id = credentials.get("site", {}).get("id")
                print(f"âœ… ç™»å½•æˆåŠŸ (Token: {self.auth_token[:20]}...)")
                return True
            else:
                print(f"âŒ ç™»å½•å¤±è´¥: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ ç™»å½•å¼‚å¸¸: {e}")
            return False
    
    def sign_out(self):
        """ç™»å‡ºé‡Šæ”¾ token"""
        if not self.auth_token:
            return
        
        signout_url = f"{self.base_url}/api/{self.api_version}/auth/signout"
        headers = {"X-Tableau-Auth": self.auth_token}
        
        try:
            response = requests.post(signout_url, headers=headers, timeout=30)
            if response.status_code == 204:
                print("âœ… å·²ç™»å‡º")
        except Exception as e:
            print(f"ç™»å‡ºå¼‚å¸¸: {e}")
        finally:
            self.auth_token = None
    
    def execute_query(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œ GraphQL æŸ¥è¯¢"""
        if not self.auth_token:
            raise RuntimeError("æœªç™»å½•ï¼Œè¯·å…ˆè°ƒç”¨ sign_in()")
        
        url = f"{self.base_url}/api/metadata/graphql"
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Tableau-Auth": self.auth_token
        }
        
        response = requests.post(url, headers=headers, json={"query": query}, timeout=60)
        
        if response.status_code == 200:
            return response.json()
        else:
            raise RuntimeError(f"GraphQL æŸ¥è¯¢å¤±è´¥: {response.status_code} - {response.text}")
    
    def fetch_views_usage(self) -> Dict[str, int]:
        """ä» REST API è·å–è§†å›¾ä½¿ç”¨ç»Ÿè®¡ (REST API)"""
        if not self.auth_token or not self.site_id:
            raise RuntimeError("æœªç™»å½•ï¼Œè¯·å…ˆè°ƒç”¨ sign_in()")
        
        # REST API endpoint for views
        url = f"{self.base_url}/api/{self.api_version}/sites/{self.site_id}/views"
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-Tableau-Auth": self.auth_token
        }
        
        usage_map = {}
        page_number = 1
        page_size = 100
        
        print(f"  æ­£åœ¨è°ƒç”¨ REST API è·å–è®¿é—®ç»Ÿè®¡: {url}")
        
        while True:
            params = {
                "pageNumber": page_number,
                "pageSize": page_size,
                "includeUsageStatistics": "true"
            }
            
            try:
                response = requests.get(url, headers=headers, params=params, timeout=30)
                
                if response.status_code != 200:
                    print(f"  âŒ REST API è·å–å¤±è´¥: {response.status_code} - {response.text}")
                    break
                
                data = response.json()
                views = data.get("views", {}).get("view", [])
                
                if not views:
                    break
                
                for view in views:
                    luid = view.get("id")
                    usage = view.get("usage", {})
                    # usage å¯èƒ½æ˜¯ Noneï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ totalViewCount
                    if usage:
                        total_count = usage.get("totalViewCount", 0)
                        if luid:
                            usage_map[luid] = int(total_count)
                
                # Check pagination
                pagination = data.get("pagination", {})
                total_available = int(pagination.get("totalAvailable", 0))
                
                print(f"    - Page {page_number}: è·å– {len(views)} ä¸ªè§†å›¾, æ€»è¿›åº¦ {len(usage_map)}/{total_available}")
                
                if len(usage_map) >= total_available or len(views) < page_size:
                    break
                    
                page_number += 1
                
            except Exception as e:
                print(f"  âŒ è·å–è§†å›¾ç»Ÿè®¡å¼‚å¸¸: {e}")
                break
                
        return usage_map    
    def fetch_databases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            databaseServers {
                id
                luid
                name
                connectionType
                hostName
                port
                service
                description
                isCertified
                certificationNote
            }
        }
        """
        result = self.execute_query(query)
        # å…¼å®¹å¤„ç†ï¼šå…ˆå°è¯• databaseServersï¼Œå¤±è´¥åˆ™å›é€€åˆ° databases
        data = result.get("data", {})
        servers = data.get("databaseServers")
        if servers is not None:
            return servers
        # å›é€€åˆ°æ—§æŸ¥è¯¢
        return self._fetch_databases_fallback()
    
    def _fetch_databases_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨æ—§ç‰ˆæŸ¥è¯¢è·å–æ•°æ®åº“"""
        query = """
        {
            databases {
                id
                name
                connectionType
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("databases", [])
    
    def fetch_tables(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ•°æ®è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            databaseTables {
                id
                luid
                name
                schema
                fullName
                description
                isEmbedded
                projectName
                database {
                    id
                    name
                    connectionType
                }
                columns {
                    id
                    name
                    description
                    remoteType
                    isNullable
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ï¼ˆæŸäº›å­—æ®µå¯èƒ½ä¸è¢«æ”¯æŒï¼‰
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š: {result['errors']}")
            # å°è¯•ç®€åŒ–æŸ¥è¯¢
            return self._fetch_tables_fallback()
        
        return result.get("data", {}).get("databaseTables", [])
    
    def _fetch_tables_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨ç®€åŒ–æŸ¥è¯¢è·å–è¡¨ï¼ˆåŒ…å«åˆ—ä¿¡æ¯ï¼‰"""
        query = """
        {
            databaseTables {
                id
                name
                schema
                fullName
                isEmbedded
                database {
                    id
                    name
                    connectionType
                }
                columns {
                    id
                    name
                    remoteType
                    description
                    isNullable
                }
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("databaseTables", [])
    
    def fetch_datasources(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·²å‘å¸ƒæ•°æ®æºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        query = """
        {
            publishedDatasources {
                id
                luid
                name
                description
                uri
                projectName
                hasExtracts
                extractLastRefreshTime
                extractLastIncrementalUpdateTime
                extractLastUpdateTime
                isCertified
                certificationNote
                certifierDisplayName
                containsUnsupportedCustomSql
                hasActiveWarning
                createdAt
                updatedAt
                vizportalUrlId
                owner {
                    id
                    username
                    name
                }
                upstreamTables {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯å¹¶å›é€€
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š: {result['errors']}")
            return self._fetch_datasources_fallback()
        
        return result.get("data", {}).get("publishedDatasources", [])
    
    def _fetch_datasources_fallback(self) -> List[Dict]:
        """å›é€€ï¼šä½¿ç”¨ç®€åŒ–æŸ¥è¯¢è·å–æ•°æ®æº"""
        query = """
        {
            publishedDatasources {
                id
                name
                projectName
                hasExtracts
                extractLastRefreshTime
                isCertified
                owner {
                    username
                }
                upstreamTables {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        return result.get("data", {}).get("publishedDatasources", [])
    
    def fetch_workbooks(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å·¥ä½œç°¿ï¼ˆä¼˜åŒ–ç‰ˆï¼šRobust Aliased Chunking + Null Owner Fallbackï¼‰"""
        all_workbooks = []
        
        print(f"  æ­£åœ¨è·å–å·¥ä½œç°¿åˆ—è¡¨...")
        # 1. è·å–æ‰€æœ‰ ID (Safe Query)
        list_query = """
        {
            workbooks {
                id
                name
            }
        }
        """
        list_result = self.execute_query(list_query)
        if "errors" in list_result and not list_result.get("data"):
            print(f"  âš ï¸ è·å–å·¥ä½œç°¿åˆ—è¡¨å¤±è´¥: {list_result['errors']}")
            return []
            
        workbooks_meta = list_result.get("data", {}).get("workbooks") or []
        print(f"  éœ€åŒæ­¥ {len(workbooks_meta)} ä¸ªå·¥ä½œç°¿è¯¦æƒ…...")
        
        # 2. åˆ†æ‰¹è·å–è¯¦æƒ…
        chunk_size = 10
        total = len(workbooks_meta)
        
        for i in range(0, total, chunk_size):
            chunk = workbooks_meta[i:i+chunk_size]
            
            # å°è¯•æ‰¹é‡è·å– (å« metadata)
            try:
                self._fetch_workbooks_chunk(chunk, all_workbooks, include_owner=True)
            except Exception as e:
                print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} é‡åˆ°é”™è¯¯ï¼Œå°è¯•é™çº§é‡è¯• (ä¸å« Owner)...")
                try:
                    self._fetch_workbooks_chunk(chunk, all_workbooks, include_owner=False)
                except Exception as e2:
                    print(f"  âŒ æ‰¹æ¬¡ {i//chunk_size + 1} å½»åº•å¤±è´¥: {e2}")

            print(f"    - å·¥ä½œç°¿: å·²å¤„ç† {min(i+chunk_size, total)}/{total}")

        return all_workbooks
    
    def _fetch_workbooks_chunk(self, chunk: List[Dict], all_workbooks: List[Dict], include_owner: bool = True):
        """è¾…åŠ©ï¼šæ‰¹é‡è·å–å·¥ä½œç°¿è¯¦æƒ…"""
        query_parts = []
        owner_field = """
                    owner {
                        id
                        username
                        name
                    }
        """ if include_owner else ""
        
        for idx, wb in enumerate(chunk):
            wb_id = wb["id"]
            query_parts.append(f"""
            wb{idx}: workbooks(filter: {{id: "{wb_id}"}}) {{
                id
                luid
                name
                description
                uri
                projectName
                createdAt
                updatedAt
                containsUnsupportedCustomSql
                vizportalUrlId
                {owner_field}
                upstreamDatasources {{
                    id
                    name
                }}
                sheets {{
                    id
                    luid
                    name
                    path
                    index
                    createdAt
                    updatedAt
                }}
                dashboards {{
                    id
                    luid
                    name
                    path
                    index
                    createdAt
                    updatedAt
                    sheets {{
                        id
                    }}
                }}
                embeddedDatasources {{
                    id
                    name
                    upstreamDatasources {{
                        id
                        name
                    }}
                    upstreamTables {{
                        id
                        name
                    }}
                    fields {{
                        id
                        name
                        description
                        ... on ColumnField {{
                            dataType
                            role
                            isHidden
                            folderName
                        }}
                        ... on CalculatedField {{
                            dataType
                            role
                            isHidden
                            folderName
                            formula
                        }}
                    }}
                }}
            }}
            """)
        
        full_query = "{" + "\n".join(query_parts) + "}"
        result = self.execute_query(full_query)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ strict error (å¯¼è‡´ data ä¸º null)
        if "data" not in result or not result["data"]:
             # æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘é™çº§
             raise Exception("Data is null, likely non-nullable field violation")
             
        data = result.get("data", {})
        
        for key, wb_list in data.items():
            if not wb_list: continue 
            wb_detail = wb_list[0]
            
            all_workbooks.append(wb_detail)
    
    def fetch_fields(self) -> List[Dict]:
        all_fields = []
        
        # 1. è·å–æ‰€æœ‰æ•°æ®æº ID
        print(f"  æ­£åœ¨è·å–æ•°æ®æºåˆ—è¡¨ä»¥åŒæ­¥å­—æ®µ...")
        ds_query = """
        {
            publishedDatasources {
                id
                name
            }
            embeddedDatasources {
                id
                name
                upstreamDatasources {
                    id
                    name
                }
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "errors" in ds_result and not ds_result.get("data"):
            print(f"  âš ï¸ è·å–æ•°æ®æºå¤±è´¥: {ds_result['errors']}")
            return []
            
        published = ds_result.get("data", {}).get("publishedDatasources") or []
        embedded = ds_result.get("data", {}).get("embeddedDatasources") or []
        
        # å»ºç«‹åµŒå…¥å¼åˆ°å‘å¸ƒæ˜ å°„
        embedded_to_published = {}
        for ds in embedded:
            upstreams = ds.get("upstreamDatasources") or []
            if upstreams:
                embedded_to_published[ds["id"]] = upstreams[0]["id"]

        # åˆ†åˆ«å¤„ç†ä¸¤ç§æ•°æ®æº
        self._batch_fetch_fields(published, "publishedDatasources", all_fields)
        self._batch_fetch_fields(embedded, "embeddedDatasources", all_fields, embedded_to_published)
        
        print(f"  âœ… å…±é‡‡é›†åˆ° {len(all_fields)} ä¸ªå­—æ®µ")
        return all_fields

    def _batch_fetch_fields(self, datasources: List[Dict], type_name: str, all_fields: List[Dict], 
                             embedded_to_published: Dict = None):
        """æ‰¹é‡è·å–å­—æ®µè¯¦æƒ… (è¾…åŠ©æ–¹æ³•)"""
        if not datasources:
            return
        
        embedded_to_published = embedded_to_published or {}

        print(f"  åŒæ­¥ {type_name}: {len(datasources)} ä¸ª...")
        chunk_size = 10
        total = len(datasources)
        
        for i in range(0, total, chunk_size):
            chunk = datasources[i:i+chunk_size]
            
            # åŠ¨æ€æ„å»º Alias Filter æŸ¥è¯¢
            query_parts = []
            for idx, ds in enumerate(chunk):
                ds_id = ds["id"]
                query_parts.append(f"""
                q{idx}: {type_name}(filter: {{id: "{ds_id}"}}) {{
                    id
                    name
                    fields {{
                        __typename
                        id
                        name
                        description
                        ... on ColumnField {{
                            role
                            isHidden
                            upstreamColumns {{
                                id
                                name
                                remoteType
                                table {{
                                    id
                                    name
                                    __typename

                                    ... on CustomSQLTable {{
                                        upstreamTables {{
                                            id
                                            name
                                        }}
                                    }}
                                }}
                            }}
                        }}
                        ... on CalculatedField {{
                            role
                            isHidden
                            formula
                            upstreamFields {{
                                id
                                name
                                datasource {{
                                    id
                                    name
                                }}
                                ... on ColumnField {{
                                    upstreamColumns {{
                                        id
                                        name
                                        table {{
                                            id
                                            name
                                            __typename

                                            ... on CustomSQLTable {{
                                                upstreamTables {{
                                                    id
                                                    name
                                                }}
                                            }}
                                        }}
                                    }}
                                }}
                            }}
                        }}
                        ... on GroupField {{
                            role
                            isHidden
                        }}
                        ... on DatasourceField {{
                            remoteField {{
                                id
                                name
                                description
                                datasource {{
                                    id
                                    name
                                    __typename
                                }}
                            }}
                            upstreamColumns {{
                                id
                                name
                                table {{
                                    id
                                    name
                                    __typename

                                    ... on CustomSQLTable {{
                                        upstreamTables {{
                                            id
                                            name
                                        }}
                                    }}
                                }}
                            }}
                        }}
                    }}
                }}
                """)
            
            full_query = "{" + "\n".join(query_parts) + "}"
            
            try:
                result = self.execute_query(full_query)
                if "errors" in result:
                    print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} éƒ¨åˆ†å¤±è´¥: {result['errors'][0].get('message')}")
                
                data = result.get("data", {})
                if not data: continue
                
                for key, ds_list in data.items():
                    if not ds_list: continue
                    # filteræŸ¥è¯¢è¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                    ds_data = ds_list[0]
                    ds_id = ds_data.get("id")
                    ds_name = ds_data.get("name")
                    
                    # è¡€ç¼˜ç©¿é€ï¼šå¦‚æœæ˜¯åµŒå…¥å¼ä¸”æœ‰ä¸Šæ¸¸å‘å¸ƒå¼ï¼Œåˆ™ä½¿ç”¨å‘å¸ƒå¼çš„ ID
                    final_ds_id = embedded_to_published.get(ds_id, ds_id)
                    
                    fields_list = ds_data.get("fields") or []
                    for field in fields_list:
                        if field and field.get("id"):
                            field["datasource_id"] = final_ds_id
                            field["datasource_name"] = ds_name
                            field["parent_datasource_id"] = ds_id  # ä¿ç•™åŸå§‹ ID å¤‡ç”¨
                            all_fields.append(field)
                
                print(f"    - {type_name}: å·²å¤„ç† {min(i+chunk_size, total)}/{total}")
                
            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡æŸ¥è¯¢å¼‚å¸¸: {e}")

    def fetch_calculated_fields(self) -> List[Dict]:
        """è·å–æ‰€æœ‰è®¡ç®—å­—æ®µ"""
        query = """
        {
            calculatedFields {
                id
                name
                description
                formula
                dataType
                role
                datasource {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯
        if "errors" in result:
            print(f"  âš ï¸ GraphQL é”™è¯¯: {result['errors']}")
            return []
        
        data = result.get("data")
        if data is None:
            return []
        
        calc_fields = data.get("calculatedFields") or []
        
        # å°è¯•ç©¿é€ï¼šå¯¹äºæ²¡æœ‰ datasource æˆ–è€… datasource ä¸ºåµŒå…¥å¼çš„ï¼Œ
        # åœ¨è¿”å›å‰å¯ä»¥æ ¹æ®å†…éƒ¨é€»è¾‘å¢å¼ºï¼Œä½†æœ€æ ¸å¿ƒçš„ç©¿é€å·²åœ¨ _batch_fetch_fields å®Œæˆã€‚
        # è¿™é‡Œæˆ‘ä»¬ç¡®ä¿ cf ä¹Ÿæºå¸¦å¿…è¦çš„ datasource_idã€‚
        for cf in calc_fields:
            if cf and cf.get("datasource"):
                cf["datasource_id"] = cf["datasource"].get("id")
            
        return calc_fields
    
    def fetch_views_with_fields(self) -> List[Dict]:
        """è·å–è§†å›¾åŠå…¶ä½¿ç”¨çš„å­—æ®µï¼ˆè¿­ä»£ä¼˜åŒ–ç‰ˆï¼šé€šè¿‡ Filter-ID åˆ†é¡µé‡‡é›†ï¼‰"""
        all_view_fields = []
        
        print(f"  æ­£åœ¨è·å–è§†å›¾å­—æ®µå…³è”(ä¼˜åŒ–ç‰ˆ)...")
        
        # 1. è·å–æ‰€æœ‰å·¥ä½œç°¿ ID
        wb_query = """
        {
            workbooks {
                id
                name
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "errors" in wb_result and not wb_result.get("data"):
             print(f"  âš ï¸ è·å–å·¥ä½œç°¿åˆ—è¡¨å¤±è´¥: {wb_result['errors']}")
             return []
             
        workbooks = wb_result.get("data", {}).get("workbooks") or []
        print(f"  éœ€åŒæ­¥ {len(workbooks)} ä¸ªå·¥ä½œç°¿çš„è§†å›¾å…³è”...")
        
        # 2. é€ä¸ªå·¥ä½œç°¿æŸ¥è¯¢ (æˆ–å°æ‰¹é‡)
        # è€ƒè™‘åˆ°è§†å›¾åŒ…å«çš„å­—æ®µå¼•ç”¨èŠ‚ç‚¹å¯èƒ½å¾ˆå¤šï¼Œè¿™é‡Œé‡‡ç”¨æ¯æ¬¡æŸ¥ 5 ä¸ªå·¥ä½œç°¿
        chunk_size = 5
        total = len(workbooks)
        
        for i in range(0, total, chunk_size):
            chunk = workbooks[i:i+chunk_size]
            
            # åŠ¨æ€æ„å»º Alias Filter æŸ¥è¯¢
            query_parts = []
            for idx, wb in enumerate(chunk):
                wb_id = wb["id"]
                query_parts.append(f"""
                wb{idx}: workbooks(filter: {{id: "{wb_id}"}}) {{
                    id
                    name
                    sheets {{
                        id
                        name
                        sheetFieldInstances {{
                            id
                            name
                            datasource {{
                                id
                            }}
                        }}
                    }}
                }}
                """)
            
            full_query = "{" + "\n".join(query_parts) + "}"
            
            try:
                result = self.execute_query(full_query)
                if "errors" in result:
                    print(f"  âš ï¸ æ‰¹æ¬¡ {i//chunk_size + 1} éƒ¨åˆ†å¤±è´¥: {result['errors'][0].get('message')}")
                
                data = result.get("data", {})
                if not data: continue
                
                # è§£æåˆ«åç»“æœ
                for key, wb_list in data.items():
                    if not wb_list: continue
                    # workbooks è¿”å›çš„æ˜¯åˆ—è¡¨
                    wb_data = wb_list[0]
                    wb_id = wb_data.get("id")
                    
                    sheets = wb_data.get("sheets") or []
                    for sheet in sheets:
                        if not sheet: continue
                        view_id = sheet.get("id")
                        view_name = sheet.get("name")
                        fields = sheet.get("sheetFieldInstances") or []
                        
                        for field in fields:
                            if field and field.get("id"):
                                all_view_fields.append({
                                    "view_id": view_id,
                                    "view_name": view_name,
                                    "workbook_id": wb_id,
                                    "field_id": field.get("id"),
                                    "field_name": field.get("name"),
                                    "datasource_id": (field.get("datasource") or {}).get("id")
                                })
                                
                print(f"    - å·²å¤„ç† {min(i+chunk_size, total)}/{total} ä¸ªå·¥ä½œç°¿, ç´¯è®¡å…³è” {len(all_view_fields)}")
                
            except Exception as e:
                print(f"  âŒ æ‰¹æ¬¡æŸ¥è¯¢å¼‚å¸¸: {e}")
        
        print(f"  âœ… æŠ“å–åˆ° {len(all_view_fields)} ä¸ªå­—æ®µå…³è”å…³ç³»")
        return all_view_fields
    
    def _fetch_views_with_fields_fallback(self) -> List[Dict]:
        """å¤‡ç”¨æ–¹æ³•ï¼šé€šè¿‡å·¥ä½œç°¿çš„æ•°æ®æºå…³ç³»é—´æ¥è·å–"""
        # ç”±äº Tableau API é™åˆ¶ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€åŒ–ç­–ç•¥ï¼š
        # é€šè¿‡ calculatedFields çš„ datasource å…³ç³»æ¥å»ºç«‹å­—æ®µâ†’è§†å›¾çš„é—´æ¥å…³è”
        query = """
        {
            workbooks {
                id
                name
                sheets {
                    id
                    name
                }
                upstreamDatasources {
                    id
                    name
                }
            }
        }
        """
        result = self.execute_query(query)
        
        if "errors" in result:
            print(f"  âš ï¸ å¤‡ç”¨æŸ¥è¯¢ä¹Ÿå¤±è´¥: {result['errors']}")
            return []
        
        data = result.get("data")
        if data is None:
            return []
        
        workbooks = data.get("workbooks") or []
        
        # è·å–æ•°æ®æºåˆ°å­—æ®µçš„æ˜ å°„
        ds_to_fields = {}
        fields_result = self.execute_query("""
        {
            publishedDatasources {
                id
                fields {
                    id
                    name
                }
            }
        }
        """)
        if "data" in fields_result and fields_result["data"]:
            for ds in (fields_result["data"].get("publishedDatasources") or []):
                if ds:
                    ds_to_fields[ds["id"]] = ds.get("fields") or []
        
        # æ„å»ºè§†å›¾â†’å­—æ®µå…³è”
        view_fields = []
        for wb in workbooks:
            if not wb:
                continue
            sheets = wb.get("sheets") or []
            datasources = wb.get("upstreamDatasources") or []
            
            for sheet in sheets:
                if not sheet:
                    continue
                # å°†æ•°æ®æºçš„å­—æ®µå…³è”åˆ°è§†å›¾
                for ds in datasources:
                    if not ds:
                        continue
                    for field in ds_to_fields.get(ds.get("id"), []):
                        if field and field.get("id"):
                            view_fields.append({
                                "view_id": sheet.get("id"),
                                "view_name": sheet.get("name"),
                                "workbook_id": wb.get("id"),
                                "field_id": field.get("id"),
                                "field_name": field.get("name")
                            })
        
        return view_fields


    def fetch_users(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ Tableau ç”¨æˆ·"""
        query = """
        {
            tableauUsers {
                id
                luid
                name
                username
                email
                domain
                siteRole
            }
        }
        """
        result = self.execute_query(query)
        
        # æ£€æŸ¥é”™è¯¯
        if "errors" in result:
            print(f"  âš ï¸ GraphQL è­¦å‘Š (users): {result['errors']}")
            # å°è¯•ç®€åŒ–æŸ¥è¯¢
            return self._fetch_users_fallback()
        
        return result.get("data", {}).get("tableauUsers", [])
    
    def _fetch_users_fallback(self) -> List[Dict]:
        """å›é€€ï¼šé€šè¿‡ owner å…³ç³»æ”¶é›†ç”¨æˆ·"""
        users_dict = {}
        
        # ä»æ•°æ®æºæ”¶é›†ç”¨æˆ·
        ds_query = """
        {
            publishedDatasources {
                owner {
                    id
                    username
                    name
                }
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "data" in ds_result and ds_result["data"]:
            for ds in (ds_result["data"].get("publishedDatasources") or []):
                if ds and ds.get("owner"):
                    owner = ds["owner"]
                    if owner.get("id"):
                        users_dict[owner["id"]] = {
                            "id": owner.get("id"),
                            "name": owner.get("name"),
                            "username": owner.get("username")
                        }
        
        # ä»å·¥ä½œç°¿æ”¶é›†ç”¨æˆ·
        wb_query = """
        {
            workbooks {
                owner {
                    id
                    username
                    name
                }
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "data" in wb_result and wb_result["data"]:
            for wb in (wb_result["data"].get("workbooks") or []):
                if wb and wb.get("owner"):
                    owner = wb["owner"]
                    if owner.get("id"):
                        users_dict[owner["id"]] = {
                            "id": owner.get("id"),
                            "name": owner.get("name"),
                            "username": owner.get("username")
                        }
        
        return list(users_dict.values())
    
    def fetch_projects(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ Tableau é¡¹ç›®"""
        # é€šè¿‡æ•°æ®æºå’Œå·¥ä½œç°¿çš„ projectName æ”¶é›†é¡¹ç›®ä¿¡æ¯
        # æ³¨æ„ï¼šTableau Metadata API æ²¡æœ‰ç›´æ¥çš„ projects æŸ¥è¯¢ï¼Œéœ€è¦é—´æ¥æ”¶é›†
        projects_dict = {}
        
        # ä»æ•°æ®æºæ”¶é›†é¡¹ç›®
        ds_query = """
        {
            publishedDatasources {
                projectName
                projectVizportalUrlId
            }
        }
        """
        ds_result = self.execute_query(ds_query)
        if "data" in ds_result and ds_result["data"]:
            for ds in (ds_result["data"].get("publishedDatasources") or []):
                if ds and ds.get("projectName"):
                    project_name = ds["projectName"]
                    if project_name and project_name not in projects_dict:
                        projects_dict[project_name] = {
                            "name": project_name,
                            "vizportalUrlId": ds.get("projectVizportalUrlId")
                        }
        
        # ä»å·¥ä½œç°¿æ”¶é›†é¡¹ç›®
        wb_query = """
        {
            workbooks {
                projectName
                projectVizportalUrlId
            }
        }
        """
        wb_result = self.execute_query(wb_query)
        if "data" in wb_result and wb_result["data"]:
            for wb in (wb_result["data"].get("workbooks") or []):
                if wb and wb.get("projectName"):
                    project_name = wb["projectName"]
                    if project_name and project_name not in projects_dict:
                        projects_dict[project_name] = {
                            "name": project_name,
                            "vizportalUrlId": wb.get("projectVizportalUrlId")
                        }
        
        # ç”Ÿæˆå”¯ä¸€ ID (ä½¿ç”¨ MD5 ä¿è¯ç¨³å®šæ€§)
        result = []
        for name, proj in projects_dict.items():
            # ä½¿ç”¨ MD5 ç”Ÿæˆç¨³å®šçš„ ID (å‰8ä½ä½œä¸º ID)
            import hashlib
            name_hash = hashlib.md5(name.encode('utf-8')).hexdigest()
            proj["id"] = f"project_{name_hash[:8]}"
            result.append(proj)
        
        return result


class MetadataSync:
    """å…ƒæ•°æ®åŒæ­¥ç®¡ç†å™¨"""
    
    def __init__(self, client: TableauMetadataClient, db_path: str = None):
        self.client = client
        self.db_path = db_path or Config.DATABASE_PATH
        self.engine = get_engine(self.db_path)
        self.session = get_session(self.engine)
        self.sync_log: Optional[SyncLog] = None
    
    def _start_sync_log(self, sync_type: str):
        """å¼€å§‹åŒæ­¥æ—¥å¿—"""
        self.sync_log = SyncLog(
            sync_type=sync_type,
            status="running",
            started_at=datetime.now(),
            records_synced=0
        )
        self.session.add(self.sync_log)
        self.session.commit()
    
    def _complete_sync_log(self, records: int, error: str = None):
        """å®ŒæˆåŒæ­¥æ—¥å¿—"""
        if self.sync_log:
            self.sync_log.status = "failed" if error else "completed"
            self.sync_log.completed_at = datetime.utcnow()
            self.sync_log.records_synced = records
            self.sync_log.error_message = error
            self.session.commit()

    def _cleanup_orphaned_records(self, model_class, current_ids: List[str], filter_condition=None):
        """æ¸…ç†æ•°æ®åº“ä¸­å­˜åœ¨ä½†æœ¬æ¬¡åŒæ­¥æœªå‘ç°çš„è®°å½•ï¼ˆç‰©ç†åˆ é™¤ï¼‰"""
        if not current_ids and filter_condition is None:
            return 0
        
        query = self.session.query(model_class)
        if filter_condition is not None:
            query = query.filter(filter_condition)
        
        # è¿‡æ»¤æ‰æœ¬æ¬¡åŒæ­¥å‘ç°çš„ IDs
        orphaned = query.filter(~model_class.id.in_(current_ids)).all()
        
        count = 0
        for record in orphaned:
            # å¯¹äº Fieldï¼Œè¿˜éœ€è¦æ¸…ç†ç›¸å…³çš„ CalculatedField å’Œä¾èµ–
            if model_class == Field:
                self.session.query(CalculatedField).filter_by(field_id=record.id).delete()
                self.session.query(FieldDependency).filter(
                    (FieldDependency.source_field_id == record.id) | 
                    (FieldDependency.dependency_field_id == record.id)
                ).delete()
            
            self.session.delete(record)
            count += 1
            
        if count > 0:
            print(f"  ğŸ§¹ æ¸…ç†äº† {count} ä¸ªå·²ä¸å­˜åœ¨çš„ {model_class.__name__} è®°å½•")
        return count
    
    def sync_databases(self) -> int:
        """åŒæ­¥æ•°æ®åº“ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“¦ åŒæ­¥æ•°æ®åº“...")
        self._start_sync_log("databases")
        
        try:
            databases = self.client.fetch_databases()
            count = 0
            current_ids = []
            
            for db_data in databases:
                if not db_data or not db_data.get("id"):
                    continue
                
                current_ids.append(db_data["id"])
                db = self.session.query(Database).filter_by(id=db_data["id"]).first()
                if not db:
                    db = Database(id=db_data["id"])
                    self.session.add(db)
                
                db.name = db_data.get("name", "")
                db.luid = db_data.get("luid")
                db.connection_type = db_data.get("connectionType")
                db.host_name = db_data.get("hostName")
                db.port = db_data.get("port")
                db.service = db_data.get("service")
                db.description = db_data.get("description")
                db.is_certified = db_data.get("isCertified", False)
                db.certification_note = db_data.get("certificationNote")
                db.platform = db_data.get("platform")
                db.updated_at = datetime.now()
                
                count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®åº“è®°å½•
            self._cleanup_orphaned_records(Database, current_ids)
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®åº“")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_tables(self) -> int:
        """åŒæ­¥æ•°æ®è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“‹ åŒæ­¥æ•°æ®è¡¨...")
        self._start_sync_log("tables")
        
        try:
            tables = self.client.fetch_tables()
            table_count = 0
            column_count = 0
            current_ids = []
            
            for table_data in tables:
                if not table_data or not table_data.get("id"):
                    continue
                
                # ä¸å†è·³è¿‡åµŒå…¥å¼è¡¨ï¼Œå› ä¸ºå­—æ®µçš„ upstream_column_id å¯èƒ½å¼•ç”¨åµŒå…¥å¼è¡¨çš„åˆ—
                # æ ‡è®° is_embedded ä»¥ä¾¿åœ¨ UI ä¸­åŒºåˆ†
                is_embedded = table_data.get("isEmbedded", False)
                
                current_ids.append(table_data["id"])
                table = self.session.query(DBTable).filter_by(id=table_data["id"]).first()
                if not table:
                    table = DBTable(id=table_data["id"])
                    self.session.add(table)
                
                table.name = table_data.get("name", "")
                table.luid = table_data.get("luid")
                table.full_name = table_data.get("fullName")
                table.schema = table_data.get("schema")
                
                # å…³è”æ•°æ®åº“
                db_info = table_data.get("database", {})
                if db_info:
                    table.database_id = db_info.get("id")
                    table.connection_type = db_info.get("connectionType", "")
                
                table.table_type = table_data.get("tableType")
                table.description = table_data.get("description")
                table.is_embedded = is_embedded  # æ­£ç¡®æ ‡è®°åµŒå…¥å¼è¡¨
                table.is_certified = table_data.get("isCertified", False)
                table.certification_note = table_data.get("certificationNote")
                table.project_name = table_data.get("projectName")
                
                # è§£ææ—¶é—´
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = table_data.get(time_field)
                    if time_val:
                        try:
                            # å…¼å®¹ä¸åŒæ ¼å¼
                            dt = datetime.fromisoformat(time_val.replace("Z", "+00:00"))
                            setattr(table, attr_name, dt)
                        except:
                            pass
                
                # åŒæ­¥åˆ— (Columns)
                columns = table_data.get("columns", [])
                for col_data in columns:
                    if not col_data or not col_data.get("id"):
                        continue
                    
                    col = self.session.query(DBColumn).filter_by(id=col_data["id"]).first()
                    if not col:
                        col = DBColumn(id=col_data["id"])
                        self.session.add(col)
                    
                    col.name = col_data.get("name", "")
                    col.remote_type = col_data.get("remoteType")
                    col.description = col_data.get("description")
                    col.is_nullable = col_data.get("isNullable")
                    col.table_id = table.id
                    column_count += 1
                
                table_count += 1
                if table_count % 100 == 0:
                    self.session.commit()
                    print(f"  - æ•°æ®è¡¨: å·²å¤„ç† {table_count}/{len(tables)}")
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ­£å¼è¡¨è®°å½•ï¼ˆæ’é™¤åµŒå…¥å¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†åŒæ­¥å®ƒä»¬ï¼‰
            self._cleanup_orphaned_records(DBTable, current_ids, filter_condition=(DBTable.is_embedded == False))
            
            self._complete_sync_log(table_count)
            print(f"  âœ… åŒæ­¥ {table_count} ä¸ªæ•°æ®è¡¨, {column_count} ä¸ªåˆ—")
            return table_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
    
    def sync_datasources(self) -> int:
        """åŒæ­¥æ•°æ®æºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ”— åŒæ­¥æ•°æ®æº...")
        self._start_sync_log("datasources")
        
        try:
            datasources = self.client.fetch_datasources()
            count = 0
            current_ids = []
            
            for ds_data in datasources:
                if not ds_data or not ds_data.get("id"):
                    continue
                
                # ä»…åŒæ­¥å‘å¸ƒå¼æ•°æ®æº
                if ds_data.get("isEmbedded"):
                    continue
                
                current_ids.append(ds_data["id"])
                ds = self.session.query(Datasource).filter_by(id=ds_data["id"]).first()
                if not ds:
                    ds = Datasource(id=ds_data["id"])
                    self.session.add(ds)
                
                ds.name = ds_data.get("name", "")
                ds.luid = ds_data.get("luid")
                ds.description = ds_data.get("description")
                ds.uri = ds_data.get("uri")
                ds.project_name = ds_data.get("projectName", "")
                ds.has_extract = ds_data.get("hasExtracts", False)
                ds.is_certified = ds_data.get("isCertified", False)
                ds.certification_note = ds_data.get("certificationNote")
                ds.certifier_display_name = ds_data.get("certifierDisplayName")
                ds.contains_unsupported_custom_sql = ds_data.get("containsUnsupportedCustomSql", False)
                ds.has_active_warning = ds_data.get("hasActiveWarning", False)
                ds.vizportal_url_id = ds_data.get("vizportalUrlId")
                ds.is_embedded = False # æ˜ç¡®è®¾ç½®ä¸ºFalseï¼Œå› ä¸ºæˆ‘ä»¬è¿‡æ»¤æ‰äº†åµŒå…¥å¼æ•°æ®æº
                
                owner = ds_data.get("owner", {})
                if owner:
                    ds.owner = owner.get("username", "")
                    ds.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [
                    ("extractLastRefreshTime", "extract_last_refresh_time"),
                    ("extractLastIncrementalUpdateTime", "extract_last_incremental_update_time"),
                    ("extractLastUpdateTime", "extract_last_update_time"),
                    ("createdAt", "created_at"),
                    ("updatedAt", "updated_at")
                ]:
                    time_val = ds_data.get(time_field)
                    if time_val:
                        try:
                            setattr(ds, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                count += 1
                
                # åŒæ­¥è¡¨åˆ°æ•°æ®æºçš„å…³ç³»
                upstream_tables = ds_data.get("upstreamTables", [])
                if upstream_tables:
                    print(f"  ğŸ“Š æ•°æ®æº {ds_data.get('name')} çš„ä¸Šæ¸¸è¡¨: {len(upstream_tables)} ä¸ª")
                    # æŠ½æ ·æ‰“å° ID æ ¼å¼
                    if len(upstream_tables) > 0:
                        print(f"     ç¤ºä¾‹è¡¨ ID: {upstream_tables[0].get('id')}")

                for tbl in upstream_tables:
                    if not tbl or not tbl.get("id"):
                        continue
                    rel = self.session.execute(
                        select(table_to_datasource).where(
                            table_to_datasource.c.table_id == tbl["id"],
                            table_to_datasource.c.datasource_id == ds_data["id"]
                        )
                    ).first()
                    
                    if not rel:
                        try:
                            self.session.execute(
                                table_to_datasource.insert().values(
                                    table_id=tbl["id"],
                                    datasource_id=ds_data["id"],
                                    relationship_type="upstream"
                                )
                            )
                        except:
                            pass
                
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„æ•°æ®æºï¼ˆæ’é™¤åµŒå…¥å¼ï¼‰
            self._cleanup_orphaned_records(Datasource, current_ids, filter_condition=(Datasource.is_embedded == False))
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªæ•°æ®æº")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            return 0
            
    def _save_embedded_datasource(self, ds_data: Dict, workbook_id: str, source_published_ds_id: str = None):
        """ä¿å­˜åµŒå…¥å¼æ•°æ®æº (åŒ…æ‹¬ç›´è¿åœºæ™¯å’Œå¼•ç”¨å·²å‘å¸ƒæ•°æ®æºåœºæ™¯)"""
        try:
            ds_id = ds_data["id"]
            ds = self.session.query(Datasource).filter_by(id=ds_id).first()
            if not ds:
                ds = Datasource(id=ds_id)
                self.session.add(ds)
            
            ds.name = ds_data.get("name") or "Embedded Datasource"
            ds.is_embedded = True
            ds.project_name = "(Embedded)" # åµŒå…¥å¼æºé€šå¸¸æ²¡æœ‰ç‹¬ç«‹çš„é¡¹ç›®å½’å±ï¼Œå› ä¸ºå®ƒå±äºå·¥ä½œç°¿
            
            # ğŸ†• è®¾ç½®æºå·²å‘å¸ƒæ•°æ®æºIDï¼ˆè¡€ç¼˜å…³ç³»ï¼‰
            if source_published_ds_id:
                ds.source_published_datasource_id = source_published_ds_id
            
            # å»ºç«‹ä¸Šæ¸¸è¡¨å…³è” (ç›´è¿æºçš„å…³é”®è¡€ç¼˜)
            upstream_tables = ds_data.get("upstreamTables", [])
            for tbl in upstream_tables:
                if not tbl or not tbl.get("id"):
                     continue
                
                # ä½¿ç”¨ select æŸ¥è¯¢é¿å…é‡å¤æ’å…¥é”™è¯¯
                rel = self.session.execute(
                    select(table_to_datasource).where(
                        table_to_datasource.c.table_id == tbl["id"],
                        table_to_datasource.c.datasource_id == ds_id
                    )
                ).first()
                
                if not rel:
                    try:
                        self.session.execute(
                            table_to_datasource.insert().values(
                                table_id=tbl["id"],
                                datasource_id=ds_id,
                                relationship_type="upstream"
                            )
                        )
                    except:
                        pass # å¿½ç•¥ä¸»é”®å†²çª

        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜åµŒå…¥å¼æ•°æ®æºå¤±è´¥: {e}")
    
    def sync_workbooks(self) -> int:
        """åŒæ­¥å·¥ä½œç°¿å’Œè§†å›¾ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("\nğŸ“Š åŒæ­¥å·¥ä½œç°¿...")
        self._start_sync_log("workbooks")
        
        try:
            workbooks = self.client.fetch_workbooks()
            wb_count = 0
            view_count = 0
            current_wb_ids = []
            current_view_ids = []
            
            for wb_data in workbooks:
                current_wb_ids.append(wb_data["id"])
                wb = self.session.query(Workbook).filter_by(id=wb_data["id"]).first()
                if not wb:
                    wb = Workbook(id=wb_data["id"])
                    self.session.add(wb)
                
                wb.name = wb_data.get("name", "")
                wb.luid = wb_data.get("luid")
                wb.description = wb_data.get("description")
                wb.uri = wb_data.get("uri")
                wb.project_name = wb_data.get("projectName", "")
                wb.contains_unsupported_custom_sql = wb_data.get("containsUnsupportedCustomSql", False)
                wb.has_active_warning = wb_data.get("hasActiveWarning", False)
                wb.vizportal_url_id = wb_data.get("vizportalUrlId")
                
                owner = wb_data.get("owner", {})
                if owner:
                    wb.owner = owner.get("username", "")
                    wb.owner_id = owner.get("id")
                
                # è§£ææ—¶é—´å­—æ®µ
                for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                    time_val = wb_data.get(time_field)
                    if time_val:
                        try:
                            setattr(wb, attr_name, datetime.fromisoformat(
                                time_val.replace("Z", "+00:00")
                            ))
                        except:
                            pass
                
                wb_count += 1
                
                # åŒæ­¥æ•°æ®æºåˆ°å·¥ä½œç°¿çš„å…³ç³» (Published)
                upstream_ds = wb_data.get("upstreamDatasources", [])
                for ds in upstream_ds:
                    if not ds or not ds.get("id"):
                        continue
                    self._link_datasource_to_workbook(ds["id"], wb_data["id"])

                # åŒæ­¥åµŒå…¥å¼æ•°æ®æº (Embedded)
                embedded_ds = wb_data.get("embeddedDatasources", [])
                for eds in embedded_ds:
                    if not eds or not eds.get("id"):
                        continue
                    
                    upstream_published = eds.get("upstreamDatasources", [])
                    upstream_ds_id = None
                    
                    if upstream_published:
                        # åœºæ™¯1ï¼šåµŒå…¥å¼æºå¼•ç”¨äº†å·²å‘å¸ƒæ•°æ®æº (ç©¿é€æ¨¡å¼)
                        # å°†ä¸Šæ¸¸å‘å¸ƒå¼æ•°æ®æºå…³è”åˆ°å·¥ä½œç°¿
                        for up_ds in upstream_published:
                            if up_ds and up_ds.get("id"):
                                self._link_datasource_to_workbook(up_ds["id"], wb_data["id"])
                        upstream_ds_id = upstream_published[0]["id"]
                        
                        # ğŸ†• åœºæ™¯1ä¹Ÿä¿å­˜åµŒå…¥å¼æ•°æ®æºè®°å½•ï¼Œå¹¶è®¾ç½® source_published_datasource_id
                        self._save_embedded_datasource(eds, wb_data["id"], source_published_ds_id=upstream_ds_id)
                        # ğŸ”§ ä¿®å¤ï¼šåœºæ™¯1ä¹Ÿéœ€è¦å»ºç«‹åµŒå…¥å¼æ•°æ®æºåˆ°å·¥ä½œç°¿çš„å…³è”
                        self._link_datasource_to_workbook(eds["id"], wb_data["id"])
                    else:
                        # åœºæ™¯2ï¼šå®Œå…¨ç‹¬ç«‹çš„åµŒå…¥å¼ç›´è¿æº (ä¿ç•™æ¨¡å¼)
                        # ä¿å­˜è¯¥åµŒå…¥å¼æ•°æ®æºï¼Œæ ‡è®° is_embedded=True
                        # è¿™æ ·å·¥ä½œç°¿å°±æœ‰äº†ä¸€ä¸ªå…³è”çš„ Datasourceï¼Œå­—æ®µä¹Ÿæœ‰äº†å½’å±
                        self._save_embedded_datasource(eds, wb_data["id"])
                        # åŒæ—¶ä¹Ÿå»ºç«‹ Datasource -> Workbook å…³è” (è™½ç„¶ä¸Šé¢å·²ç»åœ¨ DB å±‚é¢å»ºç«‹äº†ï¼Œä½†è¿™é‡Œæ˜¾å¼é“¾æ¥)
                        self._link_datasource_to_workbook(eds["id"], wb_data["id"])
                        upstream_ds_id = eds["id"]
                    
                    # åŒæ­¥åµŒå…¥å¼å­—æ®µ
                    eds_fields = eds.get("fields", [])
                    for f_data in eds_fields:
                        self._sync_field(f_data, datasource_id=upstream_ds_id, workbook_id=wb_data["id"])



                
                # åŒæ­¥è§†å›¾ (sheets + dashboards)
                for idx, sheet in enumerate(wb_data.get("sheets", [])):
                    if not sheet or not sheet.get("id"):
                        continue
                    current_view_ids.append(sheet["id"])
                    view = self.session.query(View).filter_by(id=sheet["id"]).first()
                    if not view:
                        view = View(id=sheet["id"])
                        self.session.add(view)
                    
                    view.name = sheet.get("name", "")
                    view.luid = sheet.get("luid")
                    view.path = sheet.get("path")
                    view.index = sheet.get("index", idx)
                    view.view_type = "sheet"
                    view.workbook_id = wb_data["id"]
                    
                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = sheet.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    view_count += 1
                
                # åŒæ­¥ä»ªè¡¨æ¿ (dashboards)
                for idx, dashboard in enumerate(wb_data.get("dashboards", [])):
                    if not dashboard or not dashboard.get("id"):
                        continue
                    current_view_ids.append(dashboard["id"])
                    view = self.session.query(View).filter_by(id=dashboard["id"]).first()
                    if not view:
                        view = View(id=dashboard["id"])
                        self.session.add(view)
                    
                    view.name = dashboard.get("name", "")
                    view.luid = dashboard.get("luid")
                    view.path = dashboard.get("path")
                    view.index = dashboard.get("index", idx)
                    view.view_type = "dashboard"
                    view.workbook_id = wb_data["id"]
                    

                    # è§£ææ—¶é—´
                    for time_field, attr_name in [("createdAt", "created_at"), ("updatedAt", "updated_at")]:
                        time_val = dashboard.get(time_field)
                        if time_val:
                            try:
                                setattr(view, attr_name, datetime.fromisoformat(
                                    time_val.replace("Z", "+00:00")
                                ))
                            except:
                                pass
                    
                    # åŒæ­¥ä»ªè¡¨æ¿ä¸ sheet çš„å…³è”
                    contained_sheets = dashboard.get("sheets", [])
                    for contained_sheet in contained_sheets:
                         if contained_sheet and contained_sheet.get("id"):
                             sheet_id = contained_sheet.get("id")
                             # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                             rel = self.session.execute(
                                 select(dashboard_to_sheet).where(
                                     dashboard_to_sheet.c.dashboard_id == dashboard["id"],
                                     dashboard_to_sheet.c.sheet_id == sheet_id
                                 )
                             ).first()
                             if not rel:
                                 try:
                                     self.session.execute(
                                         dashboard_to_sheet.insert().values(
                                             dashboard_id=dashboard["id"],
                                             sheet_id=sheet_id
                                         )
                                     )
                                 except Exception as e:
                                     print(f"  âš ï¸ å…³è” sheet å¤±è´¥: {e}")
                                     pass
                    
                    view_count += 1
            
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„å·¥ä½œç°¿
            self._cleanup_orphaned_records(Workbook, current_wb_ids)
            # æ¸…ç†å·²ä¸å­˜åœ¨çš„è§†å›¾
            self._cleanup_orphaned_records(View, current_view_ids)
            
            self._complete_sync_log(wb_count)
            print(f"  âœ… åŒæ­¥ {wb_count} ä¸ªå·¥ä½œç°¿, {view_count} ä¸ªè§†å›¾")
            return wb_count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
            return 0
    
    def sync_fields(self) -> int:
        """åŒæ­¥å­—æ®µï¼ˆå«å»é‡é€»è¾‘ï¼‰"""
        print("\nğŸ”¤ åŒæ­¥å­—æ®µ...")
        self._start_sync_log("fields")
        
        try:
            from backend.models import table_to_datasource, Datasource
            # å»ºç«‹ç‰©ç†è¡¨åˆ°å‘å¸ƒå¼æ•°æ®æºçš„æ˜ å°„ï¼Œç”¨äºç©¿é€è¡¥é½
            # ä»…åŒ…å«éåµŒå…¥å¼æ•°æ®æº
            table_real_ds_map = {}
            ds_to_table_rels = self.session.execute(
                select(table_to_datasource.c.table_id, table_to_datasource.c.datasource_id)
                .join(Datasource, Datasource.id == table_to_datasource.c.datasource_id)
                .where(Datasource.is_embedded == 0)
            ).fetchall()
            for tid, dsid in ds_to_table_rels:
                if tid not in table_real_ds_map:
                    table_real_ds_map[tid] = dsid

            fields = self.client.fetch_fields()
            
            # --- å»é‡å‡†å¤‡å¼€å§‹ ---
            # 1. åˆ†ç¦»å·²å‘å¸ƒå­—æ®µå’ŒåµŒå…¥å¼å­—æ®µ
            published_fields = []
            embedded_fields = []
            
            # ç¼“å­˜å·²å‘å¸ƒçš„å­—æ®µï¼š(datasource_id, name) -> field_id
            published_field_cache = {}
            
            # é¢„å¤„ç†ï¼šåˆ†ç±»
            for f in fields:
                if not f or not f.get("id"): continue
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºåµŒå…¥å¼ (é€šè¿‡åŸå§‹ datasource_id å’Œ parent_datasource_id å¯¹æ¯”)
                # fetch_fields ä¸­æœ‰ä¸€æ­¥: field["parent_datasource_id"] = ds_id (åŸå§‹ID)
                # field["datasource_id"] = final_ds_id (ç©¿é€åçš„ID)
                
                orig_ds_id = f.get("parent_datasource_id")
                final_ds_id = f.get("datasource_id")
                
                # å¦‚æœ orig_ds_id != final_ds_idï¼Œè¯´æ˜å‘ç”Ÿäº†ç©¿é€ï¼Œå®ƒæ˜¯åµŒå…¥å¼å­—æ®µ
                if orig_ds_id and final_ds_id and orig_ds_id != final_ds_id:
                    embedded_fields.append(f)
                else:
                    published_fields.append(f)
            
            print(f"  - å­—æ®µé¢„å¤„ç†: å·²å‘å¸ƒ {len(published_fields)} ä¸ª, åµŒå…¥å¼ {len(embedded_fields)} ä¸ª")
            
            count = 0
            calc_count = 0
            skipped_count = 0
            current_ids = []
            
            # --- ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†å·²å‘å¸ƒå­—æ®µ (æ„å»ºåŸºå‡†) ---
            for f_data in published_fields:
                self._process_single_field(f_data, table_real_ds_map)
                current_ids.append(f_data["id"])
                
                # åŠ å…¥ç¼“å­˜
                ds_id = f_data.get("datasource_id")
                name = f_data.get("name")
                if ds_id and name:
                     published_field_cache[(ds_id, name)] = f_data["id"]

                count += 1
                if count % 1000 == 0:
                    self.session.commit()
            
            # --- ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†åµŒå…¥å¼å­—æ®µ (æŸ¥é‡) ---
            for f_data in embedded_fields:
                ds_id = f_data.get("datasource_id") # è¿™æ˜¯ç©¿é€åçš„ ID (å³å·²å‘å¸ƒæºID)
                name = f_data.get("name")
                
                # æ£€æŸ¥æ˜¯å¦é‡å¤
                if ds_id and name and (ds_id, name) in published_field_cache:
                    # å‘ç°é‡å¤ï¼è·³è¿‡ä¿å­˜ï¼Œä½†å¯èƒ½éœ€è¦è®°å½•ï¼ˆä¸ºäº†åç»­ view å…³è”ï¼‰
                    # ä¸‹ä¸€æ­¥ fetch_views_with_fields ä¼šå¤„ç†é‡è¿
                    skipped_count += 1
                    continue
                
                # å¦‚æœä¸é‡å¤ï¼ˆä¾‹å¦‚å·¥ä½œç°¿ç‰¹æœ‰çš„è®¡ç®—å­—æ®µï¼‰ï¼Œåˆ™ä¿å­˜
                self._process_single_field(f_data, table_real_ds_map)
                current_ids.append(f_data["id"])
                count += 1
                
            self.session.commit()
            
            # æ¸…ç†æ•°æ®åº“ä¸­å·²ä¸å­˜åœ¨çš„è®°å½•
            self._cleanup_orphaned_records(Field, current_ids)
            
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µ (å…¶ä¸­ {calc_count} ä¸ªè®¡ç®—å­—æ®µ, å»é‡è·³è¿‡ {skipped_count} ä¸ª)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def _process_single_field(self, f_data, table_real_ds_map):
        """è¾…åŠ©ï¼šå¤„ç†å•ä¸ªå­—æ®µçš„ä¿å­˜é€»è¾‘"""
        from backend.models import DBTable, DBColumn

        # è·å–/åˆ›å»º Field è®°å½•
        field = self.session.query(Field).filter_by(id=f_data["id"]).first()
        if not field:
            field = Field(id=f_data["id"])
            self.session.add(field)
        
        field.name = f_data.get("name") or ""
        field.description = f_data.get("description") or ""
        
        # è·å–åˆå§‹ datasource_id
        ds_id = f_data.get("datasource_id")
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µè¯¦æƒ… (æå‰è§£æä»¥ä¾¿è·å– table_id å’Œ schema ç©¿é€)
        typename = f_data.get("__typename")
        target_table_id = None
        
        if typename == "ColumnField":
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    table_info = first_col.get("table")
                    if table_info:
                        target_table_id = self._get_physical_table_id(table_info)
                        field.table_id = target_table_id

        # è¡€ç¼˜è¡¥é½ï¼šå¦‚æœå½“å‰ datasource_id æŒ‡å‘çš„ä¸æ˜¯å‘å¸ƒå¼ï¼ˆæˆ–ä¸å­˜åœ¨ï¼‰ï¼Œå°è¯•é€šè¿‡ table_id æ‰¾å‘å¸ƒå¼
        if ds_id:
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šå…è®¸å¼•ç”¨åµŒå…¥å¼æ•°æ®æº (is_embedded=1)ï¼Œåªè¦å®ƒå­˜åœ¨äºæ•°æ®åº“ä¸­
            exists = self.session.query(Datasource).filter_by(id=ds_id).first()
            if not exists:
                # å°è¯•é€šè¿‡ table_id æ‰¾å‘å¸ƒå¼æ•°æ®æº
                if target_table_id in table_real_ds_map:
                    ds_id = table_real_ds_map[target_table_id]
                else:
                    # æ— æ³•ç©¿é€åˆ°æœ‰æ•ˆæ•°æ®æºï¼Œåˆ™è®¾ä¸º NULL
                    ds_id = None
        
        field.datasource_id = ds_id

        
        # é»˜è®¤å€¼
        field.data_type = ""
        field.role = ""
        field.is_calculated = False
        field.formula = ""
        field.is_hidden = False
        field.folder_name = f_data.get("folderName")
        field.fully_qualified_name = ""
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µ
        typename = f_data.get("__typename")
        if typename == "CalculatedField":
            field.is_calculated = True
            field.formula = f_data.get("formula") or ""
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            field.folder_name = f_data.get("folderName")
            
            # æŒ‡æ ‡è¡€ç¼˜ç©¿é€ï¼šé€šè¿‡ upstreamFields æ‰¾ç‰©ç†æ•°æ®æºå’Œç‰©ç†è¡¨
            upstream_fields = f_data.get("upstreamFields") or []
            for uf in upstream_fields:
                if uf:
                    # 1. å°è¯•è·å–ç‰©ç†è¡¨ (ä»ä¸Šæ¸¸å­—æ®µçš„ upstreamColumns)
                    upstream_cols = uf.get("upstreamColumns") or []
                    if upstream_cols and not field.table_id:
                        for col in upstream_cols:
                            if col and col.get("table"):
                                field.table_id = self._get_physical_table_id(col["table"])
                                break
                    
                    # 2. å°è¯•è·å–å‘å¸ƒå¼æ•°æ®æº
                    if uf.get("datasource"):
                        ref_ds_id = uf["datasource"].get("id")
                        if ref_ds_id:
                            exists = self.session.query(Datasource).filter_by(id=ref_ds_id, is_embedded=0).first()
                            if exists:
                                ds_id = ref_ds_id
                                # ç»§ç»­éå†ä»¥æ‰¾æ›´å¤šçš„table_idï¼Œä½†æ•°æ®æºå·²ç¡®å®š
        elif typename == "ColumnField":
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            field.folder_name = f_data.get("folderName")
            
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    
                    # B1 Fix: å°è¯•è¡¥å…¨ç¼ºå¤±çš„ç‰©ç†åˆ—
                    if field.upstream_column_id:
                        db_col = self.session.query(DBColumn).filter_by(id=field.upstream_column_id).first()
                        
                        # å¦‚æœæœ¬åœ°æ²¡æœ‰è¯¥åˆ—ï¼Œä½†æˆ‘ä»¬çŸ¥é“å®ƒå±äºæŸå¼ è¡¨ï¼Œåˆ™åˆ›å»ºè¯¥è¡¨å’Œåˆ—
                        if not db_col and first_col.get("table") and first_col["table"].get("id"):
                            try:
                                real_table_id = first_col["table"]["id"]
                                # ç¡®ä¿è¡¨å­˜åœ¨
                                real_table_id = first_col["table"]["id"]
                                table_typename = first_col["table"].get("__typename")
                                
                                # ç¡®ä¿è¡¨å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
                                real_table = self.session.query(DBTable).filter_by(id=real_table_id).first()
                                new_name = first_col["table"].get("name")
                                
                                if not real_table:
                                    real_table = DBTable(id=real_table_id)
                                    real_table.name = new_name or "Unknown Table"
                                    # å¦‚æœæ˜¯ DatabaseTableï¼Œåˆ™è®¤ä¸ºæ˜¯ç‰©ç†è¡¨ï¼›å¦åˆ™ (EmbeddedTableç­‰) ä¸ºåµŒå…¥è¡¨
                                    real_table.is_embedded = (table_typename != "DatabaseTable")
                                    self.session.add(real_table)
                                    print(f"    ğŸ”¨ è¡¥å…¨ç¼ºå¤±è¡¨: {real_table.name} (Type: {table_typename})")
                                elif real_table.name == "Unknown Table" and new_name:
                                    real_table.name = new_name
                                    print(f"    ğŸ”¨ æ›´æ–°ç¼ºå¤±è¡¨å: {real_table.name}")

                                # åˆ›å»ºè¡¥å…¨åˆ—
                                new_col = DBColumn(id=field.upstream_column_id)
                                new_col.name = field.upstream_column_name
                                new_col.remote_type = first_col.get("remoteType")
                                new_col.table_id = real_table_id
                                self.session.add(new_col)
                                self.session.flush() # ç«‹å³æäº¤
                                print(f"    ğŸ”¨ ä¿®å¤ç¼ºå¤±ç‰©ç†åˆ—: {new_col.name} -> {real_table.name}")
                                db_col = new_col
                            except Exception as e:
                                print(f"    âš ï¸ ä¿®å¤ç‰©ç†åˆ—/è¡¨å¤±è´¥: {e}")

                        if db_col and db_col.remote_type:
                            field.remote_type = db_col.remote_type
                    
                    table_info = first_col.get("table")
                    if table_info:
                        field.table_id = self._get_physical_table_id(table_info)
        elif typename == "DatasourceField":
            # å¤„ç† DatasourceFieldï¼ˆé€šå¸¸æ˜¯åµŒå…¥å¼æ•°æ®æºä¸­å¼•ç”¨å·²å‘å¸ƒæ•°æ®æºçš„å­—æ®µï¼‰
            field.data_type = f_data.get("dataType") or ""
            field.role = (f_data.get("role") or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            
            # è§£æ remoteFieldï¼ˆæŒ‡å‘å·²å‘å¸ƒæ•°æ®æºä¸­çš„åŸå§‹å­—æ®µï¼‰
            remote_field = f_data.get("remoteField")
            if remote_field:
                field.remote_field_id = remote_field.get("id")
                field.remote_field_name = remote_field.get("name")
                
                # å¦‚æœæœ‰ remoteFieldï¼Œå°è¯•è·å–å…¶æ•°æ®æºä¿¡æ¯ç”¨äºè¿½æº¯
                remote_ds = remote_field.get("datasource")
                if remote_ds:
                    remote_ds_id = remote_ds.get("id")
                    # æ£€æŸ¥ remoteField çš„æ•°æ®æºæ˜¯å¦ä¸ºå·²å‘å¸ƒæ•°æ®æº
                    remote_ds_type = remote_ds.get("__typename")
                    if remote_ds_type == "PublishedDatasource":
                        # æ›´æ–°å½“å‰å­—æ®µæ‰€å±çš„åµŒå…¥å¼æ•°æ®æºçš„ source_published_datasource_id
                        # ä½¿ç”¨ parent_datasource_id è€Œä¸æ˜¯ ds_idï¼Œå› ä¸º ds_id å¯èƒ½å·²è¢«è¡€ç¼˜ç©¿é€
                        parent_ds_id = f_data.get("parent_datasource_id")
                        if parent_ds_id:
                            current_ds = self.session.query(Datasource).filter_by(id=parent_ds_id).first()
                            if current_ds and current_ds.is_embedded and not current_ds.source_published_datasource_id:
                                current_ds.source_published_datasource_id = remote_ds_id
            
            # å…³è”ä¸Šæ¸¸è¡¨å’Œåˆ—
            upstream_cols = f_data.get("upstreamColumns") or []
            if upstream_cols and len(upstream_cols) > 0:
                first_col = upstream_cols[0]
                if first_col:
                    field.upstream_column_id = first_col.get("id")
                    field.upstream_column_name = first_col.get("name")
                    table_info = first_col.get("table")
                    if table_info:
                        field.table_id = self._get_physical_table_id(table_info)
        
        # å¤„ç†è®¡ç®—å­—æ®µè¯¦æƒ…
        if f_data.get("isCalculated"):
            calc_field = self.session.query(CalculatedField).filter_by(
                field_id=f_data["id"]
            ).first()
            if not calc_field:
                calc_field = CalculatedField(field_id=f_data["id"])
                self.session.add(calc_field)
            
            calc_field.name = f_data.get("name") or ""
            calc_field.formula = f_data.get("formula") or ""

        # D Fix: å¦‚æœæ­¤æ—¶è¿˜æ²¡æœ‰ table_idï¼Œå°è¯•é€šè¿‡åç§°åŒ¹é…ç‰©ç†è¡¨
        if not field.table_id and field.name:
            # ä»…å½“å­—æ®µåä¸ç°æœ‰ç‰©ç†è¡¨åå®Œå…¨ä¸€è‡´æ—¶å…³è”
            # æ’é™¤å¸¸è§çš„é€šç”¨åç§°
            ignored_names = [":Measure Names", "Measure Values", "Number of Records", "è®°å½•æ•°"]
            if field.name not in ignored_names:
                matched_table = self.session.query(DBTable).filter_by(name=field.name).first()
                if matched_table:
                    field.table_id = matched_table.id
                    print(f"    ğŸ”¨ ä¿®å¤æ— å…³è”è¡¨å­—æ®µ: {field.name} -> å…³è”åˆ°è¡¨ {matched_table.name}")

    def _get_physical_table_id(self, table_info):
        """å°è¯•ä» Table å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯ EmbeddedTable æˆ– CustomSQLTableï¼‰ä¸­æå–ç‰©ç† Table ID"""
        if not table_info:
            return None
            
        typename = table_info.get("__typename")
        table_id = table_info.get("id")
        
        # å¦‚æœæ˜¯ DatabaseTableï¼Œç›´æ¥è¿”å›å…¶ ID
        if typename == "DatabaseTable":
            return table_id
            
        # å¦‚æœæ˜¯ EmbeddedTable æˆ– CustomSQLTableï¼Œå°è¯•ç©¿é€åˆ° upstreamTables
        upstream_tables = table_info.get("upstreamTables") or []
        if upstream_tables and len(upstream_tables) > 0:
            # è¿”å›ç¬¬ä¸€ä¸ªä¸Šæ¸¸ç‰©ç†è¡¨çš„ ID (é€šå¸¸æ˜¯ DatabaseTable)
            # æ³¨æ„ï¼šupstreamTables å¯èƒ½è¿”å›å¤šä¸ªï¼Œé€šå¸¸å–ç¬¬ä¸€ä¸ª
            return upstream_tables[0].get("id")
            
        return table_id

    
    def sync_calculated_fields(self) -> int:
        """åŒæ­¥è®¡ç®—å­—æ®µ"""
        print("\nğŸ“ åŒæ­¥è®¡ç®—å­—æ®µ...")
        self._start_sync_log("calculated_fields")
        
        try:
            calc_fields = self.client.fetch_calculated_fields()
            count = 0
            
            for cf_data in calc_fields:
                if not cf_data or not cf_data.get("id"):
                    continue
                
                # å…ˆç¡®ä¿ Field è®°å½•å­˜åœ¨
                field = self.session.query(Field).filter_by(id=cf_data["id"]).first()
                if not field:
                    field = Field(id=cf_data["id"])
                    self.session.add(field)
                
                field.name = cf_data.get("name") or ""
                field.description = cf_data.get("description") or ""
                field.data_type = cf_data.get("dataType") or ""
                field.is_calculated = True
                field.formula = cf_data.get("formula") or ""
                field.role = (cf_data.get("role") or "").lower()
                if not field.datasource_id:
                    field.datasource_id = cf_data.get("datasource_id")
                
                # æ›´æ–°/åˆ›å»º CalculatedField è®°å½•
                calc_field = self.session.query(CalculatedField).filter_by(
                    field_id=cf_data["id"]
                ).first()
                if not calc_field:
                    calc_field = CalculatedField(field_id=cf_data["id"])
                    self.session.add(calc_field)
                
                calc_field.name = cf_data.get("name") or ""
                calc_field.formula = cf_data.get("formula") or ""
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªè®¡ç®—å­—æ®µ")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_field_to_view(self) -> int:
        """åŒæ­¥å­—æ®µåˆ°è§†å›¾çš„å…³è”å…³ç³»ï¼ˆå«æ™ºèƒ½é‡è¿ï¼‰"""
        print("\nğŸ”— åŒæ­¥å­—æ®µâ†’è§†å›¾å…³è”...")
        self._start_sync_log("field_to_view")
        
        try:
            # 1. æ¸…ç†æ—§æ•°æ® (å…¨é‡åŒæ­¥ç­–ç•¥)
            # ç”±äºæˆ‘ä»¬åšäº†å»é‡ï¼Œå¿…é¡»æ¸…é™¤æ—§çš„å¯èƒ½æŒ‡å‘æ— æ•ˆIDçš„é“¾æ¥
            self.session.execute(text("DELETE FROM field_to_view"))
            self.session.commit()
            print("  ğŸ§¹ å·²æ¸…ç©ºæ—§çš„å­—æ®µå…³è”å…³ç³»")

            view_fields = self.client.fetch_views_with_fields()
            
            # 2. å‡†å¤‡æŸ¥æ‰¾ç¼“å­˜ (Name + Datasource -> FieldID)
            # ç”¨äºå½“åŸå§‹ field_id æ˜¯åµŒå…¥å¼å‰¯æœ¬ï¼ˆå·²è¢«å»é‡ï¼‰æ—¶ï¼Œæ‰¾å›å·²å‘å¸ƒçš„çœŸèº«
            from backend.models import Datasource
            print("  - æ„å»ºå­—æ®µæŸ¥æ‰¾ç¼“å­˜...")
            
            # è·å–æ‰€æœ‰å·²å‘å¸ƒçš„å­—æ®µä¿¡æ¯: (datasource_id, name) -> field_id
            # ä»…åŠ è½½å·²å‘å¸ƒæ•°æ®æºçš„å­—æ®µ
            published_fields_map = {} 
            result = self.session.execute(
                select(Field.id, Field.name, Field.datasource_id)
                .join(Datasource, Datasource.id == Field.datasource_id)
                .where(Datasource.is_embedded == 0)
            ).fetchall()
            
            for fid, fname, fdsid in result:
                if fdsid and fname:
                    published_fields_map[(fdsid, fname)] = fid
            
            # è¿˜éœ€è¦åµŒå…¥å¼æ•°æ®æºID -> å‘å¸ƒå¼æ•°æ®æºID çš„æ˜ å°„
            # è¿™åœ¨ fetch_fields æœŸé—´ç”¨åˆ°äº†ï¼Œè¿™é‡Œé‡æ–°æ„å»ºæˆ–é€šè¿‡ table_to_datasource æ¨æ–­
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ embedded_ds_id åœ¨ backend/models.py é‡Œæ²¡æœ‰ç›´æ¥å­˜å‚¨æ˜ å°„ï¼Œ
            # ä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡ "Tableau Metadata API" çš„ç‰¹æ€§ï¼šembedded field çš„ datasource_id å¾€å¾€æ˜¯ä¸´æ—¶çš„ã€‚
            # æˆ‘ä»¬åœ¨ sync_fields æ—¢ç„¶å·²ç»ç»Ÿä¸€äº† datasource_idï¼Œé‚£æ•°æ®åº“é‡Œçš„ Field.datasource_id éƒ½æ˜¯å‘å¸ƒå¼çš„ã€‚
            
            count = 0
            relinked_count = 0
            skipped = 0
            
            # ç¼“å­˜æœ‰æ•ˆå­—æ®µIDé›†åˆï¼Œå‡å°‘æŸ¥è¯¢
            valid_field_ids = set([r[0] for r in result])
            
            # ä¸ºäº†å¤„ç†åµŒå…¥å¼æ•°æ®æºID -> å‘å¸ƒå¼IDï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¾…åŠ©æ˜ å°„
            # å› ä¸º view_fields è¿”å›çš„æ•°æ®ä¸­ï¼Œfield å¾€å¾€å¸¦ç€åµŒå…¥å¼ DS ID
            # æˆ‘ä»¬éœ€è¦æ„å»º: embedded_ds_id -> published_ds_id
            # è¿™å¯ä»¥é€šè¿‡ "fetch_fields" çš„é€»è¾‘å¤ç°ï¼Œæˆ–è€…æ›´ç®€å•åœ°ï¼š
            # åœ¨ sync_fields é˜¶æ®µæ²¡æœ‰æŒä¹…åŒ–è¿™ä¸ªæ˜ å°„æœ‰ç‚¹å¯æƒœã€‚
            # è¡¥æ•‘ç­–ç•¥ï¼š
            # å¦‚æœç›´æ¥æ‰¾ä¸åˆ° IDï¼Œå°è¯•ç”¨ (ä»»ä½•å‘å¸ƒå¼DS, name) åŒ¹é…ï¼Ÿä¸ï¼Œå¤ªå®½æ³›ã€‚
            # æˆ‘ä»¬å¯ä»¥å°è¯•åŒ¹é… (view.workbook -> upstreamDatasource, name)
            
            # æ„å»º Workbook -> Published Datasources æ˜ å°„
            wb_ds_map = {}
            wb_ds_rels = self.session.execute(
                select(datasource_to_workbook.c.workbook_id, datasource_to_workbook.c.datasource_id)
            ).fetchall()
            for wbid, dsid in wb_ds_rels:
                if wbid not in wb_ds_map:
                    wb_ds_map[wbid] = []
                wb_ds_map[wbid].append(dsid)

            for vf in view_fields:
                field_id = vf.get("field_id")
                field_name = vf.get("field_name")
                view_id = vf.get("view_id")
                workbook_id = vf.get("workbook_id") # éœ€è¦ fetch_views_with_fields è¿”å› workbook_id
                
                if not field_id or not view_id:
                    skipped += 1
                    continue
                
                final_field_id = field_id
                
                # æ£€æŸ¥IDæ˜¯å¦æœ‰æ•ˆ
                if field_id not in valid_field_ids:
                    # ID æ— æ•ˆï¼ˆå¯èƒ½æ˜¯è¢«å»é‡çš„åµŒå…¥å¼å­—æ®µï¼‰
                    # å°è¯•é‡è¿ï¼šåœ¨å·¥ä½œç°¿å…³è”çš„å‘å¸ƒå¼æ•°æ®æºä¸­æŸ¥æ‰¾åŒåå­—æ®µ
                    found_new_id = None
                    
                    if workbook_id and field_name and workbook_id in wb_ds_map:
                        potential_ds_ids = wb_ds_map[workbook_id]
                        for p_ds_id in potential_ds_ids:
                            key = (p_ds_id, field_name)
                            if key in published_fields_map:
                                found_new_id = published_fields_map[key]
                                break
                    
                    if found_new_id:
                        final_field_id = found_new_id
                        relinked_count += 1
                    else:
                        # ç¡®å®æ‰¾ä¸åˆ°ï¼Œæ”¾å¼ƒ
                        skipped += 1
                        continue
                
                # æ’å…¥å…³è” (æ‰¹é‡æ’å…¥ä¼˜åŒ–å¯ç•™å¾…åç»­ï¼Œç›®å‰å•æ¡æ’å…¥å¹¶å¿½ç•¥é”™è¯¯)
                try:
                    self.session.execute(
                        field_to_view.insert().values(
                            field_id=final_field_id,
                            view_id=view_id,
                            used_in_formula=False
                        )
                    )
                    count += 1
                except Exception as e:
                    # å¯èƒ½æ˜¯ä¸»é”®å†²çªï¼ˆå¦‚æœé€»è¾‘æœ‰è¯¯å¯¼è‡´é‡å¤æ’å…¥ï¼‰
                    skipped += 1
                    continue
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªå­—æ®µâ†’è§†å›¾å…³è” (é‡è¿ {relinked_count} ä¸ª, è·³è¿‡ {skipped} ä¸ª)")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_users(self) -> int:
        """åŒæ­¥ Tableau ç”¨æˆ·"""
        print("\nğŸ‘¥ åŒæ­¥ç”¨æˆ·...")
        self._start_sync_log("users")
        
        try:
            users = self.client.fetch_users()
            count = 0
            
            for u_data in users:
                if not u_data or not u_data.get("id"):
                    continue
                    
                user = self.session.query(TableauUser).filter_by(id=u_data["id"]).first()
                if not user:
                    user = TableauUser(id=u_data["id"])
                    self.session.add(user)
                
                user.luid = u_data.get("luid")
                user.name = u_data.get("username") or u_data.get("name") or ""
                user.display_name = u_data.get("name")
                user.email = u_data.get("email")
                user.domain = u_data.get("domain")
                user.site_role = u_data.get("siteRole")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªç”¨æˆ·")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def sync_projects(self) -> int:
        """åŒæ­¥ Tableau é¡¹ç›®"""
        print("\nğŸ“ åŒæ­¥é¡¹ç›®...")
        self._start_sync_log("projects")
        
        try:
            projects = self.client.fetch_projects()
            count = 0
            
            for p_data in projects:
                if not p_data or not p_data.get("name"):
                    continue
                
                project_id = p_data.get("id")
                
                project = self.session.query(Project).filter_by(id=project_id).first()
                if not project:
                    project = Project(id=project_id)
                    self.session.add(project)
                
                project.name = p_data.get("name") or ""
                project.vizportal_url_id = p_data.get("vizportalUrlId")
                count += 1
            
            self.session.commit()
            self._complete_sync_log(count)
            print(f"  âœ… åŒæ­¥ {count} ä¸ªé¡¹ç›®")
            return count
            
        except Exception as e:
            self.session.rollback()
            self._complete_sync_log(0, str(e))
            print(f"  âŒ åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0

    def sync_lineage(self) -> int:
        """åŒæ­¥æŒ‡æ ‡ä¸è¡€ç¼˜å…³ç³» (DBæŒä¹…åŒ–)"""
        print("\nğŸ•¸ï¸ åŒæ­¥è¡€ç¼˜ä¸æŒ‡æ ‡å…³ç³»...")
        count = 0
        
        try:
            # 1. æ¸…ç†ç°æœ‰ä¾èµ–å…³ç³» (å…¨é‡åŒæ­¥ç­–ç•¥)
            self.session.query(FieldDependency).delete()
            self.session.query(Metric).delete() # é‡æ–°æ„å»ºæŒ‡æ ‡è¡¨
            self.session.commit()
            
            # 2. è·å–æ‰€æœ‰è®¡ç®—å­—æ®µ
            calc_fields = self.session.query(CalculatedField, Field).join(
                Field, CalculatedField.field_id == Field.id
            ).all()
            
            # æ„å»ºå­—æ®µç´¢å¼• (Name -> ID lookup cache)
            all_fields = self.session.query(Field).all()
            field_map = {} # (datasource_id, name) -> field_id
            global_field_map = {} # name -> field_id (fallback)
            
            for f in all_fields:
                key = (f.datasource_id, f.name)
                field_map[key] = f.id
                global_field_map[f.name] = f.id
            
            for calc, field in calc_fields:
                formula = calc.formula
                if not formula:
                    continue
                    
                # A. è¯†åˆ« Metric
                # è§„åˆ™: è®¡ç®—å­—æ®µ ä¸” Role=Measure
                if field.role == 'measure':
                    metric = Metric(
                        id=field.id, # å¤ç”¨ Field ID
                        name=field.name,
                        description=field.description,
                        formula=formula,
                        metric_type='Calculated',
                        owner=field.datasource.owner if field.datasource else None
                    )
                    self.session.merge(metric)
                
                # B. è§£æä¾èµ– (åç«¯æŒä¹…åŒ–)
                refs = re.findall(r'\[(.*?)\]', formula)
                unique_refs = set(refs)
                
                for ref_name in unique_refs:
                    dep_id = None
                    
                    # 1. å°è¯•åŒæ•°æ®æºåŒ¹é…
                    if field.datasource_id:
                        dep_id = field_map.get((field.datasource_id, ref_name))
                    
                    # 2. å°è¯•å…¨å±€åŒ¹é…
                    if not dep_id:
                        dep_id = global_field_map.get(ref_name)
                    
                    # 3. åˆ›å»ºä¾èµ–è®°å½•
                    dependency = FieldDependency(
                        source_field_id=field.id,
                        dependency_field_id=dep_id, 
                        dependency_name=ref_name,
                        dependency_type='formula'
                    )
                    self.session.add(dependency)
                    count += 1
            
            self.session.commit()
            print(f"  âœ… åŒæ­¥ {count} æ¡ä¾èµ–å…³ç³»")
            return count
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ è¡€ç¼˜åŒæ­¥å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _link_datasource_to_workbook(self, datasource_id: str, workbook_id: str):
        """å»ºç«‹æ•°æ®æºä¸å·¥ä½œç°¿çš„å…³è”"""
        rel = self.session.execute(
            select(datasource_to_workbook).where(
                datasource_to_workbook.c.datasource_id == datasource_id,
                datasource_to_workbook.c.workbook_id == workbook_id
            )
        ).first()
        if not rel:
            try:
                self.session.execute(
                    datasource_to_workbook.insert().values(
                        datasource_id=datasource_id,
                        workbook_id=workbook_id
                    )
                )
            except:
                pass

    def _sync_field(self, f_data: Dict, datasource_id: str = None, workbook_id: str = None):
        """åŒæ­¥å•ä¸ªå­—æ®µï¼ˆåµŒå…¥å¼æˆ–å‘å¸ƒï¼‰"""
        if not f_data or not f_data.get("id"):
            return

        field = self.session.query(Field).filter_by(id=f_data["id"]).first()
        if not field:
            field = Field(id=f_data["id"])
            self.session.add(field)
        
        field.name = f_data.get("name") or ""
        field.description = f_data.get("description") or ""
        field.datasource_id = datasource_id
        field.workbook_id = workbook_id
        
        # é»˜è®¤å€¼
        if not field.data_type: field.data_type = ""
        if not field.role: field.role = ""
        field.is_calculated = False
        if not field.formula: field.formula = ""
        field.is_hidden = False
        field.folder_name = f_data.get("folderName")
        
        # æ ¹æ®ç±»å‹è§£æå­—æ®µ
        typename = f_data.get("__typename")
        # æŸäº› embedded field å¯èƒ½æ²¡æœ‰ __typenameï¼Œå°è¯•æ¨æ–­æˆ–è¯»å–ç›´æ¥å±æ€§
        if typename == "CalculatedField" or f_data.get("formula"):
            field.is_calculated = True
            field.formula = f_data.get("formula") or ""
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = (f_data.get("role") or field.role or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            
            # ç¡®ä¿ CalculatedField è®°å½•
            calc_sub = self.session.query(CalculatedField).filter_by(field_id=field.id).first()
            if not calc_sub:
                calc_sub = CalculatedField(field_id=field.id)
                self.session.add(calc_sub)
            calc_sub.name = field.name
            calc_sub.formula = field.formula

        elif typename == "ColumnField" or f_data.get("remoteType"):
            field.data_type = f_data.get("dataType") or field.data_type
            field.role = (f_data.get("role") or field.role or "").lower()
            field.is_hidden = f_data.get("isHidden") or False
            # åµŒå…¥å¼åˆ—é€šå¸¸æ²¡æœ‰ upstreamColumns å› ä¸ºå®ƒæ˜¯ç›´æ¥è¿æ¥

    
    def sync_all(self):
        """å…¨é‡åŒæ­¥æ‰€æœ‰å®ä½“"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹å…¨é‡åŒæ­¥ Tableau Metadata")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # æŒ‰ä¾èµ–é¡ºåºåŒæ­¥
        user_count = self.sync_users()  # å…ˆåŒæ­¥ç”¨æˆ·
        project_count = self.sync_projects()  # åŒæ­¥é¡¹ç›®
        db_count = self.sync_databases()
        table_count = self.sync_tables()
        ds_count = self.sync_datasources()
        wb_count = self.sync_workbooks()
        field_count = self.sync_fields()
        calc_count = self.sync_calculated_fields()
        calc_count = self.sync_calculated_fields()
        ftv_count = self.sync_field_to_view()
        lineage_count = self.sync_lineage()
        
        duration = (datetime.now() - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print("ğŸ“ˆ åŒæ­¥å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"  ç”¨æˆ·:   {user_count}")
        print(f"  é¡¹ç›®:   {project_count}")
        print(f"  æ•°æ®åº“: {db_count}")
        print(f"  æ•°æ®è¡¨: {table_count}")
        print(f"  æ•°æ®æº: {ds_count}")
        print(f"  å·¥ä½œç°¿: {wb_count}")
        print(f"  è¡€ç¼˜:   {lineage_count}")
        print(f"  å­—æ®µ:   {field_count}")
        print(f"  è®¡ç®—å­—æ®µ: {calc_count}")
        print(f"  å­—æ®µâ†’è§†å›¾: {ftv_count}")
        print(f"  è€—æ—¶: {duration:.2f} ç§’")
        print("=" * 60)
        
        # åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰
        self.sync_views_usage()
        
        # æœ€åï¼šè®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ
        self.calculate_stats()
    
    def sync_views_usage(self) -> int:
        """åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ï¼ˆé€šè¿‡ REST APIï¼‰å¹¶è®°å½•å†å²å¿«ç…§"""
        print("\nğŸ“Š åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡ (REST API)...")
        
        try:
            usage_map = self.client.fetch_views_usage()
            
            if not usage_map:
                print("  âš ï¸ æœªè·å–åˆ°è§†å›¾ä½¿ç”¨ç»Ÿè®¡")
                return 0
            
            updated = 0
            history_count = 0
            views = self.session.query(View).all()
            
            for view in views:
                # REST API è¿”å›çš„æ˜¯ luidï¼Œéœ€è¦åŒ¹é…
                if view.luid and view.luid in usage_map:
                    new_count = usage_map[view.luid]
                    
                    # åªæœ‰å½“è®¿é—®æ¬¡æ•°å‘ç”Ÿå˜åŒ–æ—¶æ‰è®°å½•å†å²
                    if view.total_view_count != new_count:
                        # è®°å½•å†å²å¿«ç…§
                        from backend.models import ViewUsageHistory
                        history = ViewUsageHistory(
                            view_id=view.id,
                            view_luid=view.luid,
                            total_view_count=new_count
                        )
                        self.session.add(history)
                        history_count += 1
                    
                    view.total_view_count = new_count
                    updated += 1
            
            self.session.commit()
            print(f"  âœ… æ›´æ–° {updated} ä¸ªè§†å›¾çš„ä½¿ç”¨ç»Ÿè®¡, è®°å½• {history_count} æ¡å†å²")
            
            # å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
            self._propagate_dashboard_views()
            
            return updated
            
        except Exception as e:
            print(f"  âŒ åŒæ­¥è§†å›¾ä½¿ç”¨ç»Ÿè®¡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0
    
    def _propagate_dashboard_views(self):
        """å°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡ç´¯åŠ åˆ°å…¶åŒ…å«çš„sheetä¸Š
        
        é€»è¾‘ï¼šå¦‚æœä¸€ä¸ªsheetè¢«åŒ…å«åœ¨ä»ªè¡¨ç›˜ä¸­ï¼Œç”¨æˆ·è®¿é—®ä»ªè¡¨ç›˜æ—¶ä¹Ÿç›¸å½“äºè®¿é—®äº†è¿™äº›sheetã€‚
        å› æ­¤ï¼Œå°†ä»ªè¡¨ç›˜çš„è®¿é—®é‡å®Œæ•´ç´¯åŠ åˆ°æ¯ä¸ªåŒ…å«çš„sheetä¸Šã€‚
        """
        print("  ğŸ“ˆ å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ°åŒ…å«çš„sheet...")
        
        try:
            from sqlalchemy import text
            
            # ä½¿ç”¨ SQL ç›´æ¥æ›´æ–°ï¼Œæ›´é«˜æ•ˆ
            # å¯¹äºæ¯ä¸ªè¢«ä»ªè¡¨ç›˜åŒ…å«çš„sheetï¼Œç´¯åŠ æ‰€æœ‰åŒ…å«å®ƒçš„ä»ªè¡¨ç›˜çš„è®¿é—®é‡
            update_sql = text("""
                UPDATE views
                SET total_view_count = COALESCE(total_view_count, 0) + COALESCE((
                    SELECT SUM(COALESCE(d.total_view_count, 0))
                    FROM dashboard_to_sheet ds
                    JOIN views d ON ds.dashboard_id = d.id
                    WHERE ds.sheet_id = views.id
                ), 0)
                WHERE id IN (SELECT DISTINCT sheet_id FROM dashboard_to_sheet)
            """)
            
            result = self.session.execute(update_sql)
            self.session.commit()
            
            # ç»Ÿè®¡å—å½±å“çš„sheetæ•°é‡
            affected = self.session.execute(text(
                "SELECT COUNT(DISTINCT sheet_id) FROM dashboard_to_sheet"
            )).scalar()
            
            print(f"  âœ… å·²å°†ä»ªè¡¨ç›˜è®¿é—®é‡ç´¯åŠ åˆ° {affected} ä¸ªsheet")
            
        except Exception as e:
            print(f"  âš ï¸ åˆ†æ‘Šä»ªè¡¨ç›˜è®¿é—®é‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def calculate_stats(self):
        """è®¡ç®—å¹¶æ›´æ–°é¢„å­˜ç»Ÿè®¡å­—æ®µï¼ˆåŒæ­¥ç»“æŸåè°ƒç”¨ï¼‰"""
        print("\nğŸ“Š è®¡ç®—é¢„å­˜ç»Ÿè®¡å­—æ®µ...")
        
        try:
            # ========== Workbook ç»Ÿè®¡ ==========
            workbooks = self.session.query(Workbook).all()
            for wb in workbooks:
                wb.view_count = len(wb.views) if wb.views else 0
                wb.datasource_count = len(wb.datasources) if wb.datasources else 0
                
                # ç»Ÿè®¡å­—æ®µå’ŒæŒ‡æ ‡ï¼ˆæ’é™¤åµŒå…¥å¼æ•°æ®æºä¸­çš„é‡å¤ï¼‰
                field_ids = set()
                metric_ids = set()
                
                # æ–¹æ¡ˆ1ï¼šé€šè¿‡å…³è”çš„æ•°æ®æºç»Ÿè®¡ï¼ˆæ›´å‡†ç¡®ä¸”åŒ…å«æœªå¼•ç”¨çš„èµ„äº§ï¼‰
                for ds in (wb.datasources or []):
                    # ä»…ç»Ÿè®¡éåµŒå…¥å¼æ•°æ®æºï¼Œé™¤éå·¥ä½œç°¿æœ¬èº«æ²¡æœ‰å‘å¸ƒå¼æ•°æ®æº
                    if ds.is_embedded and len([d for d in wb.datasources if not d.is_embedded]) > 0:
                        continue
                        
                    for f in (ds.fields or []):
                        if f.is_calculated:
                            if f.role == 'measure' or f.role is None:
                                metric_ids.add(f.id)
                        else:
                            field_ids.add(f.id)
                
                # æ–¹æ¡ˆ2ï¼šå›é€€åˆ°è§†å›¾å¼•ç”¨ï¼ˆå¦‚æœä¸Šè¿°ä¸ºç©ºï¼‰
                if len(field_ids) == 0 and len(metric_ids) == 0:
                    for v in (wb.views or []):
                        for f in (v.fields or []):
                            if f.is_calculated:
                                if f.role == 'measure' or f.role is None:
                                    metric_ids.add(f.id)
                            else:
                                field_ids.add(f.id)
                
                wb.field_count = len(field_ids)
                wb.metric_count = len(metric_ids)

            
            # ========== Datasource ç»Ÿè®¡ ==========
            datasources = self.session.query(Datasource).all()
            for ds in datasources:
                ds.table_count = len(ds.tables) if ds.tables else 0
                ds.workbook_count = len(ds.workbooks) if ds.workbooks else 0
                
                field_count = 0
                metric_count = 0
                for f in (ds.fields or []):
                    if f.is_calculated:
                        if f.role == 'measure' or f.role is None:
                            metric_count += 1
                    else:
                        field_count += 1
                ds.field_count = field_count
                ds.metric_count = metric_count
            
            # ========== Field & CalculatedField æ·±åº¦ç»Ÿè®¡ (æŒ‡æ ‡é¢„è®¡ç®—ä¼˜åŒ–) ==========
            print("  - è®¡ç®—å­—æ®µå’ŒæŒ‡æ ‡æ·±åº¦ç»Ÿè®¡...")
            
            # 1. è®¡ç®—å­—æ®µå…¬å¼å“ˆå¸ŒåŠæŸ¥é‡
            calc_fields = self.session.query(CalculatedField).all()
            formula_map = defaultdict(list)
            for cf in calc_fields:
                if cf.formula:
                    # æ ‡å‡†åŒ–å…¬å¼å¹¶è®¡ç®—å“ˆå¸Œ
                    formula_clean = cf.formula.strip()
                    h = hashlib.md5(formula_clean.encode('utf-8')).hexdigest()
                    cf.formula_hash = h
                    formula_map[h].append(cf)
            
            # æ›´æ–°æŸ¥é‡ä¿¡æ¯
            for h, cfs in formula_map.items():
                is_duplicate = len(cfs) > 1
                for cf in cfs:
                    cf.has_duplicates = is_duplicate
                    cf.duplicate_count = len(cfs) - 1
            
            # 2. ç»Ÿè®¡å­—æ®µè¢«è§†å›¾å¼•ç”¨çš„æ¬¡æ•° (usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°è§†å›¾å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET usage_count = (
                    SELECT COUNT(*) FROM field_to_view 
                    WHERE field_to_view.field_id = fields.id
                )
            """))
            
            # 3. ç»Ÿè®¡å­—æ®µè¢«æŒ‡æ ‡å¼•ç”¨çš„æ¬¡æ•° (metric_usage_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ¬¡æ•°...")
            self.session.execute(text("""
                UPDATE fields SET metric_usage_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_name = fields.name
                )
            """))
            
            # 4. ç»Ÿè®¡æŒ‡æ ‡ä¾èµ–æ•° (dependency_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡ä¾èµ–æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET dependency_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.source_field_id = calculated_fields.field_id
                )
            """))

            # 5. ç»Ÿè®¡æŒ‡æ ‡å¼•ç”¨æ•° (reference_count)
            print("  - ä½¿ç”¨ SQL æ‰¹é‡æ›´æ–°æŒ‡æ ‡å¼•ç”¨æ•°...")
            self.session.execute(text("""
                UPDATE calculated_fields SET reference_count = (
                    SELECT COUNT(*) FROM field_dependencies 
                    WHERE field_dependencies.dependency_field_id = calculated_fields.field_id
                )
            """))

            self.session.commit()
            print(f"  âœ… å·²æ›´æ–° {len(workbooks)} ä¸ªå·¥ä½œç°¿, {len(datasources)} ä¸ªæ•°æ®æº, {len(calc_fields)} ä¸ªè®¡ç®—å­—æ®µçš„ç»Ÿè®¡å­—æ®µ")
            
            # ========== é¢„è®¡ç®—å®Œæ•´è¡€ç¼˜é“¾ (field_full_lineage) ==========
            print("  - é¢„è®¡ç®—å®Œæ•´è¡€ç¼˜é“¾...")
            self._compute_full_lineage()
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ ç»Ÿè®¡è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _compute_full_lineage(self):
        """é¢„è®¡ç®—æ‰€æœ‰å­—æ®µçš„å®Œæ•´è¡€ç¼˜é“¾å¹¶å­˜å…¥ field_full_lineage è¡¨"""
        from backend.models import FieldFullLineage, Field, Datasource
        
        try:
            # æ¸…ç©ºæ—§æ•°æ®
            self.session.execute(text("DELETE FROM field_full_lineage"))
            
            # æ„å»ºæ•°æ®æº -> ç‰©ç†è¡¨çš„æ˜ å°„
            ds_table_map = {}  # datasource_id -> [table_ids]
            result = self.session.execute(text(
                "SELECT datasource_id, table_id FROM table_to_datasource"
            )).fetchall()
            for ds_id, tbl_id in result:
                if ds_id not in ds_table_map:
                    ds_table_map[ds_id] = []
                ds_table_map[ds_id].append(tbl_id)
            
            # éå†æ‰€æœ‰å­—æ®µ
            fields = self.session.query(Field).all()
            lineage_records = []
            
            for f in fields:
                if not f.is_calculated:
                    # åŸå§‹å­—æ®µ: ç›´æ¥è¡€ç¼˜
                    # ç‰©ç†è¡¨æ¥æº: ä¼˜å…ˆç”¨ field.table_idï¼Œå¦åˆ™ç”¨ datasource åæŸ¥
                    table_ids = []
                    if f.table_id and self.session.query(DBTable).filter_by(id=f.table_id).first():
                        table_ids = [f.table_id]
                    elif f.datasource_id and f.datasource_id in ds_table_map:
                        table_ids = ds_table_map[f.datasource_id]
                    
                    if table_ids:
                        for tbl_id in table_ids:
                            lineage_records.append({
                                'field_id': f.id,
                                'table_id': tbl_id,
                                'datasource_id': f.datasource_id,
                                'workbook_id': f.workbook_id,
                                'lineage_type': 'direct',
                                'lineage_path': f'Field -> DS -> Table'
                            })
                    else:
                        # æ— ç‰©ç†è¡¨å…³è”ï¼Œä½†ä»éœ€è®°å½•å­—æ®µå­˜åœ¨
                        lineage_records.append({
                            'field_id': f.id,
                            'table_id': None,
                            'datasource_id': f.datasource_id,
                            'workbook_id': f.workbook_id,
                            'lineage_type': 'direct',
                            'lineage_path': f'Field -> DS (no table)'
                        })
                else:
                    # è®¡ç®—å­—æ®µ: é—´æ¥è¡€ç¼˜ (é€šè¿‡æ•°æ®æºåæŸ¥ç‰©ç†è¡¨)
                    table_ids = []
                    if f.datasource_id and f.datasource_id in ds_table_map:
                        table_ids = ds_table_map[f.datasource_id]
                    
                    if table_ids:
                        for tbl_id in table_ids:
                            lineage_records.append({
                                'field_id': f.id,
                                'table_id': tbl_id,
                                'datasource_id': f.datasource_id,
                                'workbook_id': f.workbook_id,
                                'lineage_type': 'indirect',
                                'lineage_path': f'CalcField -> DS -> Table'
                            })
                    else:
                        # æ— ç‰©ç†è¡¨å…³è”
                        lineage_records.append({
                            'field_id': f.id,
                            'table_id': None,
                            'datasource_id': f.datasource_id,
                            'workbook_id': f.workbook_id,
                            'lineage_type': 'indirect',
                            'lineage_path': f'CalcField -> DS (no table)'
                        })
            
            # æ‰¹é‡æ’å…¥
            if lineage_records:
                self.session.execute(
                    text("""
                        INSERT INTO field_full_lineage 
                        (field_id, table_id, datasource_id, workbook_id, lineage_type, lineage_path)
                        VALUES (:field_id, :table_id, :datasource_id, :workbook_id, :lineage_type, :lineage_path)
                    """),
                    lineage_records
                )
                self.session.commit()
            
            print(f"  âœ… é¢„è®¡ç®— {len(lineage_records)} æ¡å®Œæ•´è¡€ç¼˜è®°å½•")
            
        except Exception as e:
            self.session.rollback()
            print(f"  âŒ é¢„è®¡ç®—è¡€ç¼˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()


def main():
    """ä¸»å‡½æ•° - æ‰§è¡ŒåŒæ­¥"""
    print("\n" + "=" * 60)
    print("Tableau Metadata åŒæ­¥å·¥å…·")
    print("=" * 60)
    
    # ä» Config è¯»å–é…ç½®
    BASE_URL = Config.TABLEAU_BASE_URL.replace('http://', 'https://')  # å¼ºåˆ¶ä½¿ç”¨ HTTPS
    PAT_NAME = Config.TABLEAU_PAT_NAME
    PAT_SECRET = Config.TABLEAU_PAT_SECRET
    USERNAME = Config.TABLEAU_USERNAME
    PASSWORD = Config.TABLEAU_PASSWORD
    
    # åˆ›å»ºå®¢æˆ·ç«¯ (ä¼˜å…ˆä½¿ç”¨ PAT)
    if PAT_NAME and PAT_SECRET:
        client = TableauMetadataClient(BASE_URL, pat_name=PAT_NAME, pat_secret=PAT_SECRET)
    else:
        client = TableauMetadataClient(BASE_URL, username=USERNAME, password=PASSWORD)
    
    # ç™»å½•
    if not client.sign_in():
        print("æ— æ³•ç™»å½•ï¼Œé€€å‡º")
        return
    
    try:
        # åˆ›å»ºåŒæ­¥ç®¡ç†å™¨
        sync = MetadataSync(client)
        
        # æ‰§è¡Œå…¨é‡åŒæ­¥
        sync.sync_all()
        
        sync.close()
        
    finally:
        # ç™»å‡º
        client.sign_out()


if __name__ == "__main__":
    main()
