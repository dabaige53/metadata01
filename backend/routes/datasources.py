"""
æ•°æ®æºæ¥å£è·¯ç”±æ¨¡å—
åŒ…å«æ•°æ®æºåˆ—è¡¨å’Œè¯¦æƒ…æ¥å£
"""
from flask import jsonify, request, g
from sqlalchemy import text, bindparam
from sqlalchemy.orm import selectinload
from . import api_bp
from .utils import build_tableau_url
from ..models import Datasource, Field, View

@api_bp.route('/datasources')
def get_datasources():
    """è·å–æ•°æ®æºåˆ—è¡¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ"""
    session = g.db_session
    search = request.args.get('search', '')
    sort = request.args.get('sort', '')
    order = request.args.get('order', 'asc')
    is_embedded = request.args.get('is_embedded', None)  # æ–°å¢: åµŒå…¥å¼ç­›é€‰
    
    from sqlalchemy.orm import selectinload
    from sqlalchemy import text
    
    # ä½¿ç”¨ selectinload é¢„åŠ è½½å…³è”æ•°æ®
    query = session.query(Datasource).options(
        selectinload(Datasource.tables),
        selectinload(Datasource.fields),
        selectinload(Datasource.workbooks)
    )
    
    # åµŒå…¥å¼ç­›é€‰
    if is_embedded is not None:
        is_emb = is_embedded == '1' or is_embedded.lower() == 'true'
        query = query.filter(Datasource.is_embedded == is_emb)
    
    if search: 
        query = query.filter(Datasource.name.ilike(f'%{search}%'))
    
    # åˆ†é¡µå‚æ•°
    page = request.args.get('page', 1, type=int)
    page_size = request.args.get('page_size', 50, type=int)
    page_size = min(page_size, 10000)
    
    # æ’åº
    if sort == 'name':
        query = query.order_by(Datasource.name.desc() if order == 'desc' else Datasource.name.asc())
    elif sort == 'project_name':
        query = query.order_by(Datasource.project_name.desc() if order == 'desc' else Datasource.project_name.asc())
    elif sort == 'table_count':
        query = query.order_by(Datasource.table_count.desc() if order == 'desc' else Datasource.table_count.asc())
    elif sort == 'workbook_count':
        query = query.order_by(Datasource.workbook_count.desc() if order == 'desc' else Datasource.workbook_count.asc())
    elif sort == 'last_refresh':
        query = query.order_by(Datasource.extract_last_refresh_time.desc() if order == 'desc' else Datasource.extract_last_refresh_time.asc())
    else:
        # é»˜è®¤æ’åº
        query = query.order_by(Datasource.name.asc())

    total_count = query.count()
    offset = (page - 1) * page_size
    
    datasources = query.limit(page_size).offset(offset).all()
    
    # é¢„æŸ¥è¯¢å„é¡¹ç»Ÿè®¡ï¼ˆä»…é’ˆå¯¹å½“å‰é¡µçš„æ•°æ®æºï¼Œç»Ÿä¸€å£å¾„ï¼‰
    ds_ids = [ds.id for ds in datasources]
    view_map = {}
    wb_map = {}
    table_map = {}
    
    if ds_ids:
        from sqlalchemy import bindparam
        # 1. è§†å›¾ç»Ÿè®¡
        stmt_view = text("""
            SELECT dw.datasource_id, COUNT(v.id) as view_count
            FROM datasource_to_workbook dw
            JOIN views v ON dw.workbook_id = v.workbook_id
            WHERE dw.datasource_id IN :ds_ids
            GROUP BY dw.datasource_id
        """).bindparams(bindparam('ds_ids', expanding=True))
        view_stats = session.execute(stmt_view, {'ds_ids': list(ds_ids)}).fetchall()
        view_map = {row[0]: row[1] for row in view_stats}
        
        # 2. å·¥ä½œç°¿ç»Ÿè®¡
        stmt_wb = text("""
            SELECT datasource_id, COUNT(workbook_id) as wb_count
            FROM datasource_to_workbook
            WHERE datasource_id IN :ds_ids
            GROUP BY datasource_id
        """).bindparams(bindparam('ds_ids', expanding=True))
        wb_stats = session.execute(stmt_wb, {'ds_ids': list(ds_ids)}).fetchall()
        wb_map = {row[0]: row[1] for row in wb_stats}
        
        # 3. ç‰©ç†è¡¨ç»Ÿè®¡ - åˆ†åˆ«ç»Ÿè®¡åµŒå…¥è¡¨å’ŒåŸå§‹è¡¨
        stmt_tbl = text("""
            SELECT 
                td.datasource_id, 
                SUM(CASE WHEN t.is_embedded = 1 THEN 1 ELSE 0 END) as embedded_count,
                SUM(CASE WHEN t.is_embedded = 0 OR t.is_embedded IS NULL THEN 1 ELSE 0 END) as regular_count
            FROM table_to_datasource td
            JOIN tables t ON td.table_id = t.id
            WHERE td.datasource_id IN :ds_ids
            GROUP BY td.datasource_id
        """).bindparams(bindparam('ds_ids', expanding=True))
        tbl_stats = session.execute(stmt_tbl, {'ds_ids': list(ds_ids)}).fetchall()
        table_map = {row[0]: {'embedded': row[1], 'regular': row[2]} for row in tbl_stats}
 
    results = []
    for ds in datasources:
        data = ds.to_dict()
        # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€ç»Ÿè®¡æ•°æ®ï¼Œç¡®ä¿ä¸è¯¦æƒ…é¡µåŠå·¥ä½œæµä¸€è‡´
        tbl_info = table_map.get(ds.id, {'embedded': 0, 'regular': 0})
        data['embedded_table_count'] = tbl_info['embedded'] if isinstance(tbl_info, dict) else 0
        data['regular_table_count'] = tbl_info['regular'] if isinstance(tbl_info, dict) else 0
        data['table_count'] = data['embedded_table_count'] + data['regular_table_count']
        data['field_count'] = ds.field_count or 0 # å­—æ®µæ•°é‡ç›®å‰ä¸»è¡¨ç›¸å¯¹å‡†ç¡®
        data['workbook_count'] = wb_map.get(ds.id, ds.workbook_count or 0)
        data['view_count'] = view_map.get(ds.id, 0)
        # ä¼˜åŒ–ï¼šä¸è¿”å›å®Œæ•´çš„å…³è”å¯¹è±¡åˆ—è¡¨ï¼Œä»…è¿”å›æ•°é‡ä»¥å‡å°‘ payload
        # å¦‚æœå‰ç«¯éœ€è¦è¯¦æƒ…ï¼Œåº”ä½¿ç”¨è¯¦æƒ…æ¥å£
        results.append(data)

    # Facets ç»Ÿè®¡
    facets = {}
    
    # is_certified facet
    cert_stats = session.execute(text("""
        SELECT 
            CASE WHEN is_certified = 1 THEN 'true' ELSE 'false' END as certified,
            COUNT(*) as cnt
        FROM datasources
        GROUP BY is_certified
    """)).fetchall()
    facets['is_certified'] = {row[0]: row[1] for row in cert_stats}
    
    # project_name facet
    project_stats = session.execute(text("""
        SELECT project_name, COUNT(*) as cnt
        FROM datasources
        WHERE project_name IS NOT NULL AND project_name != ''
        GROUP BY project_name
        ORDER BY cnt DESC
        LIMIT 20
    """)).fetchall()
    facets['project_name'] = {row[0]: row[1] for row in project_stats if row[0]}

    return jsonify({
        'items': results,
        'total': total_count,
        'page': page,
        'page_size': page_size,
        'total_pages': (total_count + page_size - 1) // page_size,
        'facets': facets
    })

@api_bp.route('/datasources/<ds_id>')
def get_datasource_detail(ds_id):
    """è·å–æ•°æ®æºè¯¦æƒ… - å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå«ä¸Šæ¸¸è¡¨ã€ä¸‹æ¸¸å·¥ä½œç°¿ï¼‰"""
    session = g.db_session
    ds = session.query(Datasource).filter(Datasource.id == ds_id).first()
    if not ds: return jsonify({'error': 'Not found'}), 404

    data = ds.to_dict()

    # ä¸Šæ¸¸ï¼šè¿æ¥çš„æ•°æ®è¡¨ï¼ˆå®Œæ•´ä¿¡æ¯ï¼‰
    tables_data = []
    for t in ds.tables:
        tables_data.append({
            'id': t.id,
            'name': t.name,
            'schema': t.schema,
            'full_name': t.full_name,
            'database_id': t.database_id,
            'database_name': t.database.name if t.database else None,
            'column_count': len(t.columns) if t.columns else 0,
            'is_embedded': t.is_embedded
        })
    data['tables'] = tables_data

    # åŸå§‹åˆ—æ•°æ®ï¼ˆä¼˜å…ˆä»å­—æ®µçš„ upstream_column åå‘è·å–ï¼Œè§£å†³ Custom SQL æ•°æ®æºæ— åˆ—çš„é—®é¢˜ï¼‰
    # æ–¹æ¡ˆï¼šé€šè¿‡ regular_fields.upstream_column_id -> db_columns è·å–å®é™…ä½¿ç”¨çš„åˆ—
    from sqlalchemy import text
    columns_data = []
    seen_column_ids = set()
    
    # æ–¹æ³•1ï¼šä»å­—æ®µçš„ upstream_column_id è·å–ï¼ˆæ¨èï¼Œè¦†ç›– Custom SQL åœºæ™¯ï¼‰
    column_query = text("""
        SELECT DISTINCT 
            c.id, c.name, c.remote_type, c.description, c.is_nullable,
            c.table_id, t.name as table_name, t.is_embedded as table_is_embedded
        FROM regular_fields f
        JOIN db_columns c ON f.upstream_column_id = c.id
        LEFT JOIN tables t ON c.table_id = t.id
        WHERE f.datasource_id = :ds_id
        ORDER BY t.name, c.name
    """)
    upstream_columns = session.execute(column_query, {'ds_id': ds_id}).fetchall()
    
    for col in upstream_columns:
        if col.id not in seen_column_ids:
            seen_column_ids.add(col.id)
            columns_data.append({
                'id': col.id,
                'name': col.name,
                'remote_type': col.remote_type,
                'table_id': col.table_id,
                'table_name': col.table_name,
                'description': col.description,
                'is_nullable': col.is_nullable,
                'is_embedded': col.table_is_embedded
            })
    
    # æ–¹æ³•2ï¼šå›é€€ - å¦‚æœå­—æ®µæ²¡æœ‰ upstream_columnï¼Œåˆ™ä» table_to_datasource å…³è”çš„è¡¨è·å–
    if not columns_data:
        for t in ds.tables:
            for col in t.columns:
                if col.id not in seen_column_ids:
                    seen_column_ids.add(col.id)
                    columns_data.append({
                        'id': col.id,
                        'name': col.name,
                        'remote_type': col.remote_type,
                        'table_id': t.id,
                        'table_name': t.name,
                        'description': col.description,
                        'is_nullable': col.is_nullable,
                        'is_embedded': t.is_embedded
                    })
    
    data['columns'] = columns_data

    # ä¸‹æ¸¸ï¼šä½¿ç”¨æ­¤æ•°æ®æºçš„å·¥ä½œç°¿
    workbooks_data = []
    for wb in ds.workbooks:
        workbooks_data.append({
            'id': wb.id,
            'name': wb.name,
            'project_name': wb.project_name,
            'owner': wb.owner,
            'view_count': len(wb.views) if wb.views else 0
        })
    data['workbooks'] = workbooks_data

    # ä¸‹æ¸¸ï¼šç”±æ­¤è¡ç”Ÿçš„åµŒå…¥å¼æ•°æ®æºå‰¯æœ¬ (v2.1 æ–°å¢)
    # ä»…å½“å½“å‰æ•°æ®æºä¸ºâ€œå·²å‘å¸ƒæ•°æ®æºâ€æ—¶æ‰æŸ¥è¯¢
    if not ds.is_embedded:
        embedded_copies = session.query(Datasource).filter(
            Datasource.source_published_datasource_id == ds.id,
            Datasource.is_embedded == True
        ).all()
        
        embedded_data = []
        for emb in embedded_copies:
            # å°è¯•æ‰¾åˆ°å…³è”çš„å·¥ä½œç°¿
            wb_info = {}
            if emb.workbooks and len(emb.workbooks) > 0:
                wb = emb.workbooks[0]
                wb_info = {'id': wb.id, 'name': wb.name}
                
            embedded_data.append({
                'id': emb.id,
                'name': emb.name,
                'workbook': wb_info,
                'field_count': len(emb.fields)
            })
        data['embedded_datasources'] = embedded_data

    # å­—æ®µåˆ—è¡¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶è¿”å›æ•°é‡ï¼Œç²¾ç®€æ•°æ®æ ¼å¼ï¼‰
    # æ³¨æ„ï¼šå¯¹äºå¤§æ•°æ®æºï¼Œå®Œæ•´å­—æ®µåˆ—è¡¨é€šè¿‡åˆ†é¡µ API è·å–
    FIELD_LIMIT = 10000  # æå‡è¯¦æƒ…é¡µå±•ç¤ºä¸Šé™ï¼Œè§£å†³å‰ç«¯åˆ—è¡¨è¢«æˆªæ–­çš„é—®é¢˜ (åŸä¸º100)
    
    full_fields = []
    metrics_list = []
    field_count = 0
    metric_count = 0
    
    # ğŸ”§ åµŒå…¥å¼æ•°æ®æºå­—æ®µä¿®å¤ï¼šå¼•ç”¨å‘å¸ƒå¼åˆ™ä»å‘å¸ƒå¼è·å–å­—æ®µåˆ—è¡¨
    source_fields = ds.fields
    if ds.is_embedded and ds.source_published_datasource_id:
        published_ds = session.query(Datasource).filter_by(
            id=ds.source_published_datasource_id
        ).first()
        if published_ds:
            source_fields = published_ds.fields
    
    for f in source_fields:
        # ç²¾ç®€çš„å­—æ®µæ‘˜è¦ï¼ˆé¿å…è¿”å›å®Œæ•´å¯¹è±¡å’Œ N+1 æŸ¥è¯¢ï¼‰
        # å¢åŠ ä¸Šæ¸¸è¡¨ä¿¡æ¯ç”¨äºå‰ç«¯åˆ†ç»„æ˜¾ç¤º
        summary = {
            'id': f.id,
            'name': f.name,
            'role': f.role,
            'data_type': f.data_type or f.remote_type,
            'description': f.description[:100] if f.description else '',
            'usage_count': f.usage_count or 0,
            'is_calculated': f.is_calculated or False,
            # ä¸Šæ¸¸è¡¨ä¿¡æ¯ï¼ˆç”¨äºåˆ†ç»„ï¼‰
            'upstream_table_id': f.table_id,
            'upstream_table_name': f.table.name if f.table else None,
            'upstream_column_name': f.upstream_column.name if f.upstream_column else None,
            # æ¥æºä¿¡æ¯ï¼ˆç”¨äºèšåˆåå±•ç¤ºï¼‰
            'workbook_id': f.workbook_id,
            'workbook_name': f.workbook.name if f.workbook else None,
            'datasource_id': f.datasource_id,
            'datasource_name': f.datasource.name if f.datasource else None,
            'is_embedded_ds': f.datasource.is_embedded if f.datasource else False
        }
        
        if f.is_calculated:
            metric_count += 1
            if len(metrics_list) < FIELD_LIMIT:
                metrics_list.append(summary)
        else:
            field_count += 1
            if len(full_fields) < FIELD_LIMIT:
                full_fields.append(summary)

    data['full_fields'] = full_fields
    data['metrics'] = metrics_list
    data['has_more_fields'] = field_count > FIELD_LIMIT
    data['has_more_metrics'] = metric_count > FIELD_LIMIT
    data['total_field_count'] = field_count
    data['total_metric_count'] = metric_count

    # ç»Ÿè®¡ä¿¡æ¯
    data['stats'] = {
        'table_count': len(ds.tables),
        'workbook_count': len(ds.workbooks),
        'field_count': field_count,  # ä½¿ç”¨å®é™…æ€»æ•°ï¼Œéé™åˆ¶åçš„è¿”å›æ•°
        'metric_count': metric_count  # ä½¿ç”¨å®é™…æ€»æ•°ï¼Œéé™åˆ¶åçš„è¿”å›æ•°
    }
    
    # æ„å»º Tableau Server åœ¨çº¿æŸ¥çœ‹é“¾æ¥
    data['tableau_url'] = build_tableau_url('datasource', uri=ds.uri, luid=ds.luid, vizportal_url_id=ds.vizportal_url_id)

    return jsonify(data)


# -------------------- æ•°æ®æºå­èµ„æºè·¯ç”± --------------------

@api_bp.route('/datasources/<ds_id>/fields')
def get_datasource_fields(ds_id):
    """è·å–æ•°æ®æºå…³è”çš„å­—æ®µåˆ—è¡¨"""
    session = g.db_session
    ds = session.query(Datasource).filter(Datasource.id == ds_id).first()
    if not ds:
        return jsonify({'error': 'Not found'}), 404

    fields_data = [{
        'id': f.id, 'name': f.name, 'role': f.role, 'data_type': f.data_type,
        'description': f.description, 'is_calculated': f.is_calculated
    } for f in ds.fields if not f.is_calculated]
    return jsonify({'items': fields_data, 'total': len(fields_data)})


@api_bp.route('/datasources/<ds_id>/workbooks')
def get_datasource_workbooks(ds_id):
    """è·å–æ•°æ®æºå…³è”çš„å·¥ä½œç°¿åˆ—è¡¨"""
    session = g.db_session
    ds = session.query(Datasource).filter(Datasource.id == ds_id).first()
    if not ds:
        return jsonify({'error': 'Not found'}), 404

    wb_data = [{
        'id': wb.id, 'name': wb.name, 'project_name': wb.project_name, 'owner': wb.owner
    } for wb in ds.workbooks]
    return jsonify({'items': wb_data, 'total': len(wb_data)})
